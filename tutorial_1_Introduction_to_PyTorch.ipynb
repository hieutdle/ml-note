{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hieutdle/hpi-nlp/blob/main/tutorial_1_Introduction_to_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2EWTUA0f4pR"
      },
      "source": [
        "# Tutorial 1: Introduction to PyTorch\n",
        " \n",
        "**Date**: 15.05.2023"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja3aPPJmf4pV"
      },
      "source": [
        "This notebook includes short introduction to PyTorch basics and guidelines on how to write your pipeline for creating and training your own neural networks. This work is highly inspired from [the tutorial by Amsterdam Univeristy](https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial2/Introduction_to_PyTorch.ipynb#scrollTo=tlY-Y6sGf4pe)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4zNUqsuf4pZ"
      },
      "source": [
        "## The Basics of PyTorch\n",
        "\n",
        "We will start with reviewing the very basic concepts of PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmOwKoFPf4pZ",
        "outputId": "3e00c425-e616-4dee-8d16-36146b0628a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Version of the installed torch is: 2.0.0+cu118\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "version = torch.__version__\n",
        "print(f'Version of the installed torch is: {version}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kOrCu7lf4pa"
      },
      "source": [
        "Check the most stable version at [the official PyTorch page](https://pytorch.org/). You can also check the CUDA compatibilities, `pip` or `conda` commands for installing other versions, etc. \n",
        "\n",
        "In general, it is recommended to keep your libraries updated to the newest ones. This is mainly because usually in the newest versions, the bugs/vulnaribilities reported in the older versions of the libraries are fixed."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we will be generating some random numbers in this tutorial, let's make sure to always generate the same sequence of random numbers in different runs. This helps with the reproducibility of your results. Read [this discussion for more information](https://stats.stackexchange.com/questions/354373/what-exactly-is-a-seed-in-a-random-number-generator)."
      ],
      "metadata": {
        "id": "V_XBaG4G03od"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0GVZ1Huf4pa",
        "outputId": "b28b9e77-ff1a-41f0-b056-097f95dd4cb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f3996df12b0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "torch.manual_seed(42) # Setting the seed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJmsusZyf4pa"
      },
      "source": [
        "### Tensors\n",
        "\n",
        "Tensors are the PyTorch equivalent to Numpy arrays, with the addition to also have support for GPU acceleration (more on that later).\n",
        "The name \"tensor\" is a generalization of concepts you already know. For instance, a vector is a 1-D tensor, and a matrix a 2-D tensor. When working with neural networks, we will use tensors of various shapes and number of dimensions.\n",
        "\n",
        "Most common functions you know from numpy can be used on tensors as well."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_tensor = torch.rand(2, 3)\n",
        "print(f'This is a randomly generated tensor with 2 rows and 3 columns: \\n{random_tensor}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4z0pcxj2bJ_",
        "outputId": "3c760954-d4b6-4f5f-a79e-48caaf86a38e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a randomly generated tensor with 2 rows and 3 columns: \n",
            "tensor([[0.8823, 0.9150, 0.3829],\n",
            "        [0.9593, 0.3904, 0.6009]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnC1ZFNif4pb",
        "outputId": "e9a83580-350c-4b50-f530-40dbb4536241",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a 3D randomly generated tensor with the size (2, 3, 4): \n",
            "tensor([[[0.9516, 0.0753, 0.8860, 0.5832],\n",
            "         [0.3376, 0.8090, 0.5779, 0.9040],\n",
            "         [0.5547, 0.3423, 0.6343, 0.3644]],\n",
            "\n",
            "        [[0.7104, 0.9464, 0.7890, 0.2814],\n",
            "         [0.7886, 0.5895, 0.7539, 0.1952],\n",
            "         [0.0050, 0.3068, 0.1165, 0.9103]]])\n"
          ]
        }
      ],
      "source": [
        "random_tensor = torch.rand(2, 3, 4)\n",
        "print(f'This is a 3D randomly generated tensor with the size (2, 3, 4): \\n{random_tensor}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ao9TrKRTf4pb"
      },
      "source": [
        "To directly assign values to the tensor during initialization, there are many alternatives including:\n",
        "\n",
        "* `torch.zeros`: Creates a tensor filled with zeros\n",
        "* `torch.ones`: Creates a tensor filled with ones\n",
        "* `torch.rand`: Creates a tensor with random values uniformly sampled between 0 and 1\n",
        "* `torch.randn`: Creates a tensor with random values sampled from a normal distribution with mean 0 and variance 1\n",
        "* `torch.arange`: Creates a tensor containing the values $N,N+1,N+2,...,M$\n",
        "* `torch.Tensor` (input list): Creates a tensor from the list elements you provide"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_list = [1, 2, 3]\n",
        "sample_tensor = torch.Tensor(sample_list)\n",
        "print(f'This is an example of 1D tensor/vector: {sample_tensor}')\n",
        "print(f'Size of the tensor: {sample_tensor.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-0MSM4-5vv7",
        "outputId": "76bfd8e9-ad6c-4375-b2df-368c444e5fd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is an example of 1D tensor/vector: tensor([1., 2., 3.])\n",
            "Size of the tensor: torch.Size([3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBIU6xd3f4pc",
        "outputId": "aaa10de4-4a66-4c20-9e14-4709ce70c8fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is an example of 2D tensor/matrix:\n",
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]])\n",
            "Size of the tensor: torch.Size([2, 3])\n"
          ]
        }
      ],
      "source": [
        "# Create nested lists for creating tensor with more dimensions\n",
        "nested_list = [[1, 2, 3], [4, 5, 6]]\n",
        "sample_tensor = torch.Tensor(nested_list)\n",
        "print(f'This is an example of 2D tensor/matrix:\\n{sample_tensor}')\n",
        "print(f'Size of the tensor: {sample_tensor.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOAgUW-pf4pc"
      },
      "source": [
        "You can obtain the shape of a tensor in the same way as in numpy (`x.shape`), or using the `.size` method:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6pUmmmif4pe"
      },
      "source": [
        "You can also convert numpy arrays to torch tensors."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "np_arr = np.array([[1, 2], [3, 4]])\n",
        "tensor = torch.from_numpy(np_arr)\n",
        "\n",
        "tensor = torch.arange(4)\n",
        "np_arr = tensor.numpy()"
      ],
      "metadata": {
        "id": "nesOcbIv7P9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np_arr.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUXjy5xaqzwB",
        "outputId": "16bf4cd4-8c8d-4100-f440-2dd2d02690cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4,)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kAOVITpf4pe"
      },
      "source": [
        "Most of the operatins from NumPy also exist in PyTorch. A full list of operations on torch tensors can be found in the [PyTorch documentation](https://pytorch.org/docs/stable/tensors.html#).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfLiZryaf4pg"
      },
      "source": [
        "A commonly used operation is matrix multiplications, which are essential for neural networks. Quite often, we have an input vector $\\mathbf{x}$, which is transformed using a learned weight matrix $\\mathbf{W}$. There are multiple ways and functions to perform matrix multiplication, some of which we list below:\n",
        "\n",
        "* `torch.matmul`: Performs the matrix product over two tensors, where the specific behavior depends on the dimensions. If both inputs are matrices (2-dimensional tensors), it performs the standard matrix product. For higher dimensional inputs, the function supports broadcasting (for details see the [documentation](https://pytorch.org/docs/stable/generated/torch.matmul.html?highlight=matmul#torch.matmul)). Can also be written as `a @ b`, similar to numpy. \n",
        "* `torch.mm`: Performs the matrix product over two matrices, but doesn't support broadcasting (see [documentation](https://pytorch.org/docs/stable/generated/torch.mm.html?highlight=torch%20mm#torch.mm))\n",
        "* `torch.bmm`: Performs the matrix product with a support batch dimension. If the first tensor $T$ is of shape ($b\\times n\\times m$), and the second tensor $R$ ($b\\times m\\times p$), the output $O$ is of shape ($b\\times n\\times p$), and has been calculated by performing $b$ matrix multiplications of the submatrices of $T$ and $R$: $O_i = T_i @ R_i$\n",
        "* `torch.mul`: Used for element-wise multiplication of tensors.\n",
        "\n",
        "Usually, we use `torch.matmul` or `torch.bmm`. We can try a matrix multiplication with `torch.matmul` below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-0pvspJf4pg",
        "outputId": "2cc611d0-66ba-422d-adc6-c1cbb2b15bed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1, 2, 3, 4, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "x = torch.arange(6)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = x.view(2, 3)\n",
        "print(\"X\", x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y29qFZw_rEKK",
        "outputId": "51b7ee86-10c6-49dc-f107-9be010d8d4ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X tensor([[0, 1, 2],\n",
            "        [3, 4, 5]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAR4nMqif4ph",
        "outputId": "11d6dc23-009b-481a-df81-0df97f80f6ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W tensor([[0, 1, 2],\n",
            "        [3, 4, 5],\n",
            "        [6, 7, 8]])\n"
          ]
        }
      ],
      "source": [
        "W = torch.arange(9).view(3, 3) # We can also stack multiple operations in a single line\n",
        "print(\"W\", W)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2R63cveLf4ph",
        "outputId": "bf8910b7-11ab-43a8-d6c5-55ecb4f56cda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "h tensor([[15, 18, 21],\n",
            "        [42, 54, 66]])\n"
          ]
        }
      ],
      "source": [
        "h = torch.matmul(x, W) # Verify the result by calculating it by hand too!\n",
        "print(\"h\", h)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qso1-PGQf4ph"
      },
      "source": [
        "#### Indexing\n",
        "\n",
        "We often have the situation where we need to select a part of a tensor. Indexing works just like in numpy, so let's try it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIjTb_Ujf4ph",
        "outputId": "10c86955-32a6-4d76-8ce8-2960b04f2003"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X tensor([[ 0,  1,  2,  3],\n",
            "        [ 4,  5,  6,  7],\n",
            "        [ 8,  9, 10, 11]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.arange(12).view(3, 4)\n",
        "print(\"X\", x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYuVggvXf4pi",
        "outputId": "7264b768-6101-4ea4-cb97-467d8fc8c4b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1, 5, 9])\n"
          ]
        }
      ],
      "source": [
        "print(x[:, 1])   # Second column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCvJs7hWf4pi",
        "outputId": "8cc14aba-9e90-493e-bdb5-762518d60198"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0, 1, 2, 3])\n"
          ]
        }
      ],
      "source": [
        "print(x[0])      # First row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNemekHBf4pi",
        "outputId": "d505997f-7232-4071-803a-df1595932a73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([3, 7])\n"
          ]
        }
      ],
      "source": [
        "print(x[:2, -1]) # First two rows, last column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYJXZXROf4pj",
        "outputId": "3db565b3-0b93-4f1c-e165-26d6079a7565"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 4,  5,  6,  7],\n",
            "        [ 8,  9, 10, 11]])\n"
          ]
        }
      ],
      "source": [
        "print(x[1:3, :]) # Middle two rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YreBahNf4pj"
      },
      "source": [
        "### Dynamic Computation Graph and Backpropagation\n",
        "\n",
        "If you are unfamiliar with backpropagation, read [this gentle overview of the topic: ](https://www.analyticsvidhya.com/blog/2023/01/gradient-descent-vs-backpropagation-whats-the-difference/).\n",
        "\n",
        "For more information on CrossEntroply: [check here](https://en.wikipedia.org/wiki/Cross_entropy) and [here](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)\n",
        "\n",
        "One of the main reasons for using PyTorch in Deep Learning projects is that we can automatically get **gradients/derivatives** of functions that we define. We will mainly use PyTorch for implementing neural networks, and they are just fancy functions. If we use weight matrices in our function that we want to learn, then those are called the **parameters** or simply the **weights**.\n",
        "\n",
        "If our neural network would output a single scalar value, we would talk about taking the **derivative**, but you will see that quite often we will have **multiple** output variables (\"values\"); in that case we talk about **gradients**. It's a more general term.\n",
        "\n",
        "Given an input $\\mathbf{x}$, we define our function by **manipulating** that input, usually by matrix-multiplications with weight matrices and additions with so-called bias vectors. As we manipulate our input, we are automatically creating a **computational graph**. This graph shows how to arrive at our output from our input. \n",
        "PyTorch is a **define-by-run** framework; this means that we can just do our manipulations, and PyTorch will keep track of that graph for us. Thus, we create a dynamic computation graph along the way.\n",
        "\n",
        "So, to recap: the only thing we have to do is to compute the **output**, and then we can ask PyTorch to automatically get the **gradients**. \n",
        "\n",
        "> **Note:  Why do we want gradients?** Consider that we have defined a function, a neural net, that is supposed to compute a certain output $y$ for an input vector $\\mathbf{x}$. We then define an **error measure** that tells us how wrong our network is; how bad it is in predicting output $y$ from input $\\mathbf{x}$. Based on this error measure, we can use the gradients to **update** the weights $\\mathbf{W}$ that were responsible for the output, so that the next time we present input $\\mathbf{x}$ to our network, the output will be closer to what we want.\n",
        "\n",
        "The first thing we have to do is to specify which tensors require gradients. By default, when we create a tensor, it does not require gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XydvXU8_f4pj",
        "outputId": "73fddc7e-5029-4985-d8e1-2c27a672ace9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "x = torch.ones((3,))\n",
        "print(x.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWjopTmRf4po"
      },
      "source": [
        "We can change this for an existing tensor using the function `requires_grad_()` (underscore indicating that this is a in-place operation). Alternatively, when creating a tensor, you can pass the argument `requires_grad=True` to most initializers we have seen above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KZeblANf4po",
        "outputId": "0e991fbf-544a-4380-e9d5-aff2846663bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "x.requires_grad_(True)\n",
        "print(x.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4l_2p7Lf4po"
      },
      "source": [
        "In order to get familiar with the concept of a computation graph, we will create one for the following function:\n",
        "\n",
        "$$y = \\frac{1}{|x|}\\sum_i \\left[(x_i + 2)^2 + 3\\right]$$\n",
        "\n",
        "You could imagine that $x$ are our parameters, and we want to optimize (either maximize or minimize) the output $y$. For this, we want to obtain the gradients $\\partial y / \\partial \\mathbf{x}$. For our example, we'll use $\\mathbf{x}=[0,1,2]$ as our input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNg-owW0f4po",
        "outputId": "4e197b87-c9e4-4557-dbff-04f508c0fbef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X tensor([0., 1., 2.], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "x = torch.arange(3, dtype=torch.float32, requires_grad=True) # Only float tensors can have gradients\n",
        "print(\"X\", x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWQfHJ6Pf4po"
      },
      "source": [
        "Now let's build the computation graph step by step. You can combine multiple operations in a single line, but we will separate them here to get a better understanding of how each operation is added to the computation graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-A1sIQvKf4pp",
        "outputId": "94eeeec0-ce24-46cd-a4f6-fc123a9fc3df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Y tensor(12.6667, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ],
      "source": [
        "a = x + 2\n",
        "b = a ** 2\n",
        "c = b + 3\n",
        "y = c.mean()\n",
        "print(\"Y\", y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDpRvqUcf4pp"
      },
      "source": [
        "Using the statements above, we have created a computation graph that looks similar to the figure below:\n",
        "\n",
        "<center style=\"width: 100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial2/pytorch_computation_graph.svg?raw=1\" width=\"200px\"></center>\n",
        "\n",
        "We calculate $a$ based on the inputs $x$ and the constant $2$, $b$ is $a$ squared, and so on. The visualization is an abstraction of the dependencies between inputs and outputs of the operations we have applied.\n",
        "Each node of the computation graph has automatically defined a function for calculating the gradients with respect to its inputs, `grad_fn`. You can see this when we printed the output tensor $y$. This is why the computation graph is usually visualized in the reverse direction (arrows point from the result to the inputs). We can perform backpropagation on the computation graph by calling the function `backward()` on the last output, which effectively calculates the gradients for each tensor that has the property `requires_grad=True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWPY8qc4f4pp"
      },
      "outputs": [],
      "source": [
        "y.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPbjfCo3f4pp"
      },
      "source": [
        "`x.grad` will now contain the gradient $\\partial y/ \\partial \\mathcal{x}$, and this gradient indicates how a change in $\\mathbf{x}$ will affect output $y$ given the current input $\\mathbf{x}=[0,1,2]$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPzx137Ef4pp",
        "outputId": "7e16579b-e5a6-470c-d8c7-94c134586706"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1.3333, 2.0000, 2.6667])\n"
          ]
        }
      ],
      "source": [
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RTa0h7Hf4pp"
      },
      "source": [
        "We can also verify these gradients by hand. We will calculate the gradients using the chain rule, in the same way as PyTorch did it:\n",
        "\n",
        "$$\\frac{\\partial y}{\\partial x_i} = \\frac{\\partial y}{\\partial c_i}\\frac{\\partial c_i}{\\partial b_i}\\frac{\\partial b_i}{\\partial a_i}\\frac{\\partial a_i}{\\partial x_i}$$\n",
        "\n",
        "Note that we have simplified this equation to index notation, and by using the fact that all operation besides the mean do not combine the elements in the tensor. The partial derivatives are:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial a_i}{\\partial x_i} = 1,\\hspace{1cm}\n",
        "\\frac{\\partial b_i}{\\partial a_i} = 2\\cdot a_i\\hspace{1cm}\n",
        "\\frac{\\partial c_i}{\\partial b_i} = 1\\hspace{1cm}\n",
        "\\frac{\\partial y}{\\partial c_i} = \\frac{1}{3}\n",
        "$$\n",
        "\n",
        "Hence, with the input being $\\mathbf{x}=[0,1,2]$, our gradients are $\\partial y/\\partial \\mathbf{x}=[4/3,2,8/3]$. The previous code cell should have printed the same result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zbc4rn0xf4pq"
      },
      "source": [
        "### GPU support\n",
        "\n",
        "A crucial feature of PyTorch is the support of GPUs, short for Graphics Processing Unit. A GPU can perform many thousands of small operations in parallel, making it very well suitable for performing large matrix operations in neural networks. When comparing GPUs to CPUs, we can list the following main differences (credit: [Kevin Krewell, 2009](https://blogs.nvidia.com/blog/2009/12/16/whats-the-difference-between-a-cpu-and-a-gpu/)) \n",
        "\n",
        "<center style=\"width: 100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial2/comparison_CPU_GPU.png?raw=1\" width=\"700px\"></center>\n",
        "\n",
        "CPUs and GPUs have both different advantages and disadvantages, which is why many computers contain both components and use them for different tasks. In case you are not familiar with GPUs, you can read up more details in this [NVIDIA blog post](https://blogs.nvidia.com/blog/2009/12/16/whats-the-difference-between-a-cpu-and-a-gpu/) or [here](https://www.intel.com/content/www/us/en/products/docs/processors/what-is-a-gpu.html). \n",
        "\n",
        "GPUs can accelerate the training of your network up to a factor of $100$ which is essential for large neural networks. PyTorch implements a lot of functionality for supporting GPUs (mostly those of NVIDIA due to the libraries [CUDA](https://developer.nvidia.com/cuda-zone) and [cuDNN](https://developer.nvidia.com/cudnn)). First, let's check whether you have a GPU available:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veRl5Ou0f4pq",
        "outputId": "f1f83494-1cd8-4c05-82d7-7e156971b373",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is the GPU available? True\n"
          ]
        }
      ],
      "source": [
        "gpu_avail = torch.cuda.is_available()\n",
        "print(f\"Is the GPU available? {gpu_avail}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZD4zswRf4pq"
      },
      "source": [
        "If you have a GPU on your computer but the command above returns False, make sure you have the correct CUDA-version installed. The `dl2022` environment comes with the CUDA 11.7, which is selected for the Lisa supercomputer. Please change it if necessary (CUDA 11.3 is currently common on Colab). On Google Colab, make sure that you have selected a GPU in your runtime setup (in the menu, check under `Runtime -> Change runtime type`). \n",
        "\n",
        "By default, all tensors you create are stored on the CPU. We can push a tensor to the GPU by using the function `.to(...)`, or `.cuda()`. However, it is often a good practice to define a `device` object in your code which points to the GPU if you have one, and otherwise to the CPU. Then, you can write your code with respect to this device object, and it allows you to run the same code on both a CPU-only system, and one with a GPU. Let's try it below. We can specify the device as follows: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Op8VMRMLf4pq",
        "outputId": "bf684143-40dc-4173-cca0-26625f0a6ea8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PS29Z7hzf4pq"
      },
      "source": [
        "Now let's create a tensor and push it to the device:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsHVFE7Df4pr",
        "outputId": "369a62e1-79d3-420b-f03c-24295f22a2a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "x = torch.zeros(2, 3)\n",
        "x = x.to(device)\n",
        "print(\"X\", x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXt-yCkFf4pr"
      },
      "source": [
        "In case you have a GPU, you should now see the attribute `device='cuda:0'` being printed next to your tensor. The zero next to cuda indicates that this is the zero-th GPU device on your computer. PyTorch also supports multi-GPU systems, but this you will only need once you have very big networks to train (if interested, see the [PyTorch documentation](https://pytorch.org/docs/stable/distributed.html#distributed-basics)). We can also compare the runtime of a large matrix multiplication on the CPU with a operation on the GPU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoPA8XVNf4pr",
        "outputId": "4a3c6e27-5f08-4a55-ca87-caaeedaa6a84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU time: 0.20694s\n",
            "GPU time: 0.00985s\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "x = torch.randn(5000, 5000)\n",
        "\n",
        "## CPU version\n",
        "start_time = time.time()\n",
        "_ = torch.matmul(x, x)\n",
        "end_time = time.time()\n",
        "print(f\"CPU time: {(end_time - start_time):6.5f}s\")\n",
        "\n",
        "## GPU version\n",
        "x = x.to(device)\n",
        "_ = torch.matmul(x, x)  # First operation to 'burn in' GPU\n",
        "# CUDA is asynchronous, so we need to use different timing functions\n",
        "start = torch.cuda.Event(enable_timing=True)\n",
        "end = torch.cuda.Event(enable_timing=True)\n",
        "start.record()\n",
        "_ = torch.matmul(x, x)\n",
        "end.record()\n",
        "torch.cuda.synchronize()  # Waits for everything to finish running on the GPU\n",
        "print(f\"GPU time: {0.001 * start.elapsed_time(end):6.5f}s\")  # Milliseconds to seconds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tx4jP7vRf4pr"
      },
      "source": [
        "Depending on the size of the operation and the CPU/GPU in your system, the speedup of this operation can be >50x. As `matmul` operations are very common in neural networks, we can already see the great benefit of training a NN on a GPU. The time estimate can be relatively noisy here because we haven't run it for multiple times. Feel free to extend this, but it also takes longer to run.\n",
        "\n",
        "When generating random numbers, the seed between CPU and GPU is not synchronized. Hence, we need to set the seed on the GPU separately to ensure a reproducible code. Note that due to different GPU architectures, running the same code on different GPUs does not guarantee the same random numbers. Still, we don't want that our code gives us a different output every time we run it on the exact same hardware. Hence, we also set the seed on the GPU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwwf2X26f4pr"
      },
      "outputs": [],
      "source": [
        "# GPU operations have a separate seed we also want to set\n",
        "if torch.cuda.is_available(): \n",
        "    torch.cuda.manual_seed(42)\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "    \n",
        "# Additionally, some operations on a GPU are implemented stochastic for efficiency\n",
        "# We want to ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqWdnNtOf4ps"
      },
      "source": [
        "## Learning by example: Positive/negative numbers\n",
        "\n",
        "If we want to build a neural network in PyTorch, we could specify all our parameters (weight matrices, bias vectors) using `Tensors` (with `requires_grad=True`), ask PyTorch to calculate the gradients and then adjust the parameters. But things can quickly get cumbersome if we have a lot of parameters. In PyTorch, there is a package called `torch.nn` that makes building neural networks more convenient. \n",
        "\n",
        "We will introduce the libraries and all additional parts you might need to train a neural network in PyTorch, using a simple example classifier on a simple problem: is the sum of two numbers even or odd? Given 2 numbers $x_1$ and $x_2$, the goal is to predict label $1$ if $x_1+x_2$ is positive and $0$ if it is negative. For simplicity, you can assume zero to be positive.\n",
        "\n",
        "(NN on the board)\n",
        "\n",
        "(Pipeline on the board)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKC3vyekf4ps"
      },
      "source": [
        "### The model\n",
        "\n",
        "The package `torch.nn` defines a series of useful classes like linear networks layers, activation functions, loss functions etc. A full list can be found [here](https://pytorch.org/docs/stable/nn.html). In case you need a certain network layer, check the documentation of the package first before writing the layer yourself as the package likely contains the code for it already. We import it below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XL_N-0eTf4ps"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4ADmYXKf4ps"
      },
      "source": [
        "Additionally to `torch.nn`, there is also `torch.nn.functional`. It contains functions that are used in network layers. This is in contrast to `torch.nn` which defines them as `nn.Modules` (more on it below), and `torch.nn` actually uses a lot of functionalities from `torch.nn.functional`. Hence, the functional package is useful in many situations, and so we import it as well here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVK8vZDzf4ps"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93zX29iNf4ps"
      },
      "source": [
        "#### nn.Module\n",
        "\n",
        "In PyTorch, a neural network is built up out of modules. Modules can contain other modules, and a neural network is considered to be a module itself as well. The basic template of a module is as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXSnSictf4pt"
      },
      "outputs": [],
      "source": [
        "class MyModule(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Some init for my module\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Function for performing the calculation of the module.\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5tqjJMaf4pt"
      },
      "source": [
        "The forward function is where the computation of the module is taken place, and is executed when you call the module (`nn = MyModule(); nn(x)`). In the init function, we usually create the parameters of the module, using `nn.Parameter`, or defining other modules that are used in the forward function. The backward calculation is done automatically, but could be overwritten as well if wanted.\n",
        "\n",
        "#### Simple classifier\n",
        "We can now make use of the pre-defined modules in the `torch.nn` package, and define our own small neural network. We will use a minimal network with a input layer, one hidden layer with tanh as activation function, and a output layer. In other words, our networks should look something like this: (look at the board in the class!)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNuu65uof4pt"
      },
      "outputs": [],
      "source": [
        "class SimpleClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, num_inputs, num_hidden, num_outputs):\n",
        "        super().__init__()\n",
        "        # Initialize the modules we need to build the network\n",
        "        self.linear1 = nn.Linear(num_inputs, num_hidden)\n",
        "        self.act_fn = nn.Tanh()\n",
        "        self.linear2 = nn.Linear(num_hidden, num_outputs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Perform the calculation of the model to determine the prediction\n",
        "        x = self.linear1(x)\n",
        "        x = self.act_fn(x)\n",
        "        output = self.linear2(x)\n",
        "        # where is softmax?\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWwD1oKPf4pt"
      },
      "source": [
        "For the examples in this notebook, we will use a tiny neural network with two input neurons and four hidden neurons. As we perform binary classification, we will use a single output neuron. Note that we do not apply a sigmoid on the output yet. This is because other functions, especially the loss, are more efficient and precise to calculate on the original outputs instead of the sigmoid output. We will discuss the detailed reason later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVAitHcPf4pt",
        "outputId": "b0d5c772-c006-4899-b1a9-7ad1e2190d2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleClassifier(\n",
            "  (linear1): Linear(in_features=2, out_features=4, bias=True)\n",
            "  (act_fn): Tanh()\n",
            "  (linear2): Linear(in_features=4, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = SimpleClassifier(num_inputs=2, num_hidden=4, num_outputs=2)\n",
        "# Printing a module shows all its submodules\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSlZ5B-Ef4pu"
      },
      "source": [
        "Printing the model lists all submodules it contains. The parameters of a module can be obtained by using its `parameters()` functions, or `named_parameters()` to get a name to each parameter object. For our small neural network, we have the following parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zxi9ivrJf4pu",
        "outputId": "d4941967-3910-4b3c-e85e-2df0f076e9a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter linear1.weight, shape torch.Size([4, 2])\n",
            "True\n",
            "Parameter linear1.bias, shape torch.Size([4])\n",
            "True\n",
            "Parameter linear2.weight, shape torch.Size([2, 4])\n",
            "True\n",
            "Parameter linear2.bias, shape torch.Size([2])\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    print(f\"Parameter {name}, shape {param.shape}\")\n",
        "    param.requires_grad_(True)\n",
        "    print(param.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAxYAHsVf4pu"
      },
      "source": [
        "Each linear layer has a weight matrix of the shape `[output, input]`, and a bias of the shape `[output]`. The tanh activation function does not have any parameters. Note that parameters are only registered for `nn.Module` objects that are direct object attributes, i.e. `self.a = ...`. If you define a list of modules, the parameters of those are not registered for the outer module and can cause some issues when you try to optimize your module. There are alternatives, like `nn.ModuleList`, `nn.ModuleDict` and `nn.Sequential`, that allow you to have different data structures of modules. We will use them in a few later tutorials and explain them there. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inqhKpMFf4pu"
      },
      "source": [
        "### The data\n",
        "\n",
        "PyTorch also provides a few functionalities to load the training and test data efficiently, summarized in the package `torch.utils.data`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8HzTj82f4pu"
      },
      "outputs": [],
      "source": [
        "import torch.utils.data as data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9SiiUIEf4pu"
      },
      "source": [
        "The data package defines two classes which are the standard interface for handling data in PyTorch: `data.Dataset`, and `data.DataLoader`. The dataset class provides an uniform interface to access the training/test data, while the data loader makes sure to efficiently load and stack the data points from the dataset into batches during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0ddhaPWf4pv"
      },
      "source": [
        "#### The dataset class\n",
        "\n",
        "The dataset class summarizes the basic functionality of a dataset in a natural way. To define a dataset in PyTorch, we simply specify two functions: `__getitem__`, and `__len__`. The get-item function has to return the $i$-th data point in the dataset, while the len function returns the size of the dataset. For the our case, we can define the dataset class as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBvp23Ldf4pv"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, size, max_range=1000):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            size - Number of data points we want to generate\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "        self.max_range = max_range\n",
        "        self._generate_sample_data()\n",
        "\n",
        "    def _generate_sample_data(self):\n",
        "        data = torch.randint(low=-self.max_range, high=self.max_range, size=(self.size, 2), dtype=torch.float32)\n",
        "        label = (data.sum(dim=1) >= 0).to(torch.float)\n",
        "\n",
        "        self.data = data\n",
        "        self.label = label\n",
        "\n",
        "    def __len__(self):\n",
        "        # Number of data point we have. Alternatively self.data.shape[0], or self.label.shape[0]\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return the idx-th data point of the dataset\n",
        "        # If we have multiple things to return (data point and label), we can return them as tuple\n",
        "        data_point = self.data[idx]\n",
        "        data_label = self.label[idx]\n",
        "        return data_point, data_label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_k7G44rf4pv"
      },
      "source": [
        "Let's try to create such a dataset and inspect it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKGbM4cEf4pv",
        "outputId": "66b70287-8151-45ee-8ef8-c827fded8aa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of dataset: 200\n",
            "Data point 0: (tensor([711., 313.]), tensor(1.))\n"
          ]
        }
      ],
      "source": [
        "dataset = CustomDataset(size=200)\n",
        "print(\"Size of dataset:\", len(dataset))\n",
        "print(\"Data point 0:\", dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRhugf2Lf4pv"
      },
      "source": [
        "To better relate to the dataset, we visualize the samples below. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1Lad39If4pw"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def visualize_samples(data, label):\n",
        "    if isinstance(data, torch.Tensor):\n",
        "        data = data.detach().numpy()\n",
        "    if isinstance(label, torch.Tensor):\n",
        "        label = label.detach().numpy()\n",
        "    data_0 = data[label == 0]\n",
        "    data_1 = data[label == 1]\n",
        "    \n",
        "    plt.figure(figsize=(4,4))\n",
        "    plt.scatter(data_0[:,0], data_0[:,1], edgecolor=\"#333\", label=\"Class 0\")\n",
        "    plt.scatter(data_1[:,0], data_1[:,1], edgecolor=\"#333\", label=\"Class 1\")\n",
        "    plt.title(\"Dataset samples\")\n",
        "    plt.ylabel(r\"$x_2$\")\n",
        "    plt.xlabel(r\"$x_1$\")\n",
        "    plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_crJDO7jf4pw",
        "outputId": "db842150-57cd-4337-eca3-68dfe0e2909e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "visualize_samples(dataset.data, dataset.label)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTqCSmzmf4pw"
      },
      "source": [
        "#### The data loader class\n",
        "\n",
        "The class `torch.utils.data.DataLoader` represents a Python iterable over a dataset with support for automatic batching, multi-process data loading and many more features. The data loader communicates with the dataset using the function `__getitem__`, and stacks its outputs as tensors over the first dimension to form a batch.\n",
        "In contrast to the dataset class, we usually don't have to define our own data loader class, but can just create an object of it with the dataset as input. Additionally, we can configure our data loader with the following input arguments (only a selection, see full list [here](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)):\n",
        "\n",
        "* `batch_size`: Number of samples to stack per batch\n",
        "* `shuffle`: If True, the data is returned in a random order. This is important during training for introducing stochasticity. \n",
        "* `num_workers`: Number of subprocesses to use for data loading. The default, 0, means that the data will be loaded in the main process which can slow down training for datasets where loading a data point takes a considerable amount of time (e.g. large images). More workers are recommended for those, but can cause issues on Windows computers. For tiny datasets as ours, 0 workers are usually faster.\n",
        "* `pin_memory`: If True, the data loader will copy Tensors into CUDA pinned memory before returning them. This can save some time for large data points on GPUs. Usually a good practice to use for a training set, but not necessarily for validation and test to save memory on the GPU.\n",
        "* `drop_last`: If True, the last batch is dropped in case it is smaller than the specified batch size. This occurs when the dataset size is not a multiple of the batch size. Only potentially helpful during training to keep a consistent batch size.\n",
        "\n",
        "Let's create a simple data loader below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_lmpulhf4pw"
      },
      "outputs": [],
      "source": [
        "data_loader = data.DataLoader(dataset, batch_size=8, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dwj2E2D9f4pw",
        "outputId": "1517586f-06b0-479d-d7da-7da7fe8d2550",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data inputs torch.Size([8, 2]) \n",
            " tensor([[-682.,  708.],\n",
            "        [-807., -908.],\n",
            "        [-364.,  973.],\n",
            "        [-462.,  209.],\n",
            "        [-146., -183.],\n",
            "        [ -92.,  798.],\n",
            "        [ 470.,  157.],\n",
            "        [ 775.,  521.]])\n",
            "Data labels torch.Size([8]) \n",
            " tensor([1., 0., 1., 0., 0., 1., 1., 1.])\n"
          ]
        }
      ],
      "source": [
        "# next(iter(...)) catches the first batch of the data loader\n",
        "# If shuffle is True, this will return a different batch every time we run this cell\n",
        "# For iterating over the whole dataset, we can simple use \"for batch in data_loader: ...\"\n",
        "data_inputs, data_labels = next(iter(data_loader))\n",
        "\n",
        "# The shape of the outputs are [batch_size, d_1,...,d_N] where d_1,...,d_N are the \n",
        "# dimensions of the data point returned from the dataset class\n",
        "print(\"Data inputs\", data_inputs.shape, \"\\n\", data_inputs)\n",
        "print(\"Data labels\", data_labels.shape, \"\\n\", data_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evn-ByFmf4px"
      },
      "source": [
        "### Optimization\n",
        "\n",
        "After defining the model and the dataset, it is time to prepare the optimization of the model. During training, we will perform the following steps:\n",
        "\n",
        "1. Get a batch from the data loader\n",
        "2. Obtain the predictions from the model for the batch\n",
        "3. Calculate the loss based on the difference between predictions and labels\n",
        "4. Backpropagation: calculate the gradients for every parameter with respect to the loss\n",
        "5. Update the parameters of the model in the direction of the gradients\n",
        "\n",
        "We have seen how we can do step 1, 2 and 4 in PyTorch. Now, we will look at step 3 and 5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVZDRXUCf4px"
      },
      "source": [
        "#### Loss modules\n",
        "\n",
        "We can calculate the loss for a batch by simply performing a few tensor operations as those are automatically added to the computation graph. For instance, for classification, we can use Cross Entropy loss. PyTorch already provides a list of predefined loss functions which we can use (see [here](https://pytorch.org/docs/stable/nn.html#loss-functions) for a full list). For instance, for CE, PyTorch has: `nn.CrossEntropyLoss()`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOcMDcoxf4px"
      },
      "outputs": [],
      "source": [
        "loss_module = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7cQS2nBf4px"
      },
      "source": [
        "#### Stochastic Gradient Descent\n",
        "\n",
        "For updating the parameters, PyTorch provides the package `torch.optim` that has most popular optimizers implemented. We will discuss the specific optimizers and their differences later in the course, but will for now use the simplest of them: `torch.optim.SGD`. Stochastic Gradient Descent updates parameters by multiplying the gradients with a small constant, called learning rate, and subtracting those from the parameters (hence minimizing the loss). Therefore, we slowly move towards the direction of minimizing the loss. A good default value of the learning rate for a small network as ours is 0.1. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UT8gj6Ef4py"
      },
      "outputs": [],
      "source": [
        "# Input to the optimizer are the parameters of the model: model.parameters()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMgBLCbFf4py"
      },
      "source": [
        "The optimizer provides two useful functions: `optimizer.step()`, and `optimizer.zero_grad()`. The step function updates the parameters based on the gradients as explained above. The function `optimizer.zero_grad()` sets the gradients of all parameters to zero. While this function seems less relevant at first, it is a crucial pre-step before performing backpropagation. If we call the `backward` function on the loss while the parameter gradients are non-zero from the previous batch, the new gradients would actually be added to the previous ones instead of overwriting them. This is done because a parameter might occur multiple times in a computation graph, and we need to sum the gradients in this case instead of replacing them. Hence, remember to call `optimizer.zero_grad()` before calculating the gradients of a batch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haEMWqhef4py"
      },
      "source": [
        "### Training\n",
        "\n",
        "Finally, we are ready to train our model. As a first step, we create a slightly larger dataset and specify a data loader with a larger batch size. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtuK6rQzf4py"
      },
      "outputs": [],
      "source": [
        "train_dataset = CustomDataset(size=2500)\n",
        "train_data_loader = data.DataLoader(train_dataset, batch_size=128, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7BqbKNNf4pz"
      },
      "source": [
        "Now, we can write a small training function. Remember our five steps: load a batch, obtain the predictions, calculate the loss, backpropagate, and update. Additionally, we have to push all data and model parameters to the device of our choice (GPU if available). For the tiny neural network we have, communicating the data to the GPU actually takes much more time than we could save from running the operation on GPU. For large networks, the communication time is significantly smaller than the actual runtime making a GPU crucial in these cases. Still, to practice, we will push the data to GPU here. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9TrMBc-f4pz",
        "outputId": "712744d1-972a-413c-ad4b-2bbbcac9dcdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimpleClassifier(\n",
              "  (linear1): Linear(in_features=2, out_features=4, bias=True)\n",
              "  (act_fn): Tanh()\n",
              "  (linear2): Linear(in_features=4, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# Push model to device. Has to be only done once\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQelTF91f4pz"
      },
      "source": [
        "In addition, we set our model to training mode. This is done by calling `model.train()`. There exist certain modules that need to perform a different forward step during training than during testing (e.g. BatchNorm and Dropout), and we can switch between them using `model.train()` and `model.eval()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEm13Hx0f4pz"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(model, optimizer, data_loader, loss_module, num_epochs=100):\n",
        "    # Set model to train mode\n",
        "    model.train() \n",
        "    \n",
        "    # Training loop\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "        for idx, (data_inputs, data_labels) in enumerate(data_loader):\n",
        "            # Before calculating the gradients, we need to ensure that they are all zero. \n",
        "            # The gradients would not be overwritten, but actually added to the existing ones.\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            ## Move input data to device (only strictly necessary if we use GPU)\n",
        "            data_inputs = data_inputs.to(device)\n",
        "            data_labels = data_labels.to(device)\n",
        "            \n",
        "            ## Run the model on the input data\n",
        "            preds = model(data_inputs)\n",
        "\n",
        "            ## Calculate the loss\n",
        "            loss = loss_module(preds, data_labels.to(torch.int64))\n",
        "            print(f' |{idx:5d}/{len(data_loader):5d} batches | loss: {loss:.3}')\n",
        " \n",
        "            # Perform backpropagation\n",
        "            loss.backward()\n",
        "            \n",
        "            ## Update the parameters\n",
        "            optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fupBvqeZf4pz",
        "outputId": "1dcb840f-f429-4179-ce1a-b97dd586f8c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 4/100 [00:02<00:52,  1.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " |    0/   20 batches | loss: 0.617\n",
            " |    1/   20 batches | loss: 0.692\n",
            " |    2/   20 batches | loss: 0.622\n",
            " |    3/   20 batches | loss: 0.594\n",
            " |    4/   20 batches | loss: 0.682\n",
            " |    5/   20 batches | loss: 0.62\n",
            " |    6/   20 batches | loss: 0.703\n",
            " |    7/   20 batches | loss: 0.634\n",
            " |    8/   20 batches | loss: 0.762\n",
            " |    9/   20 batches | loss: 0.685\n",
            " |   10/   20 batches | loss: 0.759\n",
            " |   11/   20 batches | loss: 0.503\n",
            " |   12/   20 batches | loss: 0.654\n",
            " |   13/   20 batches | loss: 0.69\n",
            " |   14/   20 batches | loss: 0.654\n",
            " |   15/   20 batches | loss: 0.665\n",
            " |   16/   20 batches | loss: 0.628\n",
            " |   17/   20 batches | loss: 0.659\n",
            " |   18/   20 batches | loss: 0.693\n",
            " |   19/   20 batches | loss: 0.632\n",
            " |    0/   20 batches | loss: 0.703\n",
            " |    1/   20 batches | loss: 0.627\n",
            " |    2/   20 batches | loss: 0.624\n",
            " |    3/   20 batches | loss: 0.568\n",
            " |    4/   20 batches | loss: 0.607\n",
            " |    5/   20 batches | loss: 0.681\n",
            " |    6/   20 batches | loss: 0.629\n",
            " |    7/   20 batches | loss: 0.565\n",
            " |    8/   20 batches | loss: 0.623\n",
            " |    9/   20 batches | loss: 0.675\n",
            " |   10/   20 batches | loss: 0.642\n",
            " |   11/   20 batches | loss: 0.68\n",
            " |   12/   20 batches | loss: 0.569\n",
            " |   13/   20 batches | loss: 0.635\n",
            " |   14/   20 batches | loss: 0.677\n",
            " |   15/   20 batches | loss: 0.613\n",
            " |   16/   20 batches | loss: 0.606\n",
            " |   17/   20 batches | loss: 0.591\n",
            " |   18/   20 batches | loss: 0.664\n",
            " |   19/   20 batches | loss: 0.57\n",
            " |    0/   20 batches | loss: 0.573\n",
            " |    1/   20 batches | loss: 0.645\n",
            " |    2/   20 batches | loss: 0.636\n",
            " |    3/   20 batches | loss: 0.693\n",
            " |    4/   20 batches | loss: 0.619\n",
            " |    5/   20 batches | loss: 0.56\n",
            " |    6/   20 batches | loss: 0.564\n",
            " |    7/   20 batches | loss: 0.601\n",
            " |    8/   20 batches | loss: 0.605\n",
            " |    9/   20 batches | loss: 0.622\n",
            " |   10/   20 batches | loss: 0.6\n",
            " |   11/   20 batches | loss: 0.557\n",
            " |   12/   20 batches | loss: 0.659\n",
            " |   13/   20 batches | loss: 0.536\n",
            " |   14/   20 batches | loss: 0.645\n",
            " |   15/   20 batches | loss: 0.444\n",
            " |   16/   20 batches | loss: 0.56\n",
            " |   17/   20 batches | loss: 0.594\n",
            " |   18/   20 batches | loss: 0.573\n",
            " |   19/   20 batches | loss: 0.572\n",
            " |    0/   20 batches | loss: 0.521\n",
            " |    1/   20 batches | loss: 0.547\n",
            " |    2/   20 batches | loss: 0.558\n",
            " |    3/   20 batches | loss: 0.588\n",
            " |    4/   20 batches | loss: 0.651\n",
            " |    5/   20 batches | loss: 0.561\n",
            " |    6/   20 batches | loss: 0.621\n",
            " |    7/   20 batches | loss: 0.553\n",
            " |    8/   20 batches | loss: 0.56\n",
            " |    9/   20 batches | loss: 0.53\n",
            " |   10/   20 batches | loss: 0.572\n",
            " |   11/   20 batches | loss: 0.483\n",
            " |   12/   20 batches | loss: 0.567\n",
            " |   13/   20 batches | loss: 0.642\n",
            " |   14/   20 batches | loss: 0.58\n",
            " |   15/   20 batches | loss: 0.581\n",
            " |   16/   20 batches | loss: 0.489\n",
            " |   17/   20 batches | loss: 0.591\n",
            " |   18/   20 batches | loss: 0.613\n",
            " |   19/   20 batches | loss: 0.62\n",
            " |    0/   20 batches | loss: 0.557\n",
            " |    1/   20 batches | loss: 0.52\n",
            " |    2/   20 batches | loss: 0.588\n",
            " |    3/   20 batches | loss: 0.531\n",
            " |    4/   20 batches | loss: 0.524\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 7/100 [00:02<00:25,  3.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " |    5/   20 batches | loss: 0.522\n",
            " |    6/   20 batches | loss: 0.563\n",
            " |    7/   20 batches | loss: 0.533\n",
            " |    8/   20 batches | loss: 0.595\n",
            " |    9/   20 batches | loss: 0.537\n",
            " |   10/   20 batches | loss: 0.548\n",
            " |   11/   20 batches | loss: 0.588\n",
            " |   12/   20 batches | loss: 0.565\n",
            " |   13/   20 batches | loss: 0.595\n",
            " |   14/   20 batches | loss: 0.562\n",
            " |   15/   20 batches | loss: 0.503\n",
            " |   16/   20 batches | loss: 0.515\n",
            " |   17/   20 batches | loss: 0.544\n",
            " |   18/   20 batches | loss: 0.553\n",
            " |   19/   20 batches | loss: 0.582\n",
            " |    0/   20 batches | loss: 0.641\n",
            " |    1/   20 batches | loss: 0.528\n",
            " |    2/   20 batches | loss: 0.516\n",
            " |    3/   20 batches | loss: 0.55\n",
            " |    4/   20 batches | loss: 0.488\n",
            " |    5/   20 batches | loss: 0.494\n",
            " |    6/   20 batches | loss: 0.488\n",
            " |    7/   20 batches | loss: 0.546\n",
            " |    8/   20 batches | loss: 0.54\n",
            " |    9/   20 batches | loss: 0.599\n",
            " |   10/   20 batches | loss: 0.529\n",
            " |   11/   20 batches | loss: 0.578\n",
            " |   12/   20 batches | loss: 0.56\n",
            " |   13/   20 batches | loss: 0.484\n",
            " |   14/   20 batches | loss: 0.533\n",
            " |   15/   20 batches | loss: 0.472\n",
            " |   16/   20 batches | loss: 0.552\n",
            " |   17/   20 batches | loss: 0.475\n",
            " |   18/   20 batches | loss: 0.491\n",
            " |   19/   20 batches | loss: 0.455\n",
            " |    0/   20 batches | loss: 0.474\n",
            " |    1/   20 batches | loss: 0.468\n",
            " |    2/   20 batches | loss: 0.505\n",
            " |    3/   20 batches | loss: 0.523\n",
            " |    4/   20 batches | loss: 0.463\n",
            " |    5/   20 batches | loss: 0.448\n",
            " |    6/   20 batches | loss: 0.575\n",
            " |    7/   20 batches | loss: 0.462\n",
            " |    8/   20 batches | loss: 0.471\n",
            " |    9/   20 batches | loss: 0.505\n",
            " |   10/   20 batches | loss: 0.542\n",
            " |   11/   20 batches | loss: 0.516\n",
            " |   12/   20 batches | loss: 0.49\n",
            " |   13/   20 batches | loss: 0.492\n",
            " |   14/   20 batches | loss: 0.522\n",
            " |   15/   20 batches | loss: 0.509\n",
            " |   16/   20 batches | loss: 0.483\n",
            " |   17/   20 batches | loss: 0.426\n",
            " |   18/   20 batches | loss: 0.515\n",
            " |   19/   20 batches | loss: 0.479\n",
            " |    0/   20 batches | loss: 0.526\n",
            " |    1/   20 batches | loss: 0.519\n",
            " |    2/   20 batches | loss: 0.466\n",
            " |    3/   20 batches | loss: 0.424\n",
            " |    4/   20 batches | loss: 0.455\n",
            " |    5/   20 batches | loss: 0.499\n",
            " |    6/   20 batches | loss: 0.447\n",
            " |    7/   20 batches | loss: 0.472\n",
            " |    8/   20 batches | loss: 0.532\n",
            " |    9/   20 batches | loss: 0.456\n",
            " |   10/   20 batches | loss: 0.471\n",
            " |   11/   20 batches | loss: 0.464\n",
            " |   12/   20 batches | loss: 0.462\n",
            " |   13/   20 batches | loss: 0.436\n",
            " |   14/   20 batches | loss: 0.468\n",
            " |   15/   20 batches | loss: 0.45\n",
            " |   16/   20 batches | loss: 0.41\n",
            " |   17/   20 batches | loss: 0.416\n",
            " |   18/   20 batches | loss: 0.427\n",
            " |   19/   20 batches | loss: 0.459\n",
            " |    0/   20 batches | loss: 0.396\n",
            " |    1/   20 batches | loss: 0.432\n",
            " |    2/   20 batches | loss: 0.404\n",
            " |    3/   20 batches | loss: 0.462\n",
            " |    4/   20 batches | loss: 0.472\n",
            " |    5/   20 batches | loss: 0.386\n",
            " |    6/   20 batches | loss: 0.407\n",
            " |    7/   20 batches | loss: 0.444\n",
            " |    8/   20 batches | loss: 0.41\n",
            " |    9/   20 batches | loss: 0.384\n",
            " |   10/   20 batches | loss: 0.424\n",
            " |   11/   20 batches | loss: 0.454\n",
            " |   12/   20 batches | loss: 0.407\n",
            " |   13/   20 batches | loss: 0.419\n",
            " |   14/   20 batches | loss: 0.421\n",
            " |   15/   20 batches | loss: 0.426\n",
            " |   16/   20 batches | loss: 0.444\n",
            " |   17/   20 batches | loss: 0.4\n",
            " |   18/   20 batches | loss: 0.45\n",
            " |   19/   20 batches | loss: 0.422\n",
            " |    0/   20 batches | loss: 0.453\n",
            " |    1/   20 batches | loss: 0.388\n",
            " |    2/   20 batches | loss: 0.405\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 13/100 [00:03<00:10,  8.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " |    3/   20 batches | loss: 0.403\n",
            " |    4/   20 batches | loss: 0.416\n",
            " |    5/   20 batches | loss: 0.455\n",
            " |    6/   20 batches | loss: 0.422\n",
            " |    7/   20 batches | loss: 0.398\n",
            " |    8/   20 batches | loss: 0.397\n",
            " |    9/   20 batches | loss: 0.418\n",
            " |   10/   20 batches | loss: 0.389\n",
            " |   11/   20 batches | loss: 0.41\n",
            " |   12/   20 batches | loss: 0.459\n",
            " |   13/   20 batches | loss: 0.414\n",
            " |   14/   20 batches | loss: 0.454\n",
            " |   15/   20 batches | loss: 0.406\n",
            " |   16/   20 batches | loss: 0.395\n",
            " |   17/   20 batches | loss: 0.4\n",
            " |   18/   20 batches | loss: 0.369\n",
            " |   19/   20 batches | loss: 0.41\n",
            " |    0/   20 batches | loss: 0.387\n",
            " |    1/   20 batches | loss: 0.392\n",
            " |    2/   20 batches | loss: 0.404\n",
            " |    3/   20 batches | loss: 0.409\n",
            " |    4/   20 batches | loss: 0.415\n",
            " |    5/   20 batches | loss: 0.432\n",
            " |    6/   20 batches | loss: 0.395\n",
            " |    7/   20 batches | loss: 0.401\n",
            " |    8/   20 batches | loss: 0.404\n",
            " |    9/   20 batches | loss: 0.382\n",
            " |   10/   20 batches | loss: 0.392\n",
            " |   11/   20 batches | loss: 0.406\n",
            " |   12/   20 batches | loss: 0.438\n",
            " |   13/   20 batches | loss: 0.382\n",
            " |   14/   20 batches | loss: 0.43\n",
            " |   15/   20 batches | loss: 0.417\n",
            " |   16/   20 batches | loss: 0.384\n",
            " |   17/   20 batches | loss: 0.369\n",
            " |   18/   20 batches | loss: 0.435\n",
            " |   19/   20 batches | loss: 0.37\n",
            " |    0/   20 batches | loss: 0.408\n",
            " |    1/   20 batches | loss: 0.427\n",
            " |    2/   20 batches | loss: 0.384\n",
            " |    3/   20 batches | loss: 0.414\n",
            " |    4/   20 batches | loss: 0.369\n",
            " |    5/   20 batches | loss: 0.384\n",
            " |    6/   20 batches | loss: 0.376\n",
            " |    7/   20 batches | loss: 0.384\n",
            " |    8/   20 batches | loss: 0.361\n",
            " |    9/   20 batches | loss: 0.383\n",
            " |   10/   20 batches | loss: 0.447\n",
            " |   11/   20 batches | loss: 0.385\n",
            " |   12/   20 batches | loss: 0.384\n",
            " |   13/   20 batches | loss: 0.382\n",
            " |   14/   20 batches | loss: 0.375\n",
            " |   15/   20 batches | loss: 0.423\n",
            " |   16/   20 batches | loss: 0.377\n",
            " |   17/   20 batches | loss: 0.405\n",
            " |   18/   20 batches | loss: 0.406\n",
            " |   19/   20 batches | loss: 0.369\n",
            " |    0/   20 batches | loss: 0.431\n",
            " |    1/   20 batches | loss: 0.392\n",
            " |    2/   20 batches | loss: 0.394\n",
            " |    3/   20 batches | loss: 0.38\n",
            " |    4/   20 batches | loss: 0.363\n",
            " |    5/   20 batches | loss: 0.384\n",
            " |    6/   20 batches | loss: 0.358\n",
            " |    7/   20 batches | loss: 0.39\n",
            " |    8/   20 batches | loss: 0.39\n",
            " |    9/   20 batches | loss: 0.371\n",
            " |   10/   20 batches | loss: 0.363\n",
            " |   11/   20 batches | loss: 0.4\n",
            " |   12/   20 batches | loss: 0.402\n",
            " |   13/   20 batches | loss: 0.374\n",
            " |   14/   20 batches | loss: 0.394\n",
            " |   15/   20 batches | loss: 0.369\n",
            " |   16/   20 batches | loss: 0.387\n",
            " |   17/   20 batches | loss: 0.376\n",
            " |   18/   20 batches | loss: 0.401\n",
            " |   19/   20 batches | loss: 0.336\n",
            " |    0/   20 batches | loss: 0.393\n",
            " |    1/   20 batches | loss: 0.385\n",
            " |    2/   20 batches | loss: 0.389\n",
            " |    3/   20 batches | loss: 0.409\n",
            " |    4/   20 batches | loss: 0.339\n",
            " |    5/   20 batches | loss: 0.387\n",
            " |    6/   20 batches | loss: 0.374\n",
            " |    7/   20 batches | loss: 0.376\n",
            " |    8/   20 batches | loss: 0.403\n",
            " |    9/   20 batches | loss: 0.361\n",
            " |   10/   20 batches | loss: 0.359\n",
            " |   11/   20 batches | loss: 0.365\n",
            " |   12/   20 batches | loss: 0.374\n",
            " |   13/   20 batches | loss: 0.381\n",
            " |   14/   20 batches | loss: 0.343\n",
            " |   15/   20 batches | loss: 0.379\n",
            " |   16/   20 batches | loss: 0.362\n",
            " |   17/   20 batches | loss: 0.349\n",
            " |   18/   20 batches | loss: 0.378\n",
            " |   19/   20 batches | loss: 0.427\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 19%|█▉        | 19/100 [00:03<00:06, 13.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " |    0/   20 batches | loss: 0.4\n",
            " |    1/   20 batches | loss: 0.359\n",
            " |    2/   20 batches | loss: 0.337\n",
            " |    3/   20 batches | loss: 0.354\n",
            " |    4/   20 batches | loss: 0.346\n",
            " |    5/   20 batches | loss: 0.378\n",
            " |    6/   20 batches | loss: 0.353\n",
            " |    7/   20 batches | loss: 0.351\n",
            " |    8/   20 batches | loss: 0.415\n",
            " |    9/   20 batches | loss: 0.384\n",
            " |   10/   20 batches | loss: 0.39\n",
            " |   11/   20 batches | loss: 0.362\n",
            " |   12/   20 batches | loss: 0.356\n",
            " |   13/   20 batches | loss: 0.354\n",
            " |   14/   20 batches | loss: 0.386\n",
            " |   15/   20 batches | loss: 0.372\n",
            " |   16/   20 batches | loss: 0.368\n",
            " |   17/   20 batches | loss: 0.34\n",
            " |   18/   20 batches | loss: 0.366\n",
            " |   19/   20 batches | loss: 0.386\n",
            " |    0/   20 batches | loss: 0.354\n",
            " |    1/   20 batches | loss: 0.356\n",
            " |    2/   20 batches | loss: 0.384\n",
            " |    3/   20 batches | loss: 0.37\n",
            " |    4/   20 batches | loss: 0.351\n",
            " |    5/   20 batches | loss: 0.359\n",
            " |    6/   20 batches | loss: 0.377\n",
            " |    7/   20 batches | loss: 0.375\n",
            " |    8/   20 batches | loss: 0.357\n",
            " |    9/   20 batches | loss: 0.33\n",
            " |   10/   20 batches | loss: 0.366\n",
            " |   11/   20 batches | loss: 0.339\n",
            " |   12/   20 batches | loss: 0.358\n",
            " |   13/   20 batches | loss: 0.339\n",
            " |   14/   20 batches | loss: 0.373\n",
            " |   15/   20 batches | loss: 0.347\n",
            " |   16/   20 batches | loss: 0.372\n",
            " |   17/   20 batches | loss: 0.346\n",
            " |   18/   20 batches | loss: 0.363\n",
            " |   19/   20 batches | loss: 0.362\n",
            " |    0/   20 batches | loss: 0.342\n",
            " |    1/   20 batches | loss: 0.369\n",
            " |    2/   20 batches | loss: 0.356\n",
            " |    3/   20 batches | loss: 0.34\n",
            " |    4/   20 batches | loss: 0.36\n",
            " |    5/   20 batches | loss: 0.319\n",
            " |    6/   20 batches | loss: 0.339\n",
            " |    7/   20 batches | loss: 0.375\n",
            " |    8/   20 batches | loss: 0.344\n",
            " |    9/   20 batches | loss: 0.377\n",
            " |   10/   20 batches | loss: 0.349\n",
            " |   11/   20 batches | loss: 0.371\n",
            " |   12/   20 batches | loss: 0.355\n",
            " |   13/   20 batches | loss: 0.365\n",
            " |   14/   20 batches | loss: 0.32\n",
            " |   15/   20 batches | loss: 0.341\n",
            " |   16/   20 batches | loss: 0.324\n",
            " |   17/   20 batches | loss: 0.355\n",
            " |   18/   20 batches | loss: 0.36\n",
            " |   19/   20 batches | loss: 0.372\n",
            " |    0/   20 batches | loss: 0.354\n",
            " |    1/   20 batches | loss: 0.34\n",
            " |    2/   20 batches | loss: 0.307\n",
            " |    3/   20 batches | loss: 0.32\n",
            " |    4/   20 batches | loss: 0.347\n",
            " |    5/   20 batches | loss: 0.367\n",
            " |    6/   20 batches | loss: 0.354\n",
            " |    7/   20 batches | loss: 0.349\n",
            " |    8/   20 batches | loss: 0.336\n",
            " |    9/   20 batches | loss: 0.358\n",
            " |   10/   20 batches | loss: 0.375\n",
            " |   11/   20 batches | loss: 0.36\n",
            " |   12/   20 batches | loss: 0.342\n",
            " |   13/   20 batches | loss: 0.334\n",
            " |   14/   20 batches | loss: 0.336\n",
            " |   15/   20 batches | loss: 0.343\n",
            " |   16/   20 batches | loss: 0.355\n",
            " |   17/   20 batches | loss: 0.324\n",
            " |   18/   20 batches | loss: 0.336\n",
            " |   19/   20 batches | loss: 0.35\n",
            " |    0/   20 batches | loss: 0.342\n",
            " |    1/   20 batches | loss: 0.337\n",
            " |    2/   20 batches | loss: 0.346\n",
            " |    3/   20 batches | loss: 0.343\n",
            " |    4/   20 batches | loss: 0.367\n",
            " |    5/   20 batches | loss: 0.364\n",
            " |    6/   20 batches | loss: 0.318\n",
            " |    7/   20 batches | loss: 0.313\n",
            " |    8/   20 batches | loss: 0.321\n",
            " |    9/   20 batches | loss: 0.327\n",
            " |   10/   20 batches | loss: 0.349\n",
            " |   11/   20 batches | loss: 0.367\n",
            " |   12/   20 batches | loss: 0.338\n",
            " |   13/   20 batches | loss: 0.331\n",
            " |   14/   20 batches | loss: 0.35\n",
            " |   15/   20 batches | loss: 0.324\n",
            " |   16/   20 batches | loss: 0.334\n",
            " |   17/   20 batches | loss: 0.332\n",
            " |   18/   20 batches | loss: 0.343\n",
            " |   19/   20 batches | loss: 0.312\n",
            " |    0/   20 batches | loss: 0.339\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|██▏       | 22/100 [00:03<00:05, 15.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " |    1/   20 batches | loss: 0.34\n",
            " |    2/   20 batches | loss: 0.353\n",
            " |    3/   20 batches | loss: 0.322\n",
            " |    4/   20 batches | loss: 0.346\n",
            " |    5/   20 batches | loss: 0.342\n",
            " |    6/   20 batches | loss: 0.329\n",
            " |    7/   20 batches | loss: 0.357\n",
            " |    8/   20 batches | loss: 0.316\n",
            " |    9/   20 batches | loss: 0.364\n",
            " |   10/   20 batches | loss: 0.306\n",
            " |   11/   20 batches | loss: 0.323\n",
            " |   12/   20 batches | loss: 0.312\n",
            " |   13/   20 batches | loss: 0.306\n",
            " |   14/   20 batches | loss: 0.344\n",
            " |   15/   20 batches | loss: 0.284\n",
            " |   16/   20 batches | loss: 0.34\n",
            " |   17/   20 batches | loss: 0.297\n",
            " |   18/   20 batches | loss: 0.356\n",
            " |   19/   20 batches | loss: 0.35\n",
            " |    0/   20 batches | loss: 0.295\n",
            " |    1/   20 batches | loss: 0.317\n",
            " |    2/   20 batches | loss: 0.329\n",
            " |    3/   20 batches | loss: 0.302\n",
            " |    4/   20 batches | loss: 0.346\n",
            " |    5/   20 batches | loss: 0.331\n",
            " |    6/   20 batches | loss: 0.319\n",
            " |    7/   20 batches | loss: 0.314\n",
            " |    8/   20 batches | loss: 0.373\n",
            " |    9/   20 batches | loss: 0.319\n",
            " |   10/   20 batches | loss: 0.323\n",
            " |   11/   20 batches | loss: 0.329\n",
            " |   12/   20 batches | loss: 0.329\n",
            " |   13/   20 batches | loss: 0.306\n",
            " |   14/   20 batches | loss: 0.336\n",
            " |   15/   20 batches | loss: 0.305\n",
            " |   16/   20 batches | loss: 0.346\n",
            " |   17/   20 batches | loss: 0.332\n",
            " |   18/   20 batches | loss: 0.341\n",
            " |   19/   20 batches | loss: 0.304\n",
            " |    0/   20 batches | loss: 0.32\n",
            " |    1/   20 batches | loss: 0.31\n",
            " |    2/   20 batches | loss: 0.322\n",
            " |    3/   20 batches | loss: 0.332\n",
            " |    4/   20 batches | loss: 0.326\n",
            " |    5/   20 batches | loss: 0.322\n",
            " |    6/   20 batches | loss: 0.314\n",
            " |    7/   20 batches | loss: 0.341\n",
            " |    8/   20 batches | loss: 0.321\n",
            " |    9/   20 batches | loss: 0.342\n",
            " |   10/   20 batches | loss: 0.336\n",
            " |   11/   20 batches | loss: 0.295\n",
            " |   12/   20 batches | loss: 0.336\n",
            " |   13/   20 batches | loss: 0.312\n",
            " |   14/   20 batches | loss: 0.306\n",
            " |   15/   20 batches | loss: 0.31\n",
            " |   16/   20 batches | loss: 0.305\n",
            " |   17/   20 batches | loss: 0.307\n",
            " |   18/   20 batches | loss: 0.324\n",
            " |   19/   20 batches | loss: 0.311\n",
            " |    0/   20 batches | loss: 0.321\n",
            " |    1/   20 batches | loss: 0.324\n",
            " |    2/   20 batches | loss: 0.343\n",
            " |    3/   20 batches | loss: 0.318\n",
            " |    4/   20 batches | loss: 0.313\n",
            " |    5/   20 batches | loss: 0.317\n",
            " |    6/   20 batches | loss: 0.292\n",
            " |    7/   20 batches | loss: 0.282\n",
            " |    8/   20 batches | loss: 0.307\n",
            " |    9/   20 batches | loss: 0.319\n",
            " |   10/   20 batches | loss: 0.301\n",
            " |   11/   20 batches | loss: 0.298\n",
            " |   12/   20 batches | loss: 0.294\n",
            " |   13/   20 batches | loss: 0.354\n",
            " |   14/   20 batches | loss: 0.308\n",
            " |   15/   20 batches | loss: 0.32\n",
            " |   16/   20 batches | loss: 0.304\n",
            " |   17/   20 batches | loss: 0.322\n",
            " |   18/   20 batches | loss: 0.298\n",
            " |   19/   20 batches | loss: 0.333\n",
            " |    0/   20 batches | loss: 0.287\n",
            " |    1/   20 batches | loss: 0.31\n",
            " |    2/   20 batches | loss: 0.313\n",
            " |    3/   20 batches | loss: 0.312\n",
            " |    4/   20 batches | loss: 0.319\n",
            " |    5/   20 batches | loss: 0.305\n",
            " |    6/   20 batches | loss: 0.34\n",
            " |    7/   20 batches | loss: 0.289\n",
            " |    8/   20 batches | loss: 0.296\n",
            " |    9/   20 batches | loss: 0.312\n",
            " |   10/   20 batches | loss: 0.292\n",
            " |   11/   20 batches | loss: 0.316\n",
            " |   12/   20 batches | loss: 0.305\n",
            " |   13/   20 batches | loss: 0.307\n",
            " |   14/   20 batches | loss: 0.307\n",
            " |   15/   20 batches | loss: 0.299\n",
            " |   16/   20 batches | loss: 0.297\n",
            " |   17/   20 batches | loss: 0.306\n",
            " |   18/   20 batches | loss: 0.31\n",
            " |   19/   20 batches | loss: 0.314\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 28%|██▊       | 28/100 [00:03<00:03, 19.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " |    0/   20 batches | loss: 0.287\n",
            " |    1/   20 batches | loss: 0.306\n",
            " |    2/   20 batches | loss: 0.282\n",
            " |    3/   20 batches | loss: 0.297\n",
            " |    4/   20 batches | loss: 0.294\n",
            " |    5/   20 batches | loss: 0.314\n",
            " |    6/   20 batches | loss: 0.293\n",
            " |    7/   20 batches | loss: 0.307\n",
            " |    8/   20 batches | loss: 0.314\n",
            " |    9/   20 batches | loss: 0.303\n",
            " |   10/   20 batches | loss: 0.323\n",
            " |   11/   20 batches | loss: 0.306\n",
            " |   12/   20 batches | loss: 0.298\n",
            " |   13/   20 batches | loss: 0.305\n",
            " |   14/   20 batches | loss: 0.296\n",
            " |   15/   20 batches | loss: 0.282\n",
            " |   16/   20 batches | loss: 0.319\n",
            " |   17/   20 batches | loss: 0.312\n",
            " |   18/   20 batches | loss: 0.281\n",
            " |   19/   20 batches | loss: 0.336\n",
            " |    0/   20 batches | loss: 0.293\n",
            " |    1/   20 batches | loss: 0.3\n",
            " |    2/   20 batches | loss: 0.312\n",
            " |    3/   20 batches | loss: 0.282\n",
            " |    4/   20 batches | loss: 0.29\n",
            " |    5/   20 batches | loss: 0.277\n",
            " |    6/   20 batches | loss: 0.308\n",
            " |    7/   20 batches | loss: 0.306\n",
            " |    8/   20 batches | loss: 0.282\n",
            " |    9/   20 batches | loss: 0.303\n",
            " |   10/   20 batches | loss: 0.318\n",
            " |   11/   20 batches | loss: 0.273\n",
            " |   12/   20 batches | loss: 0.303\n",
            " |   13/   20 batches | loss: 0.296\n",
            " |   14/   20 batches | loss: 0.28\n",
            " |   15/   20 batches | loss: 0.286\n",
            " |   16/   20 batches | loss: 0.291\n",
            " |   17/   20 batches | loss: 0.304\n",
            " |   18/   20 batches | loss: 0.295\n",
            " |   19/   20 batches | loss: 0.321\n",
            " |    0/   20 batches | loss: 0.296\n",
            " |    1/   20 batches | loss: 0.284\n",
            " |    2/   20 batches | loss: 0.29\n",
            " |    3/   20 batches | loss: 0.302\n",
            " |    4/   20 batches | loss: 0.288\n",
            " |    5/   20 batches | loss: 0.286\n",
            " |    6/   20 batches | loss: 0.244\n",
            " |    7/   20 batches | loss: 0.285\n",
            " |    8/   20 batches | loss: 0.295\n",
            " |    9/   20 batches | loss: 0.28\n",
            " |   10/   20 batches | loss: 0.297\n",
            " |   11/   20 batches | loss: 0.297\n",
            " |   12/   20 batches | loss: 0.314\n",
            " |   13/   20 batches | loss: 0.298\n",
            " |   14/   20 batches | loss: 0.285\n",
            " |   15/   20 batches | loss: 0.294\n",
            " |   16/   20 batches | loss: 0.298\n",
            " |   17/   20 batches | loss: 0.28\n",
            " |   18/   20 batches | loss: 0.314\n",
            " |   19/   20 batches | loss: 0.274\n",
            " |    0/   20 batches | loss: 0.298\n",
            " |    1/   20 batches | loss: 0.287\n",
            " |    2/   20 batches | loss: 0.273\n",
            " |    3/   20 batches | loss: 0.248\n",
            " |    4/   20 batches | loss: 0.278\n",
            " |    5/   20 batches | loss: 0.293\n",
            " |    6/   20 batches | loss: 0.277\n",
            " |    7/   20 batches | loss: 0.267\n",
            " |    8/   20 batches | loss: 0.284\n",
            " |    9/   20 batches | loss: 0.288\n",
            " |   10/   20 batches | loss: 0.303\n",
            " |   11/   20 batches | loss: 0.281\n",
            " |   12/   20 batches | loss: 0.307\n",
            " |   13/   20 batches | loss: 0.292\n",
            " |   14/   20 batches | loss: 0.301\n",
            " |   15/   20 batches | loss: 0.292\n",
            " |   16/   20 batches | loss: 0.285\n",
            " |   17/   20 batches | loss: 0.278\n",
            " |   18/   20 batches | loss: 0.261\n",
            " |   19/   20 batches | loss: 0.271\n",
            " |    0/   20 batches | loss: 0.281\n",
            " |    1/   20 batches | loss: 0.296\n",
            " |    2/   20 batches | loss: 0.275\n",
            " |    3/   20 batches | loss: 0.26\n",
            " |    4/   20 batches | loss: 0.26\n",
            " |    5/   20 batches | loss: 0.305\n",
            " |    6/   20 batches | loss: 0.252\n",
            " |    7/   20 batches | loss: 0.298\n",
            " |    8/   20 batches | loss: 0.285\n",
            " |    9/   20 batches | loss: 0.257\n",
            " |   10/   20 batches | loss: 0.297\n",
            " |   11/   20 batches | loss: 0.259\n",
            " |   12/   20 batches | loss: 0.274\n",
            " |   13/   20 batches | loss: 0.288\n",
            " |   14/   20 batches | loss: 0.267\n",
            " |   15/   20 batches | loss: 0.297\n",
            " |   16/   20 batches | loss: 0.259\n",
            " |   17/   20 batches | loss: 0.287\n",
            " |   18/   20 batches | loss: 0.29\n",
            " |   19/   20 batches | loss: 0.292\n",
            " |    0/   20 batches | loss: 0.237\n",
            " |    1/   20 batches | loss: 0.283\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▍      | 34/100 [00:04<00:03, 21.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " |    2/   20 batches | loss: 0.291\n",
            " |    3/   20 batches | loss: 0.27\n",
            " |    4/   20 batches | loss: 0.297\n",
            " |    5/   20 batches | loss: 0.253\n",
            " |    6/   20 batches | loss: 0.264\n",
            " |    7/   20 batches | loss: 0.28\n",
            " |    8/   20 batches | loss: 0.266\n",
            " |    9/   20 batches | loss: 0.267\n",
            " |   10/   20 batches | loss: 0.28\n",
            " |   11/   20 batches | loss: 0.275\n",
            " |   12/   20 batches | loss: 0.248\n",
            " |   13/   20 batches | loss: 0.263\n",
            " |   14/   20 batches | loss: 0.285\n",
            " |   15/   20 batches | loss: 0.282\n",
            " |   16/   20 batches | loss: 0.287\n",
            " |   17/   20 batches | loss: 0.287\n",
            " |   18/   20 batches | loss: 0.253\n",
            " |   19/   20 batches | loss: 0.263\n",
            " |    0/   20 batches | loss: 0.278\n",
            " |    1/   20 batches | loss: 0.238\n",
            " |    2/   20 batches | loss: 0.29\n",
            " |    3/   20 batches | loss: 0.282\n",
            " |    4/   20 batches | loss: 0.264\n",
            " |    5/   20 batches | loss: 0.274\n",
            " |    6/   20 batches | loss: 0.286\n",
            " |    7/   20 batches | loss: 0.261\n",
            " |    8/   20 batches | loss: 0.277\n",
            " |    9/   20 batches | loss: 0.257\n",
            " |   10/   20 batches | loss: 0.237\n",
            " |   11/   20 batches | loss: 0.263\n",
            " |   12/   20 batches | loss: 0.267\n",
            " |   13/   20 batches | loss: 0.256\n",
            " |   14/   20 batches | loss: 0.269\n",
            " |   15/   20 batches | loss: 0.249\n",
            " |   16/   20 batches | loss: 0.271\n",
            " |   17/   20 batches | loss: 0.249\n",
            " |   18/   20 batches | loss: 0.27\n",
            " |   19/   20 batches | loss: 0.276\n",
            " |    0/   20 batches | loss: 0.273\n",
            " |    1/   20 batches | loss: 0.259\n",
            " |    2/   20 batches | loss: 0.262\n",
            " |    3/   20 batches | loss: 0.255\n",
            " |    4/   20 batches | loss: 0.26\n",
            " |    5/   20 batches | loss: 0.267\n",
            " |    6/   20 batches | loss: 0.287\n",
            " |    7/   20 batches | loss: 0.227\n",
            " |    8/   20 batches | loss: 0.239\n",
            " |    9/   20 batches | loss: 0.296\n",
            " |   10/   20 batches | loss: 0.266\n",
            " |   11/   20 batches | loss: 0.27\n",
            " |   12/   20 batches | loss: 0.279\n",
            " |   13/   20 batches | loss: 0.266\n",
            " |   14/   20 batches | loss: 0.275\n",
            " |   15/   20 batches | loss: 0.262\n",
            " |   16/   20 batches | loss: 0.261\n",
            " |   17/   20 batches | loss: 0.242\n",
            " |   18/   20 batches | loss: 0.249\n",
            " |   19/   20 batches | loss: 0.255\n",
            " |    0/   20 batches | loss: 0.285\n",
            " |    1/   20 batches | loss: 0.245\n",
            " |    2/   20 batches | loss: 0.275\n",
            " |    3/   20 batches | loss: 0.264\n",
            " |    4/   20 batches | loss: 0.243\n",
            " |    5/   20 batches | loss: 0.27\n",
            " |    6/   20 batches | loss: 0.26\n",
            " |    7/   20 batches | loss: 0.257\n",
            " |    8/   20 batches | loss: 0.261\n",
            " |    9/   20 batches | loss: 0.229\n",
            " |   10/   20 batches | loss: 0.265\n",
            " |   11/   20 batches | loss: 0.249\n",
            " |   12/   20 batches | loss: 0.246\n",
            " |   13/   20 batches | loss: 0.284\n",
            " |   14/   20 batches | loss: 0.261\n",
            " |   15/   20 batches | loss: 0.263\n",
            " |   16/   20 batches | loss: 0.255\n",
            " |   17/   20 batches | loss: 0.257\n",
            " |   18/   20 batches | loss: 0.251\n",
            " |   19/   20 batches | loss: 0.23\n",
            " |    0/   20 batches | loss: 0.262\n",
            " |    1/   20 batches | loss: 0.25\n",
            " |    2/   20 batches | loss: 0.24\n",
            " |    3/   20 batches | loss: 0.264\n",
            " |    4/   20 batches | loss: 0.271\n",
            " |    5/   20 batches | loss: 0.276\n",
            " |    6/   20 batches | loss: 0.249\n",
            " |    7/   20 batches | loss: 0.249\n",
            " |    8/   20 batches | loss: 0.261\n",
            " |    9/   20 batches | loss: 0.254\n",
            " |   10/   20 batches | loss: 0.256\n",
            " |   11/   20 batches | loss: 0.246\n",
            " |   12/   20 batches | loss: 0.248\n",
            " |   13/   20 batches | loss: 0.257\n",
            " |   14/   20 batches | loss: 0.252\n",
            " |   15/   20 batches | loss: 0.252\n",
            " |   16/   20 batches | loss: 0.25\n",
            " |   17/   20 batches | loss: 0.256\n",
            " |   18/   20 batches | loss: 0.264\n",
            " |   19/   20 batches | loss: 0.237\n",
            " |    0/   20 batches | loss: 0.284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 37%|███▋      | 37/100 [00:04<00:02, 22.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " |    1/   20 batches | loss: 0.234\n",
            " |    2/   20 batches | loss: 0.237\n",
            " |    3/   20 batches | loss: 0.262\n",
            " |    4/   20 batches | loss: 0.251\n",
            " |    5/   20 batches | loss: 0.254\n",
            " |    6/   20 batches | loss: 0.252\n",
            " |    7/   20 batches | loss: 0.247\n",
            " |    8/   20 batches | loss: 0.261\n",
            " |    9/   20 batches | loss: 0.263\n",
            " |   10/   20 batches | loss: 0.244\n",
            " |   11/   20 batches | loss: 0.235\n",
            " |   12/   20 batches | loss: 0.254\n",
            " |   13/   20 batches | loss: 0.252\n",
            " |   14/   20 batches | loss: 0.247\n",
            " |   15/   20 batches | loss: 0.24\n",
            " |   16/   20 batches | loss: 0.254\n",
            " |   17/   20 batches | loss: 0.24\n",
            " |   18/   20 batches | loss: 0.255\n",
            " |   19/   20 batches | loss: 0.219\n",
            " |    0/   20 batches | loss: 0.256\n",
            " |    1/   20 batches | loss: 0.245\n",
            " |    2/   20 batches | loss: 0.231\n",
            " |    3/   20 batches | loss: 0.233\n",
            " |    4/   20 batches | loss: 0.268\n",
            " |    5/   20 batches | loss: 0.25\n",
            " |    6/   20 batches | loss: 0.244\n",
            " |    7/   20 batches | loss: 0.25\n",
            " |    8/   20 batches | loss: 0.259\n",
            " |    9/   20 batches | loss: 0.253\n",
            " |   10/   20 batches | loss: 0.242\n",
            " |   11/   20 batches | loss: 0.227\n",
            " |   12/   20 batches | loss: 0.249\n",
            " |   13/   20 batches | loss: 0.251\n",
            " |   14/   20 batches | loss: 0.276\n",
            " |   15/   20 batches | loss: 0.214\n",
            " |   16/   20 batches | loss: 0.252\n",
            " |   17/   20 batches | loss: 0.234\n",
            " |   18/   20 batches | loss: 0.257\n",
            " |   19/   20 batches | loss: 0.26\n",
            " |    0/   20 batches | loss: 0.26\n",
            " |    1/   20 batches | loss: 0.265\n",
            " |    2/   20 batches | loss: 0.226\n",
            " |    3/   20 batches | loss: 0.241\n",
            " |    4/   20 batches | loss: 0.255\n",
            " |    5/   20 batches | loss: 0.249\n",
            " |    6/   20 batches | loss: 0.242\n",
            " |    7/   20 batches | loss: 0.247\n",
            " |    8/   20 batches | loss: 0.249\n",
            " |    9/   20 batches | loss: 0.24\n",
            " |   10/   20 batches | loss: 0.233\n",
            " |   11/   20 batches | loss: 0.231\n",
            " |   12/   20 batches | loss: 0.268\n",
            " |   13/   20 batches | loss: 0.233\n",
            " |   14/   20 batches | loss: 0.232\n",
            " |   15/   20 batches | loss: 0.251\n",
            " |   16/   20 batches | loss: 0.247\n",
            " |   17/   20 batches | loss: 0.243\n",
            " |   18/   20 batches | loss: 0.222\n",
            " |   19/   20 batches | loss: 0.241\n",
            " |    0/   20 batches | loss: 0.225\n",
            " |    1/   20 batches | loss: 0.244\n",
            " |    2/   20 batches | loss: 0.231\n",
            " |    3/   20 batches | loss: 0.225\n",
            " |    4/   20 batches | loss: 0.243\n",
            " |    5/   20 batches | loss: 0.236\n",
            " |    6/   20 batches | loss: 0.251\n",
            " |    7/   20 batches | loss: 0.22\n",
            " |    8/   20 batches | loss: 0.232\n",
            " |    9/   20 batches | loss: 0.258\n",
            " |   10/   20 batches | loss: 0.233\n",
            " |   11/   20 batches | loss: 0.249\n",
            " |   12/   20 batches | loss: 0.254\n",
            " |   13/   20 batches | loss: 0.219\n",
            " |   14/   20 batches | loss: 0.258\n",
            " |   15/   20 batches | loss: 0.252\n",
            " |   16/   20 batches | loss: 0.234\n",
            " |   17/   20 batches | loss: 0.217\n",
            " |   18/   20 batches | loss: 0.236\n",
            " |   19/   20 batches | loss: 0.252\n",
            " |    0/   20 batches | loss: 0.224\n",
            " |    1/   20 batches | loss: 0.235\n",
            " |    2/   20 batches | loss: 0.241\n",
            " |    3/   20 batches | loss: 0.262\n",
            " |    4/   20 batches | loss: 0.227\n",
            " |    5/   20 batches | loss: 0.23\n",
            " |    6/   20 batches | loss: 0.25\n",
            " |    7/   20 batches | loss: 0.247\n",
            " |    8/   20 batches | loss: 0.228\n",
            " |    9/   20 batches | loss: 0.21\n",
            " |   10/   20 batches | loss: 0.245\n",
            " |   11/   20 batches | loss: 0.227\n",
            " |   12/   20 batches | loss: 0.244\n",
            " |   13/   20 batches | loss: 0.243\n",
            " |   14/   20 batches | loss: 0.23\n",
            " |   15/   20 batches | loss: 0.223\n",
            " |   16/   20 batches | loss: 0.22\n",
            " |   17/   20 batches | loss: 0.23\n",
            " |   18/   20 batches | loss: 0.24\n",
            " |   19/   20 batches | loss: 0.245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 43%|████▎     | 43/100 [00:04<00:02, 23.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " |    0/   20 batches | loss: 0.229\n",
            " |    1/   20 batches | loss: 0.205\n",
            " |    2/   20 batches | loss: 0.229\n",
            " |    3/   20 batches | loss: 0.244\n",
            " |    4/   20 batches | loss: 0.226\n",
            " |    5/   20 batches | loss: 0.228\n",
            " |    6/   20 batches | loss: 0.257\n",
            " |    7/   20 batches | loss: 0.211\n",
            " |    8/   20 batches | loss: 0.245\n",
            " |    9/   20 batches | loss: 0.221\n",
            " |   10/   20 batches | loss: 0.254\n",
            " |   11/   20 batches | loss: 0.208\n",
            " |   12/   20 batches | loss: 0.241\n",
            " |   13/   20 batches | loss: 0.208\n",
            " |   14/   20 batches | loss: 0.226\n",
            " |   15/   20 batches | loss: 0.216\n",
            " |   16/   20 batches | loss: 0.241\n",
            " |   17/   20 batches | loss: 0.239\n",
            " |   18/   20 batches | loss: 0.256\n",
            " |   19/   20 batches | loss: 0.239\n",
            " |    0/   20 batches | loss: 0.225\n",
            " |    1/   20 batches | loss: 0.216\n",
            " |    2/   20 batches | loss: 0.222\n",
            " |    3/   20 batches | loss: 0.213\n",
            " |    4/   20 batches | loss: 0.206\n",
            " |    5/   20 batches | loss: 0.216\n",
            " |    6/   20 batches | loss: 0.228\n",
            " |    7/   20 batches | loss: 0.215\n",
            " |    8/   20 batches | loss: 0.232\n",
            " |    9/   20 batches | loss: 0.228\n",
            " |   10/   20 batches | loss: 0.244\n",
            " |   11/   20 batches | loss: 0.228\n",
            " |   12/   20 batches | loss: 0.23\n",
            " |   13/   20 batches | loss: 0.24\n",
            " |   14/   20 batches | loss: 0.236\n",
            " |   15/   20 batches | loss: 0.228\n",
            " |   16/   20 batches | loss: 0.248\n",
            " |   17/   20 batches | loss: 0.224\n",
            " |   18/   20 batches | loss: 0.227\n",
            " |   19/   20 batches | loss: 0.226\n",
            " |    0/   20 batches | loss: 0.23\n",
            " |    1/   20 batches | loss: 0.241\n",
            " |    2/   20 batches | loss: 0.23\n",
            " |    3/   20 batches | loss: 0.236\n",
            " |    4/   20 batches | loss: 0.222\n",
            " |    5/   20 batches | loss: 0.218\n",
            " |    6/   20 batches | loss: 0.225\n",
            " |    7/   20 batches | loss: 0.217\n",
            " |    8/   20 batches | loss: 0.225\n",
            " |    9/   20 batches | loss: 0.213\n",
            " |   10/   20 batches | loss: 0.232\n",
            " |   11/   20 batches | loss: 0.212\n",
            " |   12/   20 batches | loss: 0.206\n",
            " |   13/   20 batches | loss: 0.223\n",
            " |   14/   20 batches | loss: 0.218\n",
            " |   15/   20 batches | loss: 0.233\n",
            " |   16/   20 batches | loss: 0.224\n",
            " |   17/   20 batches | loss: 0.217\n",
            " |   18/   20 batches | loss: 0.232\n",
            " |   19/   20 batches | loss: 0.221\n",
            " |    0/   20 batches | loss: 0.226\n",
            " |    1/   20 batches | loss: 0.219\n",
            " |    2/   20 batches | loss: 0.2\n",
            " |    3/   20 batches | loss: 0.221\n",
            " |    4/   20 batches | loss: 0.238\n",
            " |    5/   20 batches | loss: 0.202\n",
            " |    6/   20 batches | loss: 0.214\n",
            " |    7/   20 batches | loss: 0.214\n",
            " |    8/   20 batches | loss: 0.22\n",
            " |    9/   20 batches | loss: 0.221\n",
            " |   10/   20 batches | loss: 0.225\n",
            " |   11/   20 batches | loss: 0.261\n",
            " |   12/   20 batches | loss: 0.214\n",
            " |   13/   20 batches | loss: 0.221\n",
            " |   14/   20 batches | loss: 0.231\n",
            " |   15/   20 batches | loss: 0.213\n",
            " |   16/   20 batches | loss: 0.217\n",
            " |   17/   20 batches | loss: 0.216\n",
            " |   18/   20 batches | loss: 0.209\n",
            " |   19/   20 batches | loss: 0.22\n",
            " |    0/   20 batches | loss: 0.202\n",
            " |    1/   20 batches | loss: 0.198\n",
            " |    2/   20 batches | loss: 0.237\n",
            " |    3/   20 batches | loss: 0.227\n",
            " |    4/   20 batches | loss: 0.225\n",
            " |    5/   20 batches | loss: 0.205\n",
            " |    6/   20 batches | loss: 0.21\n",
            " |    7/   20 batches | loss: 0.221\n",
            " |    8/   20 batches | loss: 0.208\n",
            " |    9/   20 batches | loss: 0.227\n",
            " |   10/   20 batches | loss: 0.229\n",
            " |   11/   20 batches | loss: 0.218\n",
            " |   12/   20 batches | loss: 0.227\n",
            " |   13/   20 batches | loss: 0.22\n",
            " |   14/   20 batches | loss: 0.229\n",
            " |   15/   20 batches | loss: 0.209\n",
            " |   16/   20 batches | loss: 0.221\n",
            " |   17/   20 batches | loss: 0.217\n",
            " |   18/   20 batches | loss: 0.202\n",
            " |   19/   20 batches | loss: 0.217\n",
            " |    0/   20 batches | loss: 0.224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 49%|████▉     | 49/100 [00:04<00:02, 23.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " |    1/   20 batches | loss: 0.205\n",
            " |    2/   20 batches | loss: 0.203\n",
            " |    3/   20 batches | loss: 0.205\n",
            " |    4/   20 batches | loss: 0.219\n",
            " |    5/   20 batches | loss: 0.215\n",
            " |    6/   20 batches | loss: 0.232\n",
            " |    7/   20 batches | loss: 0.177\n",
            " |    8/   20 batches | loss: 0.22\n",
            " |    9/   20 batches | loss: 0.221\n",
            " |   10/   20 batches | loss: 0.23\n",
            " |   11/   20 batches | loss: 0.213\n",
            " |   12/   20 batches | loss: 0.218\n",
            " |   13/   20 batches | loss: 0.224\n",
            " |   14/   20 batches | loss: 0.212\n",
            " |   15/   20 batches | loss: 0.202\n",
            " |   16/   20 batches | loss: 0.215\n",
            " |   17/   20 batches | loss: 0.218\n",
            " |   18/   20 batches | loss: 0.223\n",
            " |   19/   20 batches | loss: 0.201\n",
            " |    0/   20 batches | loss: 0.215\n",
            " |    1/   20 batches | loss: 0.193\n",
            " |    2/   20 batches | loss: 0.2\n",
            " |    3/   20 batches | loss: 0.233\n",
            " |    4/   20 batches | loss: 0.217\n",
            " |    5/   20 batches | loss: 0.216\n",
            " |    6/   20 batches | loss: 0.194\n",
            " |    7/   20 batches | loss: 0.207\n",
            " |    8/   20 batches | loss: 0.219\n",
            " |    9/   20 batches | loss: 0.209\n",
            " |   10/   20 batches | loss: 0.206\n",
            " |   11/   20 batches | loss: 0.196\n",
            " |   12/   20 batches | loss: 0.229\n",
            " |   13/   20 batches | loss: 0.212\n",
            " |   14/   20 batches | loss: 0.2\n",
            " |   15/   20 batches | loss: 0.224\n",
            " |   16/   20 batches | loss: 0.209\n",
            " |   17/   20 batches | loss: 0.212\n",
            " |   18/   20 batches | loss: 0.221\n",
            " |   19/   20 batches | loss: 0.215\n",
            " |    0/   20 batches | loss: 0.222\n",
            " |    1/   20 batches | loss: 0.231\n",
            " |    2/   20 batches | loss: 0.216\n",
            " |    3/   20 batches | loss: 0.194\n",
            " |    4/   20 batches | loss: 0.201\n",
            " |    5/   20 batches | loss: 0.202\n",
            " |    6/   20 batches | loss: 0.224\n",
            " |    7/   20 batches | loss: 0.209\n",
            " |    8/   20 batches | loss: 0.207\n",
            " |    9/   20 batches | loss: 0.2\n",
            " |   10/   20 batches | loss: 0.213\n",
            " |   11/   20 batches | loss: 0.21\n",
            " |   12/   20 batches | loss: 0.206\n",
            " |   13/   20 batches | loss: 0.219\n",
            " |   14/   20 batches | loss: 0.206\n",
            " |   15/   20 batches | loss: 0.206\n",
            " |   16/   20 batches | loss: 0.196\n",
            " |   17/   20 batches | loss: 0.225\n",
            " |   18/   20 batches | loss: 0.21\n",
            " |   19/   20 batches | loss: 0.172\n",
            " |    0/   20 batches | loss: 0.211\n",
            " |    1/   20 batches | loss: 0.217\n",
            " |    2/   20 batches | loss: 0.232\n",
            " |    3/   20 batches | loss: 0.18\n",
            " |    4/   20 batches | loss: 0.195\n",
            " |    5/   20 batches | loss: 0.205\n",
            " |    6/   20 batches | loss: 0.207\n",
            " |    7/   20 batches | loss: 0.215\n",
            " |    8/   20 batches | loss: 0.208\n",
            " |    9/   20 batches | loss: 0.206\n",
            " |   10/   20 batches | loss: 0.201\n",
            " |   11/   20 batches | loss: 0.198\n",
            " |   12/   20 batches | loss: 0.189\n",
            " |   13/   20 batches | loss: 0.228\n",
            " |   14/   20 batches | loss: 0.2\n",
            " |   15/   20 batches | loss: 0.206\n",
            " |   16/   20 batches | loss: 0.206\n",
            " |   17/   20 batches | loss: 0.198\n",
            " |   18/   20 batches | loss: 0.203\n",
            " |   19/   20 batches | loss: 0.196\n",
            " |    0/   20 batches | loss: 0.173\n",
            " |    1/   20 batches | loss: 0.219\n",
            " |    2/   20 batches | loss: 0.218\n",
            " |    3/   20 batches | loss: 0.205\n",
            " |    4/   20 batches | loss: 0.219\n",
            " |    5/   20 batches | loss: 0.211\n",
            " |    6/   20 batches | loss: 0.21\n",
            " |    7/   20 batches | loss: 0.208\n",
            " |    8/   20 batches | loss: 0.199\n",
            " |    9/   20 batches | loss: 0.198\n",
            " |   10/   20 batches | loss: 0.205\n",
            " |   11/   20 batches | loss: 0.206\n",
            " |   12/   20 batches | loss: 0.214\n",
            " |   13/   20 batches | loss: 0.205\n",
            " |   14/   20 batches | loss: 0.203\n",
            " |   15/   20 batches | loss: 0.207\n",
            " |   16/   20 batches | loss: 0.208\n",
            " |   17/   20 batches | loss: 0.188\n",
            " |   18/   20 batches | loss: 0.193\n",
            " |   19/   20 batches | loss: 0.207\n",
            " |    0/   20 batches | loss: 0.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52%|█████▏    | 52/100 [00:04<00:01, 24.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " |    1/   20 batches | loss: 0.221\n",
            " |    2/   20 batches | loss: 0.203\n",
            " |    3/   20 batches | loss: 0.189\n",
            " |    4/   20 batches | loss: 0.226\n",
            " |    5/   20 batches | loss: 0.203\n",
            " |    6/   20 batches | loss: 0.185\n",
            " |    7/   20 batches | loss: 0.181\n",
            " |    8/   20 batches | loss: 0.209\n",
            " |    9/   20 batches | loss: 0.219\n",
            " |   10/   20 batches | loss: 0.202\n",
            " |   11/   20 batches | loss: 0.203\n",
            " |   12/   20 batches | loss: 0.199\n",
            " |   13/   20 batches | loss: 0.201\n",
            " |   14/   20 batches | loss: 0.205\n",
            " |   15/   20 batches | loss: 0.206\n",
            " |   16/   20 batches | loss: 0.186\n",
            " |   17/   20 batches | loss: 0.197\n",
            " |   18/   20 batches | loss: 0.191\n",
            " |   19/   20 batches | loss: 0.214\n",
            " |    0/   20 batches | loss: 0.2\n",
            " |    1/   20 batches | loss: 0.201\n",
            " |    2/   20 batches | loss: 0.205\n",
            " |    3/   20 batches | loss: 0.19\n",
            " |    4/   20 batches | loss: 0.223\n",
            " |    5/   20 batches | loss: 0.197\n",
            " |    6/   20 batches | loss: 0.193\n",
            " |    7/   20 batches | loss: 0.206\n",
            " |    8/   20 batches | loss: 0.202\n",
            " |    9/   20 batches | loss: 0.188\n",
            " |   10/   20 batches | loss: 0.203\n",
            " |   11/   20 batches | loss: 0.207\n",
            " |   12/   20 batches | loss: 0.204\n",
            " |   13/   20 batches | loss: 0.191\n",
            " |   14/   20 batches | loss: 0.208\n",
            " |   15/   20 batches | loss: 0.21\n",
            " |   16/   20 batches | loss: 0.187\n",
            " |   17/   20 batches | loss: 0.18\n",
            " |   18/   20 batches | loss: 0.211\n",
            " |   19/   20 batches | loss: 0.17\n",
            " |    0/   20 batches | loss: 0.201\n",
            " |    1/   20 batches | loss: 0.206\n",
            " |    2/   20 batches | loss: 0.196\n",
            " |    3/   20 batches | loss: 0.229\n",
            " |    4/   20 batches | loss: 0.201\n",
            " |    5/   20 batches | loss: 0.209\n",
            " |    6/   20 batches | loss: 0.188\n",
            " |    7/   20 batches | loss: 0.193\n",
            " |    8/   20 batches | loss: 0.205\n",
            " |    9/   20 batches | loss: 0.2\n",
            " |   10/   20 batches | loss: 0.191\n",
            " |   11/   20 batches | loss: 0.192\n",
            " |   12/   20 batches | loss: 0.199\n",
            " |   13/   20 batches | loss: 0.184\n",
            " |   14/   20 batches | loss: 0.196\n",
            " |   15/   20 batches | loss: 0.199\n",
            " |   16/   20 batches | loss: 0.186\n",
            " |   17/   20 batches | loss: 0.188\n",
            " |   18/   20 batches | loss: 0.179\n",
            " |   19/   20 batches | loss: 0.188\n",
            " |    0/   20 batches | loss: 0.182\n",
            " |    1/   20 batches | loss: 0.192\n",
            " |    2/   20 batches | loss: 0.205\n",
            " |    3/   20 batches | loss: 0.184\n",
            " |    4/   20 batches | loss: 0.192\n",
            " |    5/   20 batches | loss: 0.188\n",
            " |    6/   20 batches | loss: 0.205\n",
            " |    7/   20 batches | loss: 0.199\n",
            " |    8/   20 batches | loss: 0.189\n",
            " |    9/   20 batches | loss: 0.195\n",
            " |   10/   20 batches | loss: 0.197\n",
            " |   11/   20 batches | loss: 0.19\n",
            " |   12/   20 batches | loss: 0.191\n",
            " |   13/   20 batches | loss: 0.19\n",
            " |   14/   20 batches | loss: 0.202\n",
            " |   15/   20 batches | loss: 0.206\n",
            " |   16/   20 batches | loss: 0.193\n",
            " |   17/   20 batches | loss: 0.181\n",
            " |   18/   20 batches | loss: 0.198\n",
            " |   19/   20 batches | loss: 0.206\n",
            " |    0/   20 batches | loss: 0.188\n",
            " |    1/   20 batches | loss: 0.207\n",
            " |    2/   20 batches | loss: 0.197\n",
            " |    3/   20 batches | loss: 0.201\n",
            " |    4/   20 batches | loss: 0.184\n",
            " |    5/   20 batches | loss: 0.213\n",
            " |    6/   20 batches | loss: 0.199\n",
            " |    7/   20 batches | loss: 0.182\n",
            " |    8/   20 batches | loss: 0.203\n",
            " |    9/   20 batches | loss: 0.194\n",
            " |   10/   20 batches | loss: 0.182\n",
            " |   11/   20 batches | loss: 0.183\n",
            " |   12/   20 batches | loss: 0.171\n",
            " |   13/   20 batches | loss: 0.183\n",
            " |   14/   20 batches | loss: 0.188\n",
            " |   15/   20 batches | loss: 0.188\n",
            " |   16/   20 batches | loss: 0.19\n",
            " |   17/   20 batches | loss: 0.195\n",
            " |   18/   20 batches | loss: 0.202\n",
            " |   19/   20 batches | loss: 0.176\n",
            " |    0/   20 batches | loss: 0.203\n",
            " |    1/   20 batches | loss: 0.189\n",
            " |    2/   20 batches | loss: 0.175\n",
            " |    3/   20 batches | loss: 0.173\n",
            " |    4/   20 batches | loss: 0.191\n",
            " |    5/   20 batches | loss: 0.171\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 58%|█████▊    | 58/100 [00:05<00:01, 24.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " |    6/   20 batches | loss: 0.199\n",
            " |    7/   20 batches | loss: 0.194\n",
            " |    8/   20 batches | loss: 0.199\n",
            " |    9/   20 batches | loss: 0.191\n",
            " |   10/   20 batches | loss: 0.198\n",
            " |   11/   20 batches | loss: 0.18\n",
            " |   12/   20 batches | loss: 0.178\n",
            " |   13/   20 batches | loss: 0.197\n",
            " |   14/   20 batches | loss: 0.219\n",
            " |   15/   20 batches | loss: 0.196\n",
            " |   16/   20 batches | loss: 0.198\n",
            " |   17/   20 batches | loss: 0.181\n",
            " |   18/   20 batches | loss: 0.198\n",
            " |   19/   20 batches | loss: 0.167\n",
            " |    0/   20 batches | loss: 0.181\n",
            " |    1/   20 batches | loss: 0.202\n",
            " |    2/   20 batches | loss: 0.186\n",
            " |    3/   20 batches | loss: 0.203\n",
            " |    4/   20 batches | loss: 0.177\n",
            " |    5/   20 batches | loss: 0.171\n",
            " |    6/   20 batches | loss: 0.191\n",
            " |    7/   20 batches | loss: 0.194\n",
            " |    8/   20 batches | loss: 0.19\n",
            " |    9/   20 batches | loss: 0.177\n",
            " |   10/   20 batches | loss: 0.196\n",
            " |   11/   20 batches | loss: 0.181\n",
            " |   12/   20 batches | loss: 0.18\n",
            " |   13/   20 batches | loss: 0.198\n",
            " |   14/   20 batches | loss: 0.178\n",
            " |   15/   20 batches | loss: 0.196\n",
            " |   16/   20 batches | loss: 0.188\n",
            " |   17/   20 batches | loss: 0.18\n",
            " |   18/   20 batches | loss: 0.201\n",
            " |   19/   20 batches | loss: 0.168\n",
            " |    0/   20 batches | loss: 0.201\n",
            " |    1/   20 batches | loss: 0.188\n",
            " |    2/   20 batches | loss: 0.189\n",
            " |    3/   20 batches | loss: 0.192\n",
            " |    4/   20 batches | loss: 0.197\n",
            " |    5/   20 batches | loss: 0.162\n",
            " |    6/   20 batches | loss: 0.176\n",
            " |    7/   20 batches | loss: 0.201\n",
            " |    8/   20 batches | loss: 0.205\n",
            " |    9/   20 batches | loss: 0.167\n",
            " |   10/   20 batches | loss: 0.194\n",
            " |   11/   20 batches | loss: 0.175\n",
            " |   12/   20 batches | loss: 0.166\n",
            " |   13/   20 batches | loss: 0.189\n",
            " |   14/   20 batches | loss: 0.195\n",
            " |   15/   20 batches | loss: 0.18\n",
            " |   16/   20 batches | loss: 0.191\n",
            " |   17/   20 batches | loss: 0.179\n",
            " |   18/   20 batches | loss: 0.162\n",
            " |   19/   20 batches | loss: 0.186\n",
            " |    0/   20 batches | loss: 0.167\n",
            " |    1/   20 batches | loss: 0.182\n",
            " |    2/   20 batches | loss: 0.174\n",
            " |    3/   20 batches | loss: 0.199\n",
            " |    4/   20 batches | loss: 0.174\n",
            " |    5/   20 batches | loss: 0.198\n",
            " |    6/   20 batches | loss: 0.18\n",
            " |    7/   20 batches | loss: 0.181\n",
            " |    8/   20 batches | loss: 0.179\n",
            " |    9/   20 batches | loss: 0.169\n",
            " |   10/   20 batches | loss: 0.171\n",
            " |   11/   20 batches | loss: 0.174\n",
            " |   12/   20 batches | loss: 0.179\n",
            " |   13/   20 batches | loss: 0.187\n",
            " |   14/   20 batches | loss: 0.173\n",
            " |   15/   20 batches | loss: 0.19\n",
            " |   16/   20 batches | loss: 0.216\n",
            " |   17/   20 batches | loss: 0.18\n",
            " |   18/   20 batches | loss: 0.183\n",
            " |   19/   20 batches | loss: 0.163\n",
            " |    0/   20 batches | loss: 0.169\n",
            " |    1/   20 batches | loss: 0.196\n",
            " |    2/   20 batches | loss: 0.194\n",
            " |    3/   20 batches | loss: 0.175\n",
            " |    4/   20 batches | loss: 0.174\n",
            " |    5/   20 batches | loss: 0.158\n",
            " |    6/   20 batches | loss: 0.182\n",
            " |    7/   20 batches | loss: 0.171\n",
            " |    8/   20 batches | loss: 0.192\n",
            " |    9/   20 batches | loss: 0.172\n",
            " |   10/   20 batches | loss: 0.187\n",
            " |   11/   20 batches | loss: 0.186\n",
            " |   12/   20 batches | loss: 0.171\n",
            " |   13/   20 batches | loss: 0.19\n",
            " |   14/   20 batches | loss: 0.179\n",
            " |   15/   20 batches | loss: 0.177\n",
            " |   16/   20 batches | loss: 0.181\n",
            " |   17/   20 batches | loss: 0.191\n",
            " |   18/   20 batches | loss: 0.168\n",
            " |   19/   20 batches | loss: 0.17\n",
            " |    0/   20 batches | loss: 0.188\n",
            " |    1/   20 batches | loss: 0.169\n",
            " |    2/   20 batches | loss: 0.175\n",
            " |    3/   20 batches | loss: 0.17\n",
            " |    4/   20 batches | loss: 0.172\n",
            " |    5/   20 batches | loss: 0.181\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 64%|██████▍   | 64/100 [00:05<00:01, 24.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " |    6/   20 batches | loss: 0.168\n",
            " |    7/   20 batches | loss: 0.172\n",
            " |    8/   20 batches | loss: 0.175\n",
            " |    9/   20 batches | loss: 0.186\n",
            " |   10/   20 batches | loss: 0.183\n",
            " |   11/   20 batches | loss: 0.18\n",
            " |   12/   20 batches | loss: 0.179\n",
            " |   13/   20 batches | loss: 0.184\n",
            " |   14/   20 batches | loss: 0.179\n",
            " |   15/   20 batches | loss: 0.161\n",
            " |   16/   20 batches | loss: 0.188\n",
            " |   17/   20 batches | loss: 0.186\n",
            " |   18/   20 batches | loss: 0.196\n",
            " |   19/   20 batches | loss: 0.175\n",
            " |    0/   20 batches | loss: 0.173\n",
            " |    1/   20 batches | loss: 0.185\n",
            " |    2/   20 batches | loss: 0.157\n",
            " |    3/   20 batches | loss: 0.18\n",
            " |    4/   20 batches | loss: 0.18\n",
            " |    5/   20 batches | loss: 0.191\n",
            " |    6/   20 batches | loss: 0.191\n",
            " |    7/   20 batches | loss: 0.159\n",
            " |    8/   20 batches | loss: 0.169\n",
            " |    9/   20 batches | loss: 0.172\n",
            " |   10/   20 batches | loss: 0.189\n",
            " |   11/   20 batches | loss: 0.161\n",
            " |   12/   20 batches | loss: 0.156\n",
            " |   13/   20 batches | loss: 0.184\n",
            " |   14/   20 batches | loss: 0.188\n",
            " |   15/   20 batches | loss: 0.181\n",
            " |   16/   20 batches | loss: 0.164\n",
            " |   17/   20 batches | loss: 0.169\n",
            " |   18/   20 batches | loss: 0.189\n",
            " |   19/   20 batches | loss: 0.184\n",
            " |    0/   20 batches | loss: 0.157\n",
            " |    1/   20 batches | loss: 0.18\n",
            " |    2/   20 batches | loss: 0.167\n",
            " |    3/   20 batches | loss: 0.178\n",
            " |    4/   20 batches | loss: 0.18\n",
            " |    5/   20 batches | loss: 0.185\n",
            " |    6/   20 batches | loss: 0.173\n",
            " |    7/   20 batches | loss: 0.176\n",
            " |    8/   20 batches | loss: 0.174\n",
            " |    9/   20 batches | loss: 0.173\n",
            " |   10/   20 batches | loss: 0.172\n",
            " |   11/   20 batches | loss: 0.16\n",
            " |   12/   20 batches | loss: 0.154\n",
            " |   13/   20 batches | loss: 0.176\n",
            " |   14/   20 batches | loss: 0.167\n",
            " |   15/   20 batches | loss: 0.185\n",
            " |   16/   20 batches | loss: 0.17\n",
            " |   17/   20 batches | loss: 0.191\n",
            " |   18/   20 batches | loss: 0.175\n",
            " |   19/   20 batches | loss: 0.182\n",
            " |    0/   20 batches | loss: 0.162\n",
            " |    1/   20 batches | loss: 0.185\n",
            " |    2/   20 batches | loss: 0.191\n",
            " |    3/   20 batches | loss: 0.178\n",
            " |    4/   20 batches | loss: 0.179\n",
            " |    5/   20 batches | loss: 0.174\n",
            " |    6/   20 batches | loss: 0.162\n",
            " |    7/   20 batches | loss: 0.163\n",
            " |    8/   20 batches | loss: 0.162\n",
            " |    9/   20 batches | loss: 0.162\n",
            " |   10/   20 batches | loss: 0.159\n",
            " |   11/   20 batches | loss: 0.173\n",
            " |   12/   20 batches | loss: 0.188\n",
            " |   13/   20 batches | loss: 0.168\n",
            " |   14/   20 batches | loss: 0.164\n",
            " |   15/   20 batches | loss: 0.165\n",
            " |   16/   20 batches | loss: 0.174\n",
            " |   17/   20 batches | loss: 0.175\n",
            " |   18/   20 batches | loss: 0.163\n",
            " |   19/   20 batches | loss: 0.172\n",
            " |    0/   20 batches | loss: 0.177\n",
            " |    1/   20 batches | loss: 0.163\n",
            " |    2/   20 batches | loss: 0.168\n",
            " |    3/   20 batches | loss: 0.176\n",
            " |    4/   20 batches | loss: 0.175\n",
            " |    5/   20 batches | loss: 0.171\n",
            " |    6/   20 batches | loss: 0.17\n",
            " |    7/   20 batches | loss: 0.155\n",
            " |    8/   20 batches | loss: 0.173\n",
            " |    9/   20 batches | loss: 0.154\n",
            " |   10/   20 batches | loss: 0.158\n",
            " |   11/   20 batches | loss: 0.169\n",
            " |   12/   20 batches | loss: 0.181\n",
            " |   13/   20 batches | loss: 0.172\n",
            " |   14/   20 batches | loss: 0.175\n",
            " |   15/   20 batches | loss: 0.174\n",
            " |   16/   20 batches | loss: 0.169\n",
            " |   17/   20 batches | loss: 0.156\n",
            " |   18/   20 batches | loss: 0.177\n",
            " |   19/   20 batches | loss: 0.172\n",
            " |    0/   20 batches | loss: 0.18\n",
            " |    1/   20 batches | loss: 0.176\n",
            " |    2/   20 batches | loss: 0.171\n",
            " |    3/   20 batches | loss: 0.182\n",
            " |    4/   20 batches | loss: 0.154\n",
            " |    5/   20 batches | loss: 0.169\n",
            " |    6/   20 batches | loss: 0.176\n",
            " |    7/   20 batches | loss: 0.178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 67/100 [00:05<00:01, 24.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " |    8/   20 batches | loss: 0.169\n",
            " |    9/   20 batches | loss: 0.16\n",
            " |   10/   20 batches | loss: 0.169\n",
            " |   11/   20 batches | loss: 0.161\n",
            " |   12/   20 batches | loss: 0.171\n",
            " |   13/   20 batches | loss: 0.16\n",
            " |   14/   20 batches | loss: 0.162\n",
            " |   15/   20 batches | loss: 0.162\n",
            " |   16/   20 batches | loss: 0.156\n",
            " |   17/   20 batches | loss: 0.154\n",
            " |   18/   20 batches | loss: 0.161\n",
            " |   19/   20 batches | loss: 0.161\n",
            " |    0/   20 batches | loss: 0.168\n",
            " |    1/   20 batches | loss: 0.156\n",
            " |    2/   20 batches | loss: 0.17\n",
            " |    3/   20 batches | loss: 0.182\n",
            " |    4/   20 batches | loss: 0.161\n",
            " |    5/   20 batches | loss: 0.168\n",
            " |    6/   20 batches | loss: 0.183\n",
            " |    7/   20 batches | loss: 0.177\n",
            " |    8/   20 batches | loss: 0.183\n",
            " |    9/   20 batches | loss: 0.175\n",
            " |   10/   20 batches | loss: 0.174\n",
            " |   11/   20 batches | loss: 0.156\n",
            " |   12/   20 batches | loss: 0.156\n",
            " |   13/   20 batches | loss: 0.178\n",
            " |   14/   20 batches | loss: 0.186\n",
            " |   15/   20 batches | loss: 0.161\n",
            " |   16/   20 batches | loss: 0.154\n",
            " |   17/   20 batches | loss: 0.162\n",
            " |   18/   20 batches | loss: 0.169\n",
            " |   19/   20 batches | loss: 0.151\n",
            " |    0/   20 batches | loss: 0.158\n",
            " |    1/   20 batches | loss: 0.162\n",
            " |    2/   20 batches | loss: 0.161\n",
            " |    3/   20 batches | loss: 0.19\n",
            " |    4/   20 batches | loss: 0.171\n",
            " |    5/   20 batches | loss: 0.176\n",
            " |    6/   20 batches | loss: 0.157\n",
            " |    7/   20 batches | loss: 0.149\n",
            " |    8/   20 batches | loss: 0.167\n",
            " |    9/   20 batches | loss: 0.158\n",
            " |   10/   20 batches | loss: 0.168\n",
            " |   11/   20 batches | loss: 0.153\n",
            " |   12/   20 batches | loss: 0.158\n",
            " |   13/   20 batches | loss: 0.164\n",
            " |   14/   20 batches | loss: 0.174\n",
            " |   15/   20 batches | loss: 0.162\n",
            " |   16/   20 batches | loss: 0.151\n",
            " |   17/   20 batches | loss: 0.175\n",
            " |   18/   20 batches | loss: 0.161\n",
            " |   19/   20 batches | loss: 0.147\n",
            " |    0/   20 batches | loss: 0.171\n",
            " |    1/   20 batches | loss: 0.168\n",
            " |    2/   20 batches | loss: 0.15\n",
            " |    3/   20 batches | loss: 0.167\n",
            " |    4/   20 batches | loss: 0.182\n",
            " |    5/   20 batches | loss: 0.167\n",
            " |    6/   20 batches | loss: 0.172\n",
            " |    7/   20 batches | loss: 0.161\n",
            " |    8/   20 batches | loss: 0.159\n",
            " |    9/   20 batches | loss: 0.16\n",
            " |   10/   20 batches | loss: 0.165\n",
            " |   11/   20 batches | loss: 0.155\n",
            " |   12/   20 batches | loss: 0.143\n",
            " |   13/   20 batches | loss: 0.149\n",
            " |   14/   20 batches | loss: 0.152\n",
            " |   15/   20 batches | loss: 0.15\n",
            " |   16/   20 batches | loss: 0.164\n",
            " |   17/   20 batches | loss: 0.167\n",
            " |   18/   20 batches | loss: 0.166\n",
            " |   19/   20 batches | loss: 0.186\n",
            " |    0/   20 batches | loss: 0.176\n",
            " |    1/   20 batches | loss: 0.151\n",
            " |    2/   20 batches | loss: 0.147\n",
            " |    3/   20 batches | loss: 0.156\n",
            " |    4/   20 batches | loss: 0.148\n",
            " |    5/   20 batches | loss: 0.163\n",
            " |    6/   20 batches | loss: 0.171\n",
            " |    7/   20 batches | loss: 0.143\n",
            " |    8/   20 batches | loss: 0.162\n",
            " |    9/   20 batches | loss: 0.162\n",
            " |   10/   20 batches | loss: 0.175\n",
            " |   11/   20 batches | loss: 0.171\n",
            " |   12/   20 batches | loss: 0.171\n",
            " |   13/   20 batches | loss: 0.162\n",
            " |   14/   20 batches | loss: 0.153\n",
            " |   15/   20 batches | loss: 0.149\n",
            " |   16/   20 batches | loss: 0.147\n",
            " |   17/   20 batches | loss: 0.147\n",
            " |   18/   20 batches | loss: 0.17\n",
            " |   19/   20 batches | loss: 0.148\n",
            " |    0/   20 batches | loss: 0.16\n",
            " |    1/   20 batches | loss: 0.142\n",
            " |    2/   20 batches | loss: 0.14\n",
            " |    3/   20 batches | loss: 0.157\n",
            " |    4/   20 batches | loss: 0.165\n",
            " |    5/   20 batches | loss: 0.166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 73%|███████▎  | 73/100 [00:05<00:01, 23.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " |    6/   20 batches | loss: 0.152\n",
            " |    7/   20 batches | loss: 0.151\n",
            " |    8/   20 batches | loss: 0.176\n",
            " |    9/   20 batches | loss: 0.174\n",
            " |   10/   20 batches | loss: 0.15\n",
            " |   11/   20 batches | loss: 0.15\n",
            " |   12/   20 batches | loss: 0.164\n",
            " |   13/   20 batches | loss: 0.16\n",
            " |   14/   20 batches | loss: 0.164\n",
            " |   15/   20 batches | loss: 0.17\n",
            " |   16/   20 batches | loss: 0.151\n",
            " |   17/   20 batches | loss: 0.16\n",
            " |   18/   20 batches | loss: 0.159\n",
            " |   19/   20 batches | loss: 0.149\n",
            " |    0/   20 batches | loss: 0.158\n",
            " |    1/   20 batches | loss: 0.147\n",
            " |    2/   20 batches | loss: 0.159\n",
            " |    3/   20 batches | loss: 0.138\n",
            " |    4/   20 batches | loss: 0.174\n",
            " |    5/   20 batches | loss: 0.148\n",
            " |    6/   20 batches | loss: 0.157\n",
            " |    7/   20 batches | loss: 0.154\n",
            " |    8/   20 batches | loss: 0.147\n",
            " |    9/   20 batches | loss: 0.164\n",
            " |   10/   20 batches | loss: 0.153\n",
            " |   11/   20 batches | loss: 0.148\n",
            " |   12/   20 batches | loss: 0.164\n",
            " |   13/   20 batches | loss: 0.159\n",
            " |   14/   20 batches | loss: 0.159\n",
            " |   15/   20 batches | loss: 0.161\n",
            " |   16/   20 batches | loss: 0.173\n",
            " |   17/   20 batches | loss: 0.182\n",
            " |   18/   20 batches | loss: 0.158\n",
            " |   19/   20 batches | loss: 0.151\n",
            " |    0/   20 batches | loss: 0.161\n",
            " |    1/   20 batches | loss: 0.158\n",
            " |    2/   20 batches | loss: 0.155\n",
            " |    3/   20 batches | loss: 0.153\n",
            " |    4/   20 batches | loss: 0.148\n",
            " |    5/   20 batches | loss: 0.171\n",
            " |    6/   20 batches | loss: 0.159\n",
            " |    7/   20 batches | loss: 0.147\n",
            " |    8/   20 batches | loss: 0.157\n",
            " |    9/   20 batches | loss: 0.146\n",
            " |   10/   20 batches | loss: 0.147\n",
            " |   11/   20 batches | loss: 0.141\n",
            " |   12/   20 batches | loss: 0.156\n",
            " |   13/   20 batches | loss: 0.168\n",
            " |   14/   20 batches | loss: 0.159\n",
            " |   15/   20 batches | loss: 0.176\n",
            " |   16/   20 batches | loss: 0.15\n",
            " |   17/   20 batches | loss: 0.131\n",
            " |   18/   20 batches | loss: 0.163\n",
            " |   19/   20 batches | loss: 0.187\n",
            " |    0/   20 batches | loss: 0.182\n",
            " |    1/   20 batches | loss: 0.152\n",
            " |    2/   20 batches | loss: 0.159\n",
            " |    3/   20 batches | loss: 0.151\n",
            " |    4/   20 batches | loss: 0.144\n",
            " |    5/   20 batches | loss: 0.148\n",
            " |    6/   20 batches | loss: 0.16\n",
            " |    7/   20 batches | loss: 0.16\n",
            " |    8/   20 batches | loss: 0.171\n",
            " |    9/   20 batches | loss: 0.139\n",
            " |   10/   20 batches | loss: 0.159\n",
            " |   11/   20 batches | loss: 0.148\n",
            " |   12/   20 batches | loss: 0.15\n",
            " |   13/   20 batches | loss: 0.167\n",
            " |   14/   20 batches | loss: 0.148\n",
            " |   15/   20 batches | loss: 0.152\n",
            " |   16/   20 batches | loss: 0.14\n",
            " |   17/   20 batches | loss: 0.162\n",
            " |   18/   20 batches | loss: 0.15\n",
            " |   19/   20 batches | loss: 0.143\n",
            " |    0/   20 batches | loss: 0.152\n",
            " |    1/   20 batches | loss: 0.149\n",
            " |    2/   20 batches | loss: 0.15\n",
            " |    3/   20 batches | loss: 0.146\n",
            " |    4/   20 batches | loss: 0.159\n",
            " |    5/   20 batches | loss: 0.153\n",
            " |    6/   20 batches | loss: 0.149\n",
            " |    7/   20 batches | loss: 0.137\n",
            " |    8/   20 batches | loss: 0.154\n",
            " |    9/   20 batches | loss: 0.148\n",
            " |   10/   20 batches | loss: 0.142\n",
            " |   11/   20 batches | loss: 0.143\n",
            " |   12/   20 batches | loss: 0.147\n",
            " |   13/   20 batches | loss: 0.155\n",
            " |   14/   20 batches | loss: 0.152\n",
            " |   15/   20 batches | loss: 0.162\n",
            " |   16/   20 batches | loss: 0.16\n",
            " |   17/   20 batches | loss: 0.134\n",
            " |   18/   20 batches | loss: 0.183\n",
            " |   19/   20 batches | loss: 0.175\n",
            " |    0/   20 batches | loss: 0.163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 79%|███████▉  | 79/100 [00:05<00:00, 24.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " |    1/   20 batches | loss: 0.147\n",
            " |    2/   20 batches | loss: 0.16\n",
            " |    3/   20 batches | loss: 0.148\n",
            " |    4/   20 batches | loss: 0.145\n",
            " |    5/   20 batches | loss: 0.143\n",
            " |    6/   20 batches | loss: 0.145\n",
            " |    7/   20 batches | loss: 0.157\n",
            " |    8/   20 batches | loss: 0.158\n",
            " |    9/   20 batches | loss: 0.181\n",
            " |   10/   20 batches | loss: 0.168\n",
            " |   11/   20 batches | loss: 0.148\n",
            " |   12/   20 batches | loss: 0.14\n",
            " |   13/   20 batches | loss: 0.153\n",
            " |   14/   20 batches | loss: 0.149\n",
            " |   15/   20 batches | loss: 0.145\n",
            " |   16/   20 batches | loss: 0.133\n",
            " |   17/   20 batches | loss: 0.152\n",
            " |   18/   20 batches | loss: 0.157\n",
            " |   19/   20 batches | loss: 0.151\n",
            " |    0/   20 batches | loss: 0.133\n",
            " |    1/   20 batches | loss: 0.153\n",
            " |    2/   20 batches | loss: 0.16\n",
            " |    3/   20 batches | loss: 0.164\n",
            " |    4/   20 batches | loss: 0.153\n",
            " |    5/   20 batches | loss: 0.146\n",
            " |    6/   20 batches | loss: 0.152\n",
            " |    7/   20 batches | loss: 0.166\n",
            " |    8/   20 batches | loss: 0.144\n",
            " |    9/   20 batches | loss: 0.154\n",
            " |   10/   20 batches | loss: 0.159\n",
            " |   11/   20 batches | loss: 0.16\n",
            " |   12/   20 batches | loss: 0.137\n",
            " |   13/   20 batches | loss: 0.163\n",
            " |   14/   20 batches | loss: 0.156\n",
            " |   15/   20 batches | loss: 0.154\n",
            " |   16/   20 batches | loss: 0.139\n",
            " |   17/   20 batches | loss: 0.148\n",
            " |   18/   20 batches | loss: 0.15\n",
            " |   19/   20 batches | loss: 0.124\n",
            " |    0/   20 batches | loss: 0.142\n",
            " |    1/   20 batches | loss: 0.152\n",
            " |    2/   20 batches | loss: 0.148\n",
            " |    3/   20 batches | loss: 0.169\n",
            " |    4/   20 batches | loss: 0.145\n",
            " |    5/   20 batches | loss: 0.138\n",
            " |    6/   20 batches | loss: 0.162\n",
            " |    7/   20 batches | loss: 0.129\n",
            " |    8/   20 batches | loss: 0.169\n",
            " |    9/   20 batches | loss: 0.15\n",
            " |   10/   20 batches | loss: 0.143\n",
            " |   11/   20 batches | loss: 0.175\n",
            " |   12/   20 batches | loss: 0.156\n",
            " |   13/   20 batches | loss: 0.14\n",
            " |   14/   20 batches | loss: 0.148\n",
            " |   15/   20 batches | loss: 0.147\n",
            " |   16/   20 batches | loss: 0.133\n",
            " |   17/   20 batches | loss: 0.132\n",
            " |   18/   20 batches | loss: 0.151\n",
            " |   19/   20 batches | loss: 0.135\n",
            " |    0/   20 batches | loss: 0.14\n",
            " |    1/   20 batches | loss: 0.155\n",
            " |    2/   20 batches | loss: 0.152\n",
            " |    3/   20 batches | loss: 0.15\n",
            " |    4/   20 batches | loss: 0.139\n",
            " |    5/   20 batches | loss: 0.145\n",
            " |    6/   20 batches | loss: 0.141\n",
            " |    7/   20 batches | loss: 0.145\n",
            " |    8/   20 batches | loss: 0.134\n",
            " |    9/   20 batches | loss: 0.157\n",
            " |   10/   20 batches | loss: 0.147\n",
            " |   11/   20 batches | loss: 0.148\n",
            " |   12/   20 batches | loss: 0.144\n",
            " |   13/   20 batches | loss: 0.146\n",
            " |   14/   20 batches | loss: 0.138\n",
            " |   15/   20 batches | loss: 0.163\n",
            " |   16/   20 batches | loss: 0.144\n",
            " |   17/   20 batches | loss: 0.152\n",
            " |   18/   20 batches | loss: 0.15\n",
            " |   19/   20 batches | loss: 0.152\n",
            " |    0/   20 batches | loss: 0.146\n",
            " |    1/   20 batches | loss: 0.15\n",
            " |    2/   20 batches | loss: 0.133\n",
            " |    3/   20 batches | loss: 0.158\n",
            " |    4/   20 batches | loss: 0.141\n",
            " |    5/   20 batches | loss: 0.143\n",
            " |    6/   20 batches | loss: 0.15\n",
            " |    7/   20 batches | loss: 0.15\n",
            " |    8/   20 batches | loss: 0.163\n",
            " |    9/   20 batches | loss: 0.143\n",
            " |   10/   20 batches | loss: 0.143\n",
            " |   11/   20 batches | loss: 0.164\n",
            " |   12/   20 batches | loss: 0.13\n",
            " |   13/   20 batches | loss: 0.143\n",
            " |   14/   20 batches | loss: 0.146\n",
            " |   15/   20 batches | loss: 0.157\n",
            " |   16/   20 batches | loss: 0.154\n",
            " |   17/   20 batches | loss: 0.132\n",
            " |   18/   20 batches | loss: 0.149\n",
            " |   19/   20 batches | loss: 0.149\n",
            " |    0/   20 batches | loss: 0.147\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 82%|████████▏ | 82/100 [00:06<00:00, 24.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " |    1/   20 batches | loss: 0.148\n",
            " |    2/   20 batches | loss: 0.144\n",
            " |    3/   20 batches | loss: 0.144\n",
            " |    4/   20 batches | loss: 0.144\n",
            " |    5/   20 batches | loss: 0.156\n",
            " |    6/   20 batches | loss: 0.167\n",
            " |    7/   20 batches | loss: 0.127\n",
            " |    8/   20 batches | loss: 0.148\n",
            " |    9/   20 batches | loss: 0.136\n",
            " |   10/   20 batches | loss: 0.148\n",
            " |   11/   20 batches | loss: 0.138\n",
            " |   12/   20 batches | loss: 0.133\n",
            " |   13/   20 batches | loss: 0.157\n",
            " |   14/   20 batches | loss: 0.147\n",
            " |   15/   20 batches | loss: 0.146\n",
            " |   16/   20 batches | loss: 0.144\n",
            " |   17/   20 batches | loss: 0.158\n",
            " |   18/   20 batches | loss: 0.156\n",
            " |   19/   20 batches | loss: 0.134\n",
            " |    0/   20 batches | loss: 0.145\n",
            " |    1/   20 batches | loss: 0.143\n",
            " |    2/   20 batches | loss: 0.143\n",
            " |    3/   20 batches | loss: 0.142\n",
            " |    4/   20 batches | loss: 0.144\n",
            " |    5/   20 batches | loss: 0.155\n",
            " |    6/   20 batches | loss: 0.14\n",
            " |    7/   20 batches | loss: 0.137\n",
            " |    8/   20 batches | loss: 0.14\n",
            " |    9/   20 batches | loss: 0.145\n",
            " |   10/   20 batches | loss: 0.141\n",
            " |   11/   20 batches | loss: 0.141\n",
            " |   12/   20 batches | loss: 0.152\n",
            " |   13/   20 batches | loss: 0.15\n",
            " |   14/   20 batches | loss: 0.137\n",
            " |   15/   20 batches | loss: 0.154\n",
            " |   16/   20 batches | loss: 0.123\n",
            " |   17/   20 batches | loss: 0.158\n",
            " |   18/   20 batches | loss: 0.143\n",
            " |   19/   20 batches | loss: 0.139\n",
            " |    0/   20 batches | loss: 0.14\n",
            " |    1/   20 batches | loss: 0.148\n",
            " |    2/   20 batches | loss: 0.154\n",
            " |    3/   20 batches | loss: 0.152\n",
            " |    4/   20 batches | loss: 0.159\n",
            " |    5/   20 batches | loss: 0.135\n",
            " |    6/   20 batches | loss: 0.149\n",
            " |    7/   20 batches | loss: 0.136\n",
            " |    8/   20 batches | loss: 0.136\n",
            " |    9/   20 batches | loss: 0.128\n",
            " |   10/   20 batches | loss: 0.152\n",
            " |   11/   20 batches | loss: 0.156\n",
            " |   12/   20 batches | loss: 0.143\n",
            " |   13/   20 batches | loss: 0.134\n",
            " |   14/   20 batches | loss: 0.139\n",
            " |   15/   20 batches | loss: 0.134\n",
            " |   16/   20 batches | loss: 0.14\n",
            " |   17/   20 batches | loss: 0.152\n",
            " |   18/   20 batches | loss: 0.151\n",
            " |   19/   20 batches | loss: 0.138\n",
            " |    0/   20 batches | loss: 0.144\n",
            " |    1/   20 batches | loss: 0.139\n",
            " |    2/   20 batches | loss: 0.139\n",
            " |    3/   20 batches | loss: 0.139\n",
            " |    4/   20 batches | loss: 0.125\n",
            " |    5/   20 batches | loss: 0.156\n",
            " |    6/   20 batches | loss: 0.135\n",
            " |    7/   20 batches | loss: 0.156\n",
            " |    8/   20 batches | loss: 0.143\n",
            " |    9/   20 batches | loss: 0.167\n",
            " |   10/   20 batches | loss: 0.128\n",
            " |   11/   20 batches | loss: 0.139\n",
            " |   12/   20 batches | loss: 0.14\n",
            " |   13/   20 batches | loss: 0.13\n",
            " |   14/   20 batches | loss: 0.133\n",
            " |   15/   20 batches | loss: 0.156\n",
            " |   16/   20 batches | loss: 0.154\n",
            " |   17/   20 batches | loss: 0.151\n",
            " |   18/   20 batches | loss: 0.146\n",
            " |   19/   20 batches | loss: 0.127\n",
            " |    0/   20 batches | loss: 0.132\n",
            " |    1/   20 batches | loss: 0.13\n",
            " |    2/   20 batches | loss: 0.148\n",
            " |    3/   20 batches | loss: 0.159\n",
            " |    4/   20 batches | loss: 0.124\n",
            " |    5/   20 batches | loss: 0.145\n",
            " |    6/   20 batches | loss: 0.15\n",
            " |    7/   20 batches | loss: 0.166\n",
            " |    8/   20 batches | loss: 0.172\n",
            " |    9/   20 batches | loss: 0.144\n",
            " |   10/   20 batches | loss: 0.159\n",
            " |   11/   20 batches | loss: 0.127\n",
            " |   12/   20 batches | loss: 0.128\n",
            " |   13/   20 batches | loss: 0.129\n",
            " |   14/   20 batches | loss: 0.138\n",
            " |   15/   20 batches | loss: 0.142\n",
            " |   16/   20 batches | loss: 0.138\n",
            " |   17/   20 batches | loss: 0.155\n",
            " |   18/   20 batches | loss: 0.144\n",
            " |   19/   20 batches | loss: 0.139\n",
            " |    0/   20 batches | loss: 0.141\n",
            " |    1/   20 batches | loss: 0.155\n",
            " |    2/   20 batches | loss: 0.132\n",
            " |    3/   20 batches | loss: 0.151\n",
            " |    4/   20 batches | loss: 0.144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 88%|████████▊ | 88/100 [00:06<00:00, 25.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " |    5/   20 batches | loss: 0.121\n",
            " |    6/   20 batches | loss: 0.145\n",
            " |    7/   20 batches | loss: 0.14\n",
            " |    8/   20 batches | loss: 0.151\n",
            " |    9/   20 batches | loss: 0.15\n",
            " |   10/   20 batches | loss: 0.129\n",
            " |   11/   20 batches | loss: 0.143\n",
            " |   12/   20 batches | loss: 0.135\n",
            " |   13/   20 batches | loss: 0.136\n",
            " |   14/   20 batches | loss: 0.134\n",
            " |   15/   20 batches | loss: 0.126\n",
            " |   16/   20 batches | loss: 0.147\n",
            " |   17/   20 batches | loss: 0.128\n",
            " |   18/   20 batches | loss: 0.14\n",
            " |   19/   20 batches | loss: 0.132\n",
            " |    0/   20 batches | loss: 0.141\n",
            " |    1/   20 batches | loss: 0.139\n",
            " |    2/   20 batches | loss: 0.123\n",
            " |    3/   20 batches | loss: 0.142\n",
            " |    4/   20 batches | loss: 0.142\n",
            " |    5/   20 batches | loss: 0.146\n",
            " |    6/   20 batches | loss: 0.137\n",
            " |    7/   20 batches | loss: 0.139\n",
            " |    8/   20 batches | loss: 0.139\n",
            " |    9/   20 batches | loss: 0.121\n",
            " |   10/   20 batches | loss: 0.147\n",
            " |   11/   20 batches | loss: 0.152\n",
            " |   12/   20 batches | loss: 0.138\n",
            " |   13/   20 batches | loss: 0.13\n",
            " |   14/   20 batches | loss: 0.134\n",
            " |   15/   20 batches | loss: 0.133\n",
            " |   16/   20 batches | loss: 0.132\n",
            " |   17/   20 batches | loss: 0.135\n",
            " |   18/   20 batches | loss: 0.136\n",
            " |   19/   20 batches | loss: 0.12\n",
            " |    0/   20 batches | loss: 0.13\n",
            " |    1/   20 batches | loss: 0.122\n",
            " |    2/   20 batches | loss: 0.138\n",
            " |    3/   20 batches | loss: 0.132\n",
            " |    4/   20 batches | loss: 0.148\n",
            " |    5/   20 batches | loss: 0.131\n",
            " |    6/   20 batches | loss: 0.137\n",
            " |    7/   20 batches | loss: 0.13\n",
            " |    8/   20 batches | loss: 0.136\n",
            " |    9/   20 batches | loss: 0.147\n",
            " |   10/   20 batches | loss: 0.117\n",
            " |   11/   20 batches | loss: 0.142\n",
            " |   12/   20 batches | loss: 0.131\n",
            " |   13/   20 batches | loss: 0.135\n",
            " |   14/   20 batches | loss: 0.14\n",
            " |   15/   20 batches | loss: 0.129\n",
            " |   16/   20 batches | loss: 0.129\n",
            " |   17/   20 batches | loss: 0.136\n",
            " |   18/   20 batches | loss: 0.162\n",
            " |   19/   20 batches | loss: 0.143\n",
            " |    0/   20 batches | loss: 0.142\n",
            " |    1/   20 batches | loss: 0.135\n",
            " |    2/   20 batches | loss: 0.142\n",
            " |    3/   20 batches | loss: 0.136\n",
            " |    4/   20 batches | loss: 0.131\n",
            " |    5/   20 batches | loss: 0.142\n",
            " |    6/   20 batches | loss: 0.127\n",
            " |    7/   20 batches | loss: 0.141\n",
            " |    8/   20 batches | loss: 0.137\n",
            " |    9/   20 batches | loss: 0.126\n",
            " |   10/   20 batches | loss: 0.137\n",
            " |   11/   20 batches | loss: 0.138\n",
            " |   12/   20 batches | loss: 0.131\n",
            " |   13/   20 batches | loss: 0.136\n",
            " |   14/   20 batches | loss: 0.132\n",
            " |   15/   20 batches | loss: 0.142\n",
            " |   16/   20 batches | loss: 0.133\n",
            " |   17/   20 batches | loss: 0.141\n",
            " |   18/   20 batches | loss: 0.14\n",
            " |   19/   20 batches | loss: 0.122\n",
            " |    0/   20 batches | loss: 0.134\n",
            " |    1/   20 batches | loss: 0.139\n",
            " |    2/   20 batches | loss: 0.139\n",
            " |    3/   20 batches | loss: 0.146\n",
            " |    4/   20 batches | loss: 0.131\n",
            " |    5/   20 batches | loss: 0.136\n",
            " |    6/   20 batches | loss: 0.135\n",
            " |    7/   20 batches | loss: 0.123\n",
            " |    8/   20 batches | loss: 0.141\n",
            " |    9/   20 batches | loss: 0.134\n",
            " |   10/   20 batches | loss: 0.135\n",
            " |   11/   20 batches | loss: 0.144\n",
            " |   12/   20 batches | loss: 0.128\n",
            " |   13/   20 batches | loss: 0.117\n",
            " |   14/   20 batches | loss: 0.138\n",
            " |   15/   20 batches | loss: 0.129\n",
            " |   16/   20 batches | loss: 0.132\n",
            " |   17/   20 batches | loss: 0.128\n",
            " |   18/   20 batches | loss: 0.137\n",
            " |   19/   20 batches | loss: 0.129\n",
            " |    0/   20 batches | loss: 0.125\n",
            " |    1/   20 batches | loss: 0.136\n",
            " |    2/   20 batches | loss: 0.119\n",
            " |    3/   20 batches | loss: 0.133\n",
            " |    4/   20 batches | loss: 0.13\n",
            " |    5/   20 batches | loss: 0.144\n",
            " |    6/   20 batches | loss: 0.137\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 94%|█████████▍| 94/100 [00:06<00:00, 25.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " |    7/   20 batches | loss: 0.123\n",
            " |    8/   20 batches | loss: 0.132\n",
            " |    9/   20 batches | loss: 0.121\n",
            " |   10/   20 batches | loss: 0.15\n",
            " |   11/   20 batches | loss: 0.155\n",
            " |   12/   20 batches | loss: 0.146\n",
            " |   13/   20 batches | loss: 0.126\n",
            " |   14/   20 batches | loss: 0.123\n",
            " |   15/   20 batches | loss: 0.136\n",
            " |   16/   20 batches | loss: 0.145\n",
            " |   17/   20 batches | loss: 0.13\n",
            " |   18/   20 batches | loss: 0.122\n",
            " |   19/   20 batches | loss: 0.117\n",
            " |    0/   20 batches | loss: 0.139\n",
            " |    1/   20 batches | loss: 0.14\n",
            " |    2/   20 batches | loss: 0.128\n",
            " |    3/   20 batches | loss: 0.138\n",
            " |    4/   20 batches | loss: 0.136\n",
            " |    5/   20 batches | loss: 0.143\n",
            " |    6/   20 batches | loss: 0.135\n",
            " |    7/   20 batches | loss: 0.137\n",
            " |    8/   20 batches | loss: 0.118\n",
            " |    9/   20 batches | loss: 0.12\n",
            " |   10/   20 batches | loss: 0.136\n",
            " |   11/   20 batches | loss: 0.123\n",
            " |   12/   20 batches | loss: 0.138\n",
            " |   13/   20 batches | loss: 0.13\n",
            " |   14/   20 batches | loss: 0.127\n",
            " |   15/   20 batches | loss: 0.136\n",
            " |   16/   20 batches | loss: 0.135\n",
            " |   17/   20 batches | loss: 0.115\n",
            " |   18/   20 batches | loss: 0.124\n",
            " |   19/   20 batches | loss: 0.144\n",
            " |    0/   20 batches | loss: 0.134\n",
            " |    1/   20 batches | loss: 0.122\n",
            " |    2/   20 batches | loss: 0.138\n",
            " |    3/   20 batches | loss: 0.129\n",
            " |    4/   20 batches | loss: 0.13\n",
            " |    5/   20 batches | loss: 0.116\n",
            " |    6/   20 batches | loss: 0.123\n",
            " |    7/   20 batches | loss: 0.132\n",
            " |    8/   20 batches | loss: 0.133\n",
            " |    9/   20 batches | loss: 0.138\n",
            " |   10/   20 batches | loss: 0.131\n",
            " |   11/   20 batches | loss: 0.123\n",
            " |   12/   20 batches | loss: 0.123\n",
            " |   13/   20 batches | loss: 0.144\n",
            " |   14/   20 batches | loss: 0.145\n",
            " |   15/   20 batches | loss: 0.142\n",
            " |   16/   20 batches | loss: 0.123\n",
            " |   17/   20 batches | loss: 0.135\n",
            " |   18/   20 batches | loss: 0.129\n",
            " |   19/   20 batches | loss: 0.12\n",
            " |    0/   20 batches | loss: 0.127\n",
            " |    1/   20 batches | loss: 0.121\n",
            " |    2/   20 batches | loss: 0.132\n",
            " |    3/   20 batches | loss: 0.131\n",
            " |    4/   20 batches | loss: 0.112\n",
            " |    5/   20 batches | loss: 0.147\n",
            " |    6/   20 batches | loss: 0.123\n",
            " |    7/   20 batches | loss: 0.132\n",
            " |    8/   20 batches | loss: 0.134\n",
            " |    9/   20 batches | loss: 0.128\n",
            " |   10/   20 batches | loss: 0.127\n",
            " |   11/   20 batches | loss: 0.131\n",
            " |   12/   20 batches | loss: 0.138\n",
            " |   13/   20 batches | loss: 0.132\n",
            " |   14/   20 batches | loss: 0.123\n",
            " |   15/   20 batches | loss: 0.123\n",
            " |   16/   20 batches | loss: 0.126\n",
            " |   17/   20 batches | loss: 0.135\n",
            " |   18/   20 batches | loss: 0.13\n",
            " |   19/   20 batches | loss: 0.123\n",
            " |    0/   20 batches | loss: 0.13\n",
            " |    1/   20 batches | loss: 0.121\n",
            " |    2/   20 batches | loss: 0.113\n",
            " |    3/   20 batches | loss: 0.119\n",
            " |    4/   20 batches | loss: 0.114\n",
            " |    5/   20 batches | loss: 0.122\n",
            " |    6/   20 batches | loss: 0.11\n",
            " |    7/   20 batches | loss: 0.137\n",
            " |    8/   20 batches | loss: 0.124\n",
            " |    9/   20 batches | loss: 0.124\n",
            " |   10/   20 batches | loss: 0.134\n",
            " |   11/   20 batches | loss: 0.134\n",
            " |   12/   20 batches | loss: 0.124\n",
            " |   13/   20 batches | loss: 0.124\n",
            " |   14/   20 batches | loss: 0.133\n",
            " |   15/   20 batches | loss: 0.131\n",
            " |   16/   20 batches | loss: 0.131\n",
            " |   17/   20 batches | loss: 0.149\n",
            " |   18/   20 batches | loss: 0.136\n",
            " |   19/   20 batches | loss: 0.145\n",
            " |    0/   20 batches | loss: 0.136\n",
            " |    1/   20 batches | loss: 0.152\n",
            " |    2/   20 batches | loss: 0.12\n",
            " |    3/   20 batches | loss: 0.129\n",
            " |    4/   20 batches | loss: 0.12\n",
            " |    5/   20 batches | loss: 0.135\n",
            " |    6/   20 batches | loss: 0.134\n",
            " |    7/   20 batches | loss: 0.157\n",
            " |    8/   20 batches | loss: 0.141\n",
            " |    9/   20 batches | loss: 0.114\n",
            " |   10/   20 batches | loss: 0.133\n",
            " |   11/   20 batches | loss: 0.127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 97%|█████████▋| 97/100 [00:06<00:00, 25.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " |   12/   20 batches | loss: 0.121\n",
            " |   13/   20 batches | loss: 0.13\n",
            " |   14/   20 batches | loss: 0.112\n",
            " |   15/   20 batches | loss: 0.121\n",
            " |   16/   20 batches | loss: 0.134\n",
            " |   17/   20 batches | loss: 0.126\n",
            " |   18/   20 batches | loss: 0.124\n",
            " |   19/   20 batches | loss: 0.115\n",
            " |    0/   20 batches | loss: 0.138\n",
            " |    1/   20 batches | loss: 0.125\n",
            " |    2/   20 batches | loss: 0.109\n",
            " |    3/   20 batches | loss: 0.129\n",
            " |    4/   20 batches | loss: 0.137\n",
            " |    5/   20 batches | loss: 0.114\n",
            " |    6/   20 batches | loss: 0.123\n",
            " |    7/   20 batches | loss: 0.138\n",
            " |    8/   20 batches | loss: 0.119\n",
            " |    9/   20 batches | loss: 0.149\n",
            " |   10/   20 batches | loss: 0.124\n",
            " |   11/   20 batches | loss: 0.134\n",
            " |   12/   20 batches | loss: 0.125\n",
            " |   13/   20 batches | loss: 0.123\n",
            " |   14/   20 batches | loss: 0.124\n",
            " |   15/   20 batches | loss: 0.119\n",
            " |   16/   20 batches | loss: 0.133\n",
            " |   17/   20 batches | loss: 0.127\n",
            " |   18/   20 batches | loss: 0.12\n",
            " |   19/   20 batches | loss: 0.14\n",
            " |    0/   20 batches | loss: 0.158\n",
            " |    1/   20 batches | loss: 0.146\n",
            " |    2/   20 batches | loss: 0.15\n",
            " |    3/   20 batches | loss: 0.141\n",
            " |    4/   20 batches | loss: 0.14\n",
            " |    5/   20 batches | loss: 0.126\n",
            " |    6/   20 batches | loss: 0.117\n",
            " |    7/   20 batches | loss: 0.124\n",
            " |    8/   20 batches | loss: 0.124\n",
            " |    9/   20 batches | loss: 0.122\n",
            " |   10/   20 batches | loss: 0.101\n",
            " |   11/   20 batches | loss: 0.142\n",
            " |   12/   20 batches | loss: 0.124\n",
            " |   13/   20 batches | loss: 0.124\n",
            " |   14/   20 batches | loss: 0.129\n",
            " |   15/   20 batches | loss: 0.145\n",
            " |   16/   20 batches | loss: 0.133\n",
            " |   17/   20 batches | loss: 0.118\n",
            " |   18/   20 batches | loss: 0.107\n",
            " |   19/   20 batches | loss: 0.137\n",
            " |    0/   20 batches | loss: 0.14\n",
            " |    1/   20 batches | loss: 0.129\n",
            " |    2/   20 batches | loss: 0.14\n",
            " |    3/   20 batches | loss: 0.118\n",
            " |    4/   20 batches | loss: 0.125\n",
            " |    5/   20 batches | loss: 0.116\n",
            " |    6/   20 batches | loss: 0.126\n",
            " |    7/   20 batches | loss: 0.124\n",
            " |    8/   20 batches | loss: 0.112\n",
            " |    9/   20 batches | loss: 0.122\n",
            " |   10/   20 batches | loss: 0.131\n",
            " |   11/   20 batches | loss: 0.122\n",
            " |   12/   20 batches | loss: 0.13\n",
            " |   13/   20 batches | loss: 0.112\n",
            " |   14/   20 batches | loss: 0.109\n",
            " |   15/   20 batches | loss: 0.132\n",
            " |   16/   20 batches | loss: 0.122\n",
            " |   17/   20 batches | loss: 0.117\n",
            " |   18/   20 batches | loss: 0.114\n",
            " |   19/   20 batches | loss: 0.112\n",
            " |    0/   20 batches | loss: 0.117\n",
            " |    1/   20 batches | loss: 0.11\n",
            " |    2/   20 batches | loss: 0.143\n",
            " |    3/   20 batches | loss: 0.139\n",
            " |    4/   20 batches | loss: 0.105\n",
            " |    5/   20 batches | loss: 0.123\n",
            " |    6/   20 batches | loss: 0.117\n",
            " |    7/   20 batches | loss: 0.121\n",
            " |    8/   20 batches | loss: 0.123\n",
            " |    9/   20 batches | loss: 0.122\n",
            " |   10/   20 batches | loss: 0.125\n",
            " |   11/   20 batches | loss: 0.122\n",
            " |   12/   20 batches | loss: 0.133\n",
            " |   13/   20 batches | loss: 0.127\n",
            " |   14/   20 batches | loss: 0.127\n",
            " |   15/   20 batches | loss: 0.123\n",
            " |   16/   20 batches | loss: 0.136\n",
            " |   17/   20 batches | loss: 0.111\n",
            " |   18/   20 batches | loss: 0.127\n",
            " |   19/   20 batches | loss: 0.138\n",
            " |    0/   20 batches | loss: 0.125\n",
            " |    1/   20 batches | loss: 0.0981\n",
            " |    2/   20 batches | loss: 0.13\n",
            " |    3/   20 batches | loss: 0.119\n",
            " |    4/   20 batches | loss: 0.114\n",
            " |    5/   20 batches | loss: 0.116\n",
            " |    6/   20 batches | loss: 0.122\n",
            " |    7/   20 batches | loss: 0.131\n",
            " |    8/   20 batches | loss: 0.131\n",
            " |    9/   20 batches | loss: 0.121\n",
            " |   10/   20 batches | loss: 0.124\n",
            " |   11/   20 batches | loss: 0.131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:06<00:00, 14.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " |   12/   20 batches | loss: 0.123\n",
            " |   13/   20 batches | loss: 0.111\n",
            " |   14/   20 batches | loss: 0.113\n",
            " |   15/   20 batches | loss: 0.115\n",
            " |   16/   20 batches | loss: 0.129\n",
            " |   17/   20 batches | loss: 0.113\n",
            " |   18/   20 batches | loss: 0.12\n",
            " |   19/   20 batches | loss: 0.141\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "train_model(model, optimizer, train_data_loader, loss_module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS2adzZRf4p1"
      },
      "source": [
        "A detailed tutorial on saving and loading models in PyTorch can be found [here](https://pytorch.org/tutorials/beginner/saving_loading_models.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BXGuKTBf4p1"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Once we have trained a model, it is time to evaluate it on a held-out test set. As our dataset consist of randomly generated data points, we need to first create a test set with a corresponding data loader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0693iZ-f4p1"
      },
      "outputs": [],
      "source": [
        "test_dataset = CustomDataset(size=500)\n",
        "# drop_last -> Don't drop the last batch although it is smaller than 128\n",
        "test_data_loader = data.DataLoader(test_dataset, batch_size=128, shuffle=False, drop_last=False) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vsvic-Tf4p1"
      },
      "source": [
        "As metric, we will use accuracy which is calculated as follows:\n",
        "\n",
        "$$acc = \\frac{\\#\\text{correct predictions}}{\\#\\text{all predictions}} = \\frac{TP+TN}{TP+TN+FP+FN}$$\n",
        "\n",
        "where TP are the true positives, TN true negatives, FP false positives, and FN the fale negatives. \n",
        "\n",
        "When evaluating the model, we don't need to keep track of the computation graph as we don't intend to calculate the gradients. This reduces the required memory and speed up the model. In PyTorch, we can deactivate the computation graph using `with torch.no_grad(): ...`. Remember to additionally set the model to eval mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxupM9B1f4p1"
      },
      "outputs": [],
      "source": [
        "softmax = nn.Softmax(dim=1)\n",
        "\n",
        "def eval_model(model, data_loader):\n",
        "    model.eval() # Set model to eval mode\n",
        "    true_preds, num_preds = 0., 0.\n",
        "    \n",
        "    with torch.no_grad(): # Deactivate gradients for the following code\n",
        "        for data_inputs, data_labels in data_loader:\n",
        "            \n",
        "            # Determine prediction of model on dev set\n",
        "            data_inputs, data_labels = data_inputs.to(device), data_labels.to(device)\n",
        "            preds = model(data_inputs)\n",
        "            preds = softmax(preds)\n",
        "            pred_labels = torch.argmax(preds, dim=1)\n",
        "            \n",
        "            # Keep records of predictions for the accuracy metric (true_preds=TP+TN, num_preds=TP+TN+FP+FN)\n",
        "            true_preds += (pred_labels == data_labels).sum()\n",
        "            num_preds += data_labels.shape[0]\n",
        "            \n",
        "    acc = true_preds / num_preds\n",
        "    print(f\"Accuracy of the model: {100.0*acc:4.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xL3dPyZIf4p1",
        "outputId": "aa31b94a-0948-4346-9af5-83a0a75f5af3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model: 99.40%\n"
          ]
        }
      ],
      "source": [
        "eval_model(model, test_data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sample = torch.randint(low=-1000, high=1000, size=(1, 2), dtype=torch.float32)\n",
        "test_sample"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YFXw8skj7M3",
        "outputId": "734905f7-eb63-4e2b-b177-a14ffd4bcfa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-499.,  945.]])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model(test_sample.to(device))\n",
        "softmax = nn.Softmax(dim=1)\n",
        "pred = softmax(pred)\n",
        "pred = torch.argmax(pred, dim=1)\n",
        "pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZZt4kFBkLP_",
        "outputId": "f01ef3b5-9809-4816-cc76-891837c9433c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Odd/even classification\n",
        "Now let's train a network for classifying wheter sum of two numbers is even or odd. Given 2 numbers $x_1$ and $x_2$, the goal is to predict label $0$ if $x_1+x_2$ is even and $1$ if it is odd."
      ],
      "metadata": {
        "id": "-9QjIJlHUkvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, size, max_range=1000):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            size - Number of data points we want to generate\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "        self.max_range = max_range\n",
        "        self.generate_sample_data()\n",
        "\n",
        "    def generate_sample_data(self):\n",
        "        data = torch.randint(low=0, high=self.max_range, size=(self.size, 2), dtype=torch.float32)\n",
        "        label = ((data.sum(dim=1)%2) == 1).to(torch.float)\n",
        "\n",
        "        self.data = data\n",
        "        self.label = label\n",
        "\n",
        "    def __len__(self):\n",
        "        # Number of data point we have. Alternatively self.data.shape[0], or self.label.shape[0]\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return the idx-th data point of the dataset\n",
        "        # If we have multiple things to return (data point and label), we can return them as tuple\n",
        "        data_point = self.data[idx]\n",
        "        data_label = self.label[idx]\n",
        "        return data_point, data_label"
      ],
      "metadata": {
        "id": "KWIwYqy2qnWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = CustomDataset(size=200)\n",
        "print(\"Size of dataset:\", len(dataset))\n",
        "print(\"Data point 0:\", dataset[0])"
      ],
      "metadata": {
        "id": "xzGOyp6Fqqbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def visualize_samples(data, label):\n",
        "    if isinstance(data, torch.Tensor):\n",
        "        data = data.detach().numpy()\n",
        "    if isinstance(label, torch.Tensor):\n",
        "        label = label.detach().numpy()\n",
        "    data_0 = data[label == 0]\n",
        "    data_1 = data[label == 1]\n",
        "    \n",
        "    plt.figure(figsize=(4,4))\n",
        "    plt.scatter(data_0[:,0], data_0[:,1], edgecolor=\"#333\", label=\"Class 0\")\n",
        "    plt.scatter(data_1[:,0], data_1[:,1], edgecolor=\"#333\", label=\"Class 1\")\n",
        "    plt.title(\"Dataset samples\")\n",
        "    plt.ylabel(r\"$x_2$\")\n",
        "    plt.xlabel(r\"$x_1$\")\n",
        "    plt.legend()"
      ],
      "metadata": {
        "id": "rtkvFr5hqq_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, num_inputs, num_hidden, num_outputs):\n",
        "        super().__init__()\n",
        "        # Initialize the modules we need to build the network\n",
        "        self.linear1 = nn.Linear(num_inputs, num_hidden)\n",
        "        self.act_fn = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(num_hidden, num_hidden)\n",
        "        self.linear3 = nn.Linear(num_hidden, num_outputs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Perform the calculation of the model to determine the prediction\n",
        "        x = self.linear1(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.act_fn(x)\n",
        "        output = self.linear3(x)\n",
        "        return output"
      ],
      "metadata": {
        "id": "fv0Q06b5U48k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleClassifier(num_inputs=2, num_hidden=4, num_outputs=2)\n",
        "# Printing a module shows all its submodules\n",
        "print(model)"
      ],
      "metadata": {
        "id": "ChBmAOW7qjuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input to the optimizer are the parameters of the model: model.parameters()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "FTvD9CUAqwyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(size=2500)\n",
        "train_data_loader = data.DataLoader(train_dataset, batch_size=128, shuffle=True)"
      ],
      "metadata": {
        "id": "PKgMX7ZTqzKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Push model to device. Has to be only done once\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "vLD1MY-oq05Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(model, optimizer, data_loader, loss_module, num_epochs=100):\n",
        "    # Set model to train mode\n",
        "    model.train() \n",
        "    \n",
        "    # Training loop\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "        for idx, (data_inputs, data_labels) in enumerate(data_loader):\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            data_inputs = data_inputs.to(device)\n",
        "            data_labels = data_labels.to(device)\n",
        "            \n",
        "            preds = model(data_inputs)\n",
        "\n",
        "            loss = loss_module(preds, data_labels)\n",
        "            print(f' |{idx:5d}/{len(data_loader):5d} batches | loss: {loss:.3}')\n",
        " \n",
        "            loss.backward()\n",
        "            \n",
        "            optimizer.step()"
      ],
      "metadata": {
        "id": "rLTVF_euq29I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = CustomDataset(size=500)\n",
        "# drop_last -> Don't drop the last batch although it is smaller than 128\n",
        "test_data_loader = data.DataLoader(test_dataset, batch_size=128, shuffle=False, drop_last=False) "
      ],
      "metadata": {
        "id": "Gy6DyiJtrAzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softmax = nn.Softmax(dim=1)\n",
        "\n",
        "def eval_model(model, data_loader):\n",
        "    model.eval() # Set model to eval mode\n",
        "    true_preds, num_preds = 0., 0.\n",
        "    \n",
        "    with torch.no_grad(): # Deactivate gradients for the following code\n",
        "        for data_inputs, data_labels in data_loader:\n",
        "            \n",
        "            # Determine prediction of model on dev set\n",
        "            data_inputs, data_labels = data_inputs.to(device), data_labels.to(device)\n",
        "            preds = model(data_inputs)\n",
        "            preds = softmax(preds)\n",
        "            pred_labels = torch.argmax(preds, dim=1)\n",
        "            \n",
        "            # Keep records of predictions for the accuracy metric (true_preds=TP+TN, num_preds=TP+TN+FP+FN)\n",
        "            true_preds += (pred_labels == data_labels).sum()\n",
        "            num_preds += data_labels.shape[0]\n",
        "            \n",
        "    acc = true_preds / num_preds\n",
        "    print(f\"Accuracy of the model: {100.0*acc:4.2f}%\")"
      ],
      "metadata": {
        "id": "ZuxQv0swrDkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_model(model, test_data_loader)"
      ],
      "metadata": {
        "id": "KdMf3jrArFoX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}