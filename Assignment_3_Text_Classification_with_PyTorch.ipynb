{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hieutdle/hpi-nlp/blob/main/Assignment_3_Text_Classification_with_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSb3LpeChepX"
      },
      "source": [
        "## Exercise \"Natural Language Processing\" -- Text Classification with PyTorch\n",
        "\n",
        "\n",
        "For this course, save a COPY to your Google Drive for the tutorial (File -> Save copy in Drive). Then complete the tasks in your saved copy. If you're done, submit the notebook via moodle by sharing a link with the appropriate permissions (preferred, but please do not make changes after the deadline) or by submitting the downloaded `.ipynb` file.\n",
        "\n",
        "This is an individual assignment, i.e., submit your solutions individually.\n",
        "\n",
        "This assignment is **mandatory for participation in the exam**. You are required to obtain at least 50% of the total points in this assignment to become eligible for participating in the final exam.\n",
        "\n",
        "**This assignment will take some time**, particularly if you do not have prior experience with PyTorch. Do not just start the day before the deadline. There are 8 individual tasks, however most of them will be just a few lines of code.\n",
        "\n",
        "**Due date: 25.05.2023, 9:15 a.m.(CEST)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will need to install the following dependency and **restart** the Colab runtime (as of April '23)."
      ],
      "metadata": {
        "id": "UWDJ1QC0Oqcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install portalocker>=2.0.0"
      ],
      "metadata": {
        "id": "CZcKsUtpv333"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment, we'll build and train a Neural Network from scratch for Text Classification using PyTorch (`import torch`). We will use the `AG_NEWS` dataset to train a model that can classifiy news articles by topic.\n",
        "\n",
        "We will complete the following steps in this notebook:\n",
        "- Define a tokenizer and preprocessing pipeline for our dataset\n",
        "- Develop a Neural Network architecture using **word embeddings** (in contrast to the document-level bag-of-words emebddings from the last assignments)\n",
        "- Define a PyTorch training loop and relevant hyperparameters\n",
        "- Have fun!\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YvkUs-TQPBeq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some imports\n",
        "import torch\n",
        "import torchtext"
      ],
      "metadata": {
        "id": "iha4BPZW02bE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing"
      ],
      "metadata": {
        "id": "TGpcXRNK8d_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building your data preprocessing pipeline is a crucial part of working with text! This contains:\n",
        "- loading the dataset and constructing train, dev and test splits\n",
        "- building a tokenizer and vocabulary (all tokens that occur in the train split)\n",
        "- pre-tokenizing, truncating or padding the data to prepare for training"
      ],
      "metadata": {
        "id": "yCDHKHpBEYu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1 (2 points)**: Split the `AG_NEWS` dataset into train, dev and test splits. The dataset already comes with pre-defined train and test splits. Use a random sample of 10% of samples from the train split to construct our dev split. Use a fixed random seed of `42` to ensure reproducibility. \n",
        "\n",
        "*Always* split your data in the beginning, before doing any further processing steps on the train split or you might involuntarily leak information.\n",
        "\n",
        "\n",
        "How many unique labels are there? What are the classes?"
      ],
      "metadata": {
        "id": "zVYwkCxV7eiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.datasets import AG_NEWS\n",
        "from torch.utils.data.dataset import random_split # hint\n",
        "from torchtext.data.functional import to_map_style_dataset # hint\n",
        "\n",
        "# TODO: split dataset\n",
        "\n",
        "train_split = None  # Complete\n",
        "dev_split = None    # Complete\n",
        "test_split = None   # Complete\n",
        "\n",
        "\n",
        "assert len(train_split) == 108000\n",
        "assert len(dev_split) == 12000\n",
        "assert len(test_split) == 7600\n",
        "\n",
        "assert len(train_split[0]) == 2  # Each sample is a tuple of (topic_label, news_text)\n",
        "\n",
        "# If you have set the manual_seed to 42, you always get the sample splits in different runs\n",
        "## In the following, we test if the last sample in train_split is as expected\n",
        "assert train_split[-1][0] == 1\n",
        "assert train_split[-1][1].startswith('Soviet-style Belarus election') == True\n",
        "\n",
        "\n",
        "# TODO: What are unique label ids? How may unique labels exist? \n",
        "# Bonus question: What is the mapping between the label ids and class names? hint: https://github.com/mhjabreel/CharCnn_Keras/blob/master/data/ag_news_csv/classes.txt\n",
        "labels = set()  # Complete\n",
        "assert len(labels) == 4"
      ],
      "metadata": {
        "id": "ijaabJO4Wv__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use a basic tokenizer for English from the `torchtext` package. Observe the effects the tokenizer has one a sample input.\n",
        "\n"
      ],
      "metadata": {
        "id": "g6ucUW0MYDpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
        "\n",
        "print(train_split[0][1])\n",
        "print(tokenizer(train_split[0][1])) # (label, text) format in train_split"
      ],
      "metadata": {
        "id": "hYUOJBc6XY4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2 (3 points)**\n",
        "- (2 points) Using the tokenizer, build the total *vocabulary* of all tokens occuring in the training split of our AG News dataset. Use `torchtext.vocab.build_vocab_from_iterator` with the added special tokens `[\"<unk>\", \"<pad>\"]` (**important!**). \n",
        "- (1 point) Print the number of tokens in the vocabulary and the tokens that map to IDs `0-4`."
      ],
      "metadata": {
        "id": "fD9x1G36YwDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import build_vocab_from_iterator # hint\n",
        "\n",
        "def yield_tokens(data_split):\n",
        "    \"\"\"This might be useful\"\"\"\n",
        "    for _, text in data_split:\n",
        "        yield tokenizer(text)\n",
        "        \n",
        "def get_vocab(split):\n",
        "  # TODO: build the vocab and return it\n",
        "\n",
        "  return vocab\n",
        "\n",
        "vocab = get_vocab(train_split)\n",
        "vocab.set_default_index(vocab[\"<unk>\"]) # To handle out of vocabulary cases: if token not in vocabulary, return <unk>\n",
        "assert len(vocab) == 91154\n",
        "\n",
        "# Bonus question: What is the token id/indice for the special token '<unk>'? \n",
        "## Hint: torchtext.vocab.get_stoi() is helful for this!"
      ],
      "metadata": {
        "id": "vzNlKlgIX5E7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3 (3 points)**: Complete the `process_text` function. The function should:\n",
        "- tokenize the `text` with the given `tokenizer` (expected output: list of strings)\n",
        "- (1 point) truncate the result to a maximum of `max_sequence_length` tokens, if longer\n",
        "- (1 point) pad the result to `max_sequence_length`  using the `<pad>` token (1 point)\n",
        "- (1 point) convert all tokens into *token IDs* using the `vocab` (expected output: list of integers)\n",
        "\n",
        "\n",
        "We'll use this fucntion to prepapre our data to be fed into the GPU. By padding and truncating, we ensure every sample in the training set to be the same length, which allows us to do efficient batched training. We convert to IDs because we only need the IDs of tokens to retrieve their corresponding embedding during training."
      ],
      "metadata": {
        "id": "51zEEdR4Z9h5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_text(text, tokenizer, vocab, max_sequence_length=256, pad_token=\"<pad>\"):\n",
        "  # TODO: complete function\n",
        "  \n",
        "  tokens = None  # Complete\n",
        "\n",
        "  length = len(tokens)\n",
        "  if length > max_sequence_length:\n",
        "    # Complete\n",
        "    pass\n",
        "  elif length < max_sequence_length:\n",
        "    # Complete\n",
        "    pass\n",
        "\n",
        "  assert len(tokens) == max_sequence_length\n",
        "\n",
        "  token_ids = None # Complete        \n",
        "  \n",
        "  return token_ids\n",
        "\n",
        "mock_example_long = \"lorem ipsum dolar sonet \"*10_000\n",
        "assert len(process_text(mock_example_long, tokenizer, vocab)) == 256\n",
        "\n",
        "mock_example_short = \"lorem ipsum dolar sonet \"*2\n",
        "assert len(process_text(mock_example_short, tokenizer, vocab)) == 256\n",
        "assert process_text(mock_example_short, tokenizer, vocab)[-1] == vocab[\"<pad>\"] # <pad> tokens in end"
      ],
      "metadata": {
        "id": "7NFR8HN8YQ72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use the `process_text` function in a PyTorch `Dataloader`. There is some more stuff that needs to be done to prepare the data, which we're doing in this `collate_fn`. The `collate_fn` in a `Dataloader` is responsible for making sure the training data is in the right format for our model.\n",
        "\n",
        "Take a minute to go through the `collate_fn()` to understand what's going on."
      ],
      "metadata": {
        "id": "YL2gW7zxbPS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch, tokenizer, vocab, device):\n",
        "  prepared_labels, prepared_texts = [], []\n",
        "  for (_label, _text) in batch:\n",
        "        prepared_labels.append(int(_label) - 1) # labels in data start at 1, torch wants start at 0\n",
        "        processed_text = process_text(_text, tokenizer, vocab)\n",
        "        prepared_texts.append(processed_text)\n",
        "  \n",
        "  # turn into tensors (containing the token IDs)\n",
        "  prepared_labels = torch.tensor(prepared_labels, dtype=torch.int64)\n",
        "  prepared_texts = torch.tensor(prepared_texts, dtype=torch.int64)\n",
        "\n",
        "  # Move to GPU if necessary\n",
        "  prepared_labels, prepared_texts = prepared_labels.to(device), prepared_texts.to(device)\n",
        "  return prepared_labels, prepared_texts"
      ],
      "metadata": {
        "id": "g6GQ-SEu1Zy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network Architecture"
      ],
      "metadata": {
        "id": "8v_mJm9L8j37"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we'll define our model architecture. You will need to implement two main parts:\n",
        "1. An Embedding Layer that maps token IDs to a 1-dimensional tensor (i.e. vector) of a fixed size (in this example, choose 50)\n",
        "2. A Multi-Layer Percpetron that gets as input the **mean** of all token embeddings in the input sequence and outputs distribution over the number of classes. \n",
        "\n",
        "\n",
        "<details>\n",
        "<summary>Background: Why do we take the mean?</summary>\n",
        "<br>\n",
        "Another option is to concatenate all input embeddings in a sequence in order and feed them into the MLP (our MLP will then need an input layer of size <code>embed_dim * sequence_length</code>). This leads to a larger model but most importantly introduces a strong dependency on the <b>position</b> in the sequence a token appears in. If a token is just shifted a single position to the right, it will be fed into a completely different set of neurons. Obviously, this is undesirable. By taking the mean, we obtain a <b>position-invariant</b> model. The attentive reader will have noticed that a position-invariant model fixes the position-dependency problem, but looses relevant information (i.e. the token order) in the process. In the coming weeks, we will discuss the state-of-the-art architectural innovation to alleviate this problem... stay tuned!</details>\n",
        "\n"
      ],
      "metadata": {
        "id": "hQVAPq9XG8oX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 4 (10 points)**: \n",
        "- (4 points) Define the network architecture in the `__init__()` function below. We will use an embedding dimension of `50` and train a 2-layer MLP with a single fully-connected hidden layer with ReLU activation, you can also use dimension `50`. The output layer should have neurons according to `num_class`.\n",
        "- (4 points) Complete `init_weights()` to initialize the embedding layer and the MLP weights from a uniform distribution with range `[-1,1]`. Initialize all biases in the MLP to zero.\n",
        "- (2 points) Complete the forward pass of the network (token ID -> embedding -> MLP). For the MLP, use a fully-connected layer (`Linear` layer in `torch.nn`) and the ReLU activation function."
      ],
      "metadata": {
        "id": "4nzrIzShJ2d5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tips**\n",
        "- When defining the Embedding, make sure to supply the correct `padding_idx`. **This is crucial** to make sure that the padding embedding stays a `0`-vector instead of being optimized by gradient descent.\n",
        "- The network should only ouput the *logits* (the stuff before the softmax layer). The actual softmax to obtain a probability distribution over classes is combined with the loss function later on for numerical stability."
      ],
      "metadata": {
        "id": "dUmE-URtY_dc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torch.nn import EmbeddingBag, Linear, ReLU, Sequential # hints\n",
        "\n",
        "\n",
        "class MyTextClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class):\n",
        "        super().__init__()\n",
        "        # TODO: define embedding layer\n",
        "        pass\n",
        "\n",
        "        # TODO: define MLP\n",
        "        pass\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        # TODO: initialize weights\n",
        "        # hint: what should you set for `nn.Linear.weight` and `nn.Linear.bias` variables? For all layers of the network?\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, text):\n",
        "        # TODO forward pass\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "Tk-tcKf7KxSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "ximO_oIm-gxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now comes the last part: defining the training loop! We have prepared a skeleton for you to use below. Make sure you understand every part of the code. \n",
        "\n",
        "**Task 5 (8 points)**: Complete the training loop with all relevant steps. The training should run without errors when executing the next task and loss on the training set should steadily decrease. The code will be used in conjunction with the cell form Task 6, so you should also look at the code there for context.\n",
        "\n",
        "If you're new to PyTorch, you can find inspiration in this blog post: https://pytorch.org/tutorials/beginner/introyt/trainingyt.html."
      ],
      "metadata": {
        "id": "o4zGFOylMVbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train(model, dataloader, optimizer, loss_fn, epoch=1):\n",
        "    \"\"\"Do model training for a single epoch.\"\"\"\n",
        "    # TODO: set model to `train` mode\n",
        "    pass\n",
        "\n",
        "    total_acc, total_loss, total_count = 0, 0, 0\n",
        "    log_interval = 500\n",
        "    start_time = time.time()\n",
        "    train_progress = tqdm(dataloader, desc=f\"Epoch {epoch}\", leave=False)\n",
        "\n",
        "    for idx, (label, tokens) in enumerate(train_progress, 0):\n",
        "        # TODO: reset gradients to zero\n",
        "        pass\n",
        "\n",
        "        # TODO: do the forward pass\n",
        "        output = ...\n",
        "\n",
        "        # TODO: compute the loss\n",
        "        loss = ...\n",
        "\n",
        "        # TODO: do the backward pass\n",
        "        pass\n",
        "\n",
        "        # TODO: do the optimization step\n",
        "        pass\n",
        "\n",
        "\n",
        "        # Logging and evaluation\n",
        "        total_acc += (output.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        total_loss += loss.item()\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            avg_loss = total_loss / log_interval # loss is already mean over batch dim\n",
        "\n",
        "            # train_progress.set_description(f\"Epoch: {epoch}, loss: {avg_loss:.3}\")\n",
        "            elapsed = time.time() - start_time\n",
        "            print(f' |{idx:5d}/{len(dataloader):5d} batches | loss: {avg_loss:.3} | accuracy {total_acc/total_count:.3f}')\n",
        "            total_acc, total_loss, total_count = 0, 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(model, dataloader, loss_fn):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad(): # Faster, since we do not want gradients here\n",
        "        for idx, (label, tokens) in enumerate(dataloader):\n",
        "            # TODO: do the forward pass to get predictions\n",
        "            prediction = ...\n",
        "\n",
        "            # TODO: calculate loss\n",
        "            loss = ...\n",
        "\n",
        "            total_acc += (prediction.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count"
      ],
      "metadata": {
        "id": "OGdfs2HvLZNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations! Everything should be set up now and you can start the training using the cell below."
      ],
      "metadata": {
        "id": "wKQBwzuEb9lF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will notice that the training is quite slow. We have already implemented the use of GPUs for hardware acceleration. So let's use a GPU to make things faster ðŸš€. Google Colab lets you use a free GPU. Go to Runtime > Change runtime type > Select GPU in the dropdown. **You will need to restart the runtime and re-execute cells.** The restart should happen automatically, you might have to re-execute cells manually. \n",
        "\n",
        "\n",
        "To use the power of GPUs, our **model** and **data** need to be transferred to the GPU in our code. This is done by the `.to(device)` calls where the device is set to \"cpu\" or \"cuda\" (for Nvidia GPUs) depending on GPU availability."
      ],
      "metadata": {
        "id": "GZVcuHt2bWPf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 6 (10 points)**: Achieve an accuracy on the dev split of 90%! There are simple and more creative ways to do this. It's entirely up to you! We have defined defaults for the hyperparameters, loss functions, optimizer, etc. in the code. **Hint:** they are not optimal. Briefly summarize your changes below.\n",
        "\n",
        "If you do not know where to start: our \"Neural Network Tuning Guide\" at the end of this notebook will be helpful for you, particularly the \"Basic Wisdom\". \n",
        "\n",
        "You are allowed to collaborate with other students for this part but everyone should submit their own unqiue solution (i.e. do not copy-paste code or text answers).\n",
        "\n",
        "For this task it is especially important to keep the output of your successful training in the submission. The last evaluation on the dev set in the output should be over 90%. If you modify any of the previous cells, keep the code for the original task as a comment.\n",
        "\n",
        "\n",
        "- If you reach 80% accuracy, you'll get 3 points\n",
        "- If you reach 85%, you'll get 5 points\n",
        "- If you reach 88%, you'll get 8 points\n",
        "- If you get to >=90%, you'll get 10 points"
      ],
      "metadata": {
        "id": "Gijnm3BBcH72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Hyperparameters - sane default values for this example\n",
        "EPOCHS = 3 # epoch\n",
        "LR = 0.1 # learning rate\n",
        "BATCH_SIZE = 64 # batch size for training\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(42) # for reproducible results \n",
        "\n",
        "model = MyTextClassifier(\n",
        "    vocab_size=len(vocab),\n",
        "    embed_dim=50,\n",
        "    hidden_dim=50,\n",
        "    num_class=-1 # TODO: fill in correct value\n",
        "    )\n",
        "model = model.to(device) # Move model to GPU if necessary\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss() # this combines softmax with the actual loss function\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "\n",
        "collator = lambda batch: collate_fn(batch, tokenizer, vocab, device)\n",
        "\n",
        "train_dataloader = DataLoader(train_split, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collator)\n",
        "dev_dataloader = DataLoader(dev_split, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collator)\n",
        "\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(model, train_dataloader, optimizer, loss_fn, epoch)\n",
        "    accu_val = evaluate(model, dev_dataloader, loss_fn)\n",
        "    print('-' * 59)\n",
        "    print(f'| end of epoch {epoch:3d} | time: {time.time() - epoch_start_time:5.2f}s | ' +\n",
        "          f'dev set accuracy {accu_val:8.3f} ')\n",
        "    print('-' * 59)"
      ],
      "metadata": {
        "id": "ccrz_nfS3KdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: What did you do to achieve 90% accuracy on the dev set? Explain!\n"
      ],
      "metadata": {
        "id": "FBuSFXJJfcvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 7 (2 points)**: Finally, you should evaluate your best model on the test split. Does it perform as well as on the dev split?\n",
        "\n",
        "**The test set**: Careful, if you run this and change something about your model, your results are not statistically significant anymore!"
      ],
      "metadata": {
        "id": "s25N4C0STDTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader = DataLoader(test_split, batch_size=BATCH_SIZE,\n",
        "                             shuffle=False, collate_fn=collator)\n",
        "\n",
        "accu_test = evaluate(model, test_dataloader, criterion)\n",
        "print(f'test accuracy {accu_test:.3f}')"
      ],
      "metadata": {
        "id": "0CRKsvU_chkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Short answer: does performance match the test set? Hypothesis why / why not?\n"
      ],
      "metadata": {
        "id": "5cz4yNwJeRGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 8 (5 points)**: So far, our embedding layer was trained from a random initialization each time. However, there already exist **pretrained** word embeddings like Word2Vec or GloVe. \n",
        "- Choose an appropriate pretrained word embedding (hint: `torchtext.vocab.GloVe`. hint2: embedding dimension.)\n",
        "- Use the pretrained word embedding to initialize the model token embedding. The vocabularies in both embeddings will not be the same! Here, a naive solution is okay (only initialize the token embeddings that occur in both vocabularies). But you can also think of smarter strategies!\n",
        "- You should use the exact same setup you have used to achieve your 90% accuracy score in the previous task (if you need to change any cells for the intialization, simply copy + paste them below; then modify & re-execute. Do not modify the code above). Summarize your observations and provide a brief explanation for why they might occur."
      ],
      "metadata": {
        "id": "JyRA3GkphlKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What changes when initializing from pretrained embeddings instead of a random distribution? Explain!\n"
      ],
      "metadata": {
        "id": "vfN9R5pcjjbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Hyperparameters - sane default values for this example\n",
        "EPOCHS = 3 # epoch\n",
        "LR = 0.1 # learning rate\n",
        "BATCH_SIZE = 64 # batch size for training\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(42) # for reproducible results \n",
        "\n",
        "model = MyTextClassifier(vocab_size=len(vocab), embed_dim=50, hidden_dim=50, num_class=4)\n",
        "\n",
        "# TODO: load pretrained word embeddings and use them to init the model's embedding layer\n",
        "glove = torchtext.vocab.GloVe(name='6B', dim=50)\n",
        "\n",
        "\n",
        "model = model.to(device) # Move model to GPU if necessary\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss() # this combines softmax with the actual loss function\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "\n",
        "collator = lambda batch: collate_fn(batch, tokenizer, vocab, device)\n",
        "\n",
        "train_dataloader = DataLoader(train_split, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collator)\n",
        "dev_dataloader = DataLoader(dev_split, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collator)\n",
        "\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(model, train_dataloader, optimizer, criterion, epoch)\n",
        "    accu_val = evaluate(model, dev_dataloader, criterion)\n",
        "    print('-' * 59)\n",
        "    print(f'| end of epoch {epoch:3d} | time: {time.time() - epoch_start_time:5.2f}s | ' +\n",
        "          f'dev set accuracy {accu_val:8.3f} ')\n",
        "    print('-' * 59)"
      ],
      "metadata": {
        "id": "n5TPbX68iGR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3ed0ScnbSoR"
      },
      "source": [
        "## Neural Network Tuning Guide"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wryjR70BbZUN"
      },
      "source": [
        "We improve the performance of our neural networks by tuning **hyperparameters**. These are all things that influence the networks performance except for the neural netwroks weights (parameters), which are tuned during training. Hyperparameters include:\n",
        "- The learning rate\n",
        "- The number of training epochs\n",
        "- The choice for optimizer (SGD, Adam, ...)\n",
        "- The batch size\n",
        "- Network architecture\n",
        "  - The number of layers in the network (more layers -> deeper network)\n",
        "  - The \"width\" of layers in the network (more neurons per layer -> wider network)\n",
        "  - Activation functions\n",
        "- Regularization\n",
        "  - L2-Regularization\n",
        "  - Data augmentation (especially for images!)\n",
        "  - Dropout (randomly dropping neurons during training)\n",
        "  - Small batch sizes\n",
        "- Normalizing the input data to unit Gaussian range (`transforms.Normalize(0.5, 0.5)`, applies mostly for images) \n",
        "- Adding normalization layers (`BatchNorm`, `LayerNorm`) to the network\n",
        "- Adding a learning rate schedule\n",
        "- ... (many more)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CKBu6ducPte"
      },
      "source": [
        "This can seem overwhelming and a bit like \"alchemy\" (and there is some truth to this). But over time, you will reliably build an intuition about what hyperparameters are responsible for what kinds of behavior and which hyperparameters might be responsible for failure modes. \n",
        "\n",
        "**Basic wisdom**: You can start by training the model as long as the training loss still improves. Then, check if the loss and performance on the dev set is also still improving. If yes --> train longer / with higher learning rate, make network bigger. If no --> train shorter / with lower learning rate, add regularization. Big changes over small changes: e.g.rather than just training for one more epoch, train twice as long, then iterate and finetune later on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmP2CAPfdBN8"
      },
      "source": [
        "Some (humorful) resources are [this neural network training recipe](http://karpathy.github.io/2019/04/25/recipe/) by Andrej Karpathy, [this experience report](https://towardsdatascience.com/the-art-of-hyperparameter-tuning-in-deep-neural-nets-by-example-685cb5429a38) or [this practical advice video](https://www.youtube.com/watch?v=wKkcBPp3F1Y) by Andrew Ng."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQJ8UXZMfQ2K"
      },
      "source": [
        "Some steps you can go through that might be worthwhile if you cannot decide:\n",
        "- make the network twice as wide\n",
        "- make the network twice as deep\n",
        "- use the `Adam` optimizer instead of `SGD` and tuning optimizer hyperparameters\n",
        "- train for twice as many epochs\n",
        "- always save a checkpoint of the best model during training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db2BXayagSh4"
      },
      "source": [
        "**Bonus**: Is the network overfitting or not? Compare the performance on the dev set with the performance on the train set. A common visualization is to plot train and dev losses, as well as accuracy, over the course of training. **Having a plot of loss on the train vs. dev split over time is key to diagnose training failure modes**."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}