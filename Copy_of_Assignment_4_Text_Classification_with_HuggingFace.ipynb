{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hieutdle/ml-note/blob/main/Copy_of_Assignment_4_Text_Classification_with_HuggingFace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5811d32a"
      },
      "source": [
        "# Exercise \"Natural Language Processing\" -- Text Classification with Huggingface\n",
        "\n",
        "For this course, save a COPY to your Google Drive for the tutorial (File -> Save copy in Drive). Then complete the tasks in your saved copy. If you're done, submit your notebook with the name `{yourFirstName_yourLastName}.ipynb` via moodle **by sharing a link** with the full (i.e. read + write) permissions. We will do the grading in the Colab Notebook copy. Please avoid uploading `ipynb` files and rather, share the link to your notebook with us.\n",
        "\n",
        "This is an individual assignment, i.e., submit your solutions individually.\n",
        "\n",
        "This assignment is **mandatory for participation in the exam**. You are required to obtain at least 50% of the total points in this assignment to become eligible for participating in the final exam.\n",
        "\n",
        "\n",
        "**Due date: 15.06.2023, 9:15 a.m.(CEST)**\n",
        "\n",
        "----\n",
        "\n",
        "In this assignment we will revisit the text classification task from the previous assignment but we will solve it using the Transformer model architecture implemented with the HuggingFace `transformers` library.\n",
        "\n",
        "In particular, we will:\n",
        "- Take a look at the Transformer model architecture\n",
        "- Define data loading and processing pipelines for the `AG_NEWS` dataset\n",
        "- Train a BERT-style Transformer model for classifiying the news articles in `AG_NEWS`\n",
        "- Use Transfer Learning to boost performance on the text classification task\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "5811d32a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we need to install a few packages and we are ready to go."
      ],
      "metadata": {
        "id": "tr24ph1Cc612"
      },
      "id": "tr24ph1Cc612"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce65360b"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets evaluate accelerate scikit-learn"
      ],
      "id": "ce65360b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Background on Transformer Models"
      ],
      "metadata": {
        "id": "EwH_P31pG-Za"
      },
      "id": "EwH_P31pG-Za"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment, we will use the Transformer neural network architecture. This architecture is much more powerful than the simple MLP from last assignment!\n",
        "\n",
        "\n",
        "You might find different styles of Transformer models online. For this assignment, we will use BERT-style Transformer models (sometimes also called \"encoder-only\" Transformers). \n",
        "\n",
        "Developing an intuitive understanding of how Transformers work is important but not trivial. Besides the lecture content, we recommend this excellent write-up for Transformers: https://jalammar.github.io/illustrated-transformer/."
      ],
      "metadata": {
        "id": "rPPNjtS93iqr"
      },
      "id": "rPPNjtS93iqr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's take a look at single Transformer model. We will use the `transformers` library by HuggingFace, which makes training Transformer models really easy and convenient (and is used by many researchers around the world)."
      ],
      "metadata": {
        "id": "N7wNSk9h-fH2"
      },
      "id": "N7wNSk9h-fH2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1 (1 point)**: Load and instantiate the `distilroberta-base` model with `transformers`; then print the model object using Python's native `print` function. \n",
        "\n",
        "`distilroberta-base` is a BERT-style Transformer model derived from the RoBERTa architecture (Robustly optimized BERT Pretraining approach). As you can see, NLP researchers like puns. It has been \"distilled\", e.g. approximated using a smaller model size. We are using it so that training using Colab's single GPU is faster, since the model is smaller than the original variants.\n",
        "\n",
        "HINT: `transformers.AutoModel.from_pretrained()`"
      ],
      "metadata": {
        "id": "F2Ytu4IA-0aW"
      },
      "id": "F2Ytu4IA-0aW"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel # hint\n",
        "\n",
        "# TODO"
      ],
      "metadata": {
        "id": "TJzjnwpN-zNf"
      },
      "id": "TJzjnwpN-zNf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `print` statement should have given you a nice overview over the entire RoBERTa Transformer model. You should be able to identify the layers that are responsible for the attention mechanism. RoBERTa is a highly optimized model architecture (although nowadays even better architectures exist). That is why there are many different modules, it is okay if you do not understand the role of every last one for now.\n",
        "\n",
        "In the `embedding` module of `distilroberta-base`, you should also be able to see a module called `position_embeddings`.\n"
      ],
      "metadata": {
        "id": "ptzUhoiC_czU"
      },
      "id": "ptzUhoiC_czU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Task 2 (2 points):** What is the role of the positional embedding in BERT-style Transformer models. Why is it necessary? (roughly 2-4 sentences)"
      ],
      "metadata": {
        "id": "dxjDaSvrNcOL"
      },
      "id": "dxjDaSvrNcOL"
    },
    {
      "cell_type": "code",
      "source": [
        "# Text answer below\n"
      ],
      "metadata": {
        "id": "P3gHp5Gb_uHh"
      },
      "id": "P3gHp5Gb_uHh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3 (2 points)**: What is the role of the attention mechanism in BERT-style Transformer models? (roughly 2-4 sentences)"
      ],
      "metadata": {
        "id": "KAoSBP8xhI-0"
      },
      "id": "KAoSBP8xhI-0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Text answer below\n"
      ],
      "metadata": {
        "id": "U5IJGZ84hvKV"
      },
      "id": "U5IJGZ84hvKV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Pipeline"
      ],
      "metadata": {
        "id": "2e_Cv0HGGiJW"
      },
      "id": "2e_Cv0HGGiJW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's prepare for training our own Transformer model. As always, we need to set up a data processing pipeline."
      ],
      "metadata": {
        "id": "g-YUfYI3HDG5"
      },
      "id": "g-YUfYI3HDG5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42dc8b44"
      },
      "source": [
        "We will still use the `AG_NEWS` dataset. Last time, we used `torch.datasets` to load `AG_NEWS`. Besides `transformers`, HuggingFace also has a library called `datasets` that is widely used and supports most popular (and even unpopular) datasets. So this time, we will use `datasets` to load `AG_NEWS`!\n",
        "\n",
        "\n",
        "**Task 4 (3 points)**: \n",
        "- Load the `AG_NEWS` dataset using HuggingFace `datasets`.\n",
        "- Transformer models need a lot of compute and using the entire dataset will take too much of your time for this assignment. Shuffle the train split of the dataset and select only the first `4000` samples after shuffling; discard the rest.\n",
        "- We still need to create a dev split. Take a random sample of 10% of the samples from the (reduced) train set (hint: `datasets.train_test_split`). For compatibility with `transformers`, the dev split should be assigned under the `\"val\"` keyword (for validation). Background: validation and dev split are two names for the same thing. There is a fierce debate between ML researchers over which one is correct.\n",
        "\n",
        "Use a fixed random seed of 42 whenever random number generation is involved to ensure reproducibility. \n",
        "\n"
      ],
      "id": "42dc8b44"
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset  # hint\n",
        "import torch\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "# Wow, we already did it for you!\n",
        "dataset = load_dataset(\"ag_news\")\n",
        "dataset = dataset.shuffle(SEED)\n",
        "\n",
        "# TODO: select the first 4000 samples from the training set to amke the dataset manageable for this assignment\n",
        "\n",
        "\n",
        "assert len(dataset[\"train\"]) == 4000, f'you should have 4000 samples but you have {len(dataset[\"train\"])} samples'\n",
        "\n",
        "# TODO: generate the validation set from the training data.\n",
        "# Assume the training set was not shuffled, perform a shuffle before splitting. This is good practice, so we practice it here as well.\n",
        "# hint: you can use the train_test_split method from HF. Remember the seed!\n",
        "\n",
        "\n",
        "assert len(dataset[\"test\"]) == 7600, f'test set should not be touched but you have {len(dataset[\"test\"])} samples'\n",
        "assert len(dataset[\"train\"]) == 3600, f'train set should be 3600 samples but you have {len(dataset[\"train\"])} samples'\n",
        "assert len(dataset[\"val\"]) == 400, f'val set should be 400 samples but you have {len(dataset[\"val\"])} samples'"
      ],
      "metadata": {
        "id": "3_y69lpFauHc"
      },
      "id": "3_y69lpFauHc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 5 (2 points):** For each split, calculate the distribution of class labels and print them (e.g. train: 0 ->  0.23 % | 1 ->  0.27 % | 2 ->  0.24 % | 3 ->  0.26%). Are the labels (roughly) balanced? What could we do to guarantee a balanced random split?"
      ],
      "metadata": {
        "id": "98rRHvkZFt7T"
      },
      "id": "98rRHvkZFt7T"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "509949b6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# TODO: for each split, calculate the class label distribution and print it\n",
        "\n",
        "# EXTRA TODO: store the number of class labels in `num_labels` (we will use it later)\n",
        "# num_labels: int = ...\n"
      ],
      "id": "509949b6"
    },
    {
      "cell_type": "code",
      "source": [
        "# Are the labels (roughly) balanced? What could we do to guarantee a balanced random split?\n"
      ],
      "metadata": {
        "id": "d5CEsi8tdHnP"
      },
      "id": "d5CEsi8tdHnP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a3cdc5d"
      },
      "source": [
        "**Task 6 (7 points)**: \n",
        "\n",
        "Similarly to the assignment-3, write a *preprocess_function* which:\n",
        "- tokenizes the text with the DistilRoBERTa tokenizer (hint: `transformers.AutoTokenizer.from_pretrained()`)\n",
        "- truncates the result to a maximum of max_sequence_length tokens, if longer\n",
        "- pads the result to max_sequence_length using the padding token from DistilRoBERTa\n",
        "- converts all tokens into token IDs (expected output: list of integers)\n",
        "\n",
        "HINT: You can implement all these things manually, but they can also be done in a single line using the tokenizer implementation from HuggingFace.\n",
        "\n",
        "Finally call this function for each sample of your train, test and validation set using the `map` method from the HuggingFace `datasets` library.\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "6a3cdc5d"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer  # hint\n",
        "from typing import Dict, Any\n",
        "from datasets import Dataset\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-base\")\n",
        "\n",
        "\n",
        "# TODO: define a preprocessing function to tokenize a sample\n",
        "def preprocess_function(sample: Dict[str, Any], seq_len: int):\n",
        "    \"\"\"\n",
        "    Function applied to all the examples in the Dataset (individually or in batches). \n",
        "    It accepts as input a sample as a dictionary and return a new dictionary with the BERT tokens for that sample\n",
        "\n",
        "    Args:\n",
        "        sample Dict[str, Any]:\n",
        "            Dictionary of sample\n",
        "            \n",
        "    Returns (for a single sequence, not a batch):\n",
        "        Dict: Dictionary of tokenized sample in the following style:\n",
        "        {\n",
        "          \"input_ids\": list[int] # token ids\n",
        "          \"attention_mask\": list[int] # Mask for self-attention (padding tokens are ignored).\n",
        "        }\n",
        "        Hint: if your are using the Huggingface tokenizer implementation, this is the default output format but check it yourself to be sure!\n",
        "    \"\"\"\n",
        "   pass\n",
        "\n",
        "\n",
        "# TEST for truncation\n",
        "mock_example_long = Dataset.from_list([{\"text\": (\"lorem ipsum dolar sonet \" * 10_000) }]).map(\n",
        "    preprocess_function, batched=True, fn_kwargs={\"seq_len\": 256}\n",
        ")\n",
        "assert len(mock_example_long[0][\"input_ids\"]) == 256\n",
        "\n",
        "# TEST for padding\n",
        "mock_example_short = Dataset.from_list([{\"text\": (\"lorem ipsum dolar sonet \" * 2) }]).map(\n",
        "    preprocess_function, batched=True, fn_kwargs={\"seq_len\": 256}\n",
        ")\n",
        "assert len(mock_example_short[0][\"input_ids\"]) == 256\n",
        "assert mock_example_short[0][\"input_ids\"][-1] == tokenizer.pad_token_id\n",
        "\n",
        "# TODO: use the `map` function to tokenize your dataset. store the results in `encoded_ds`\n",
        "# encoded_ds = ...\n",
        "\n",
        "\n",
        "# TEST  dataset\n",
        "assert len(encoded_ds[\"train\"][0][\"input_ids\"]) == 256\n",
        "assert len(encoded_ds[\"val\"][0][\"input_ids\"]) == 256\n",
        "assert len(encoded_ds[\"test\"][0][\"input_ids\"]) == 256\n",
        "assert len(encoded_ds[\"train\"][50][\"input_ids\"]) == 256"
      ],
      "metadata": {
        "id": "dDiFd3Iudukh"
      },
      "id": "dDiFd3Iudukh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "640a8dac"
      },
      "source": [
        "**Task 7 (3 points)**: Define a function to evaluate the model during training. The function is automatically called by the trainer on the validation or test set, it must take an `EvalPrediction` object \n",
        "(see https://huggingface.co/docs/transformers/main/en/internal/trainer_utils#transformers.EvalPrediction) \n",
        "and return a dictionary `dict[str, float]` mapping metric names (`str`) to metric values (`float`).\n",
        "\n",
        "\n",
        "HINT: Again, HuggingFace has a convenient library for evaluation called `evaluate`.\n"
      ],
      "id": "640a8dac"
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate # hint\n",
        "import numpy as np\n",
        "from transformers import EvalPrediction\n",
        "\n",
        "\n",
        "# TODO\n",
        "def compute_metrics(eval_pred: EvalPrediction):\n",
        "    \"\"\"\n",
        "    The function that will be used to compute metrics at evaluation. Must take a EvalPrediction and return a dictionary string to metric values.\n",
        "\n",
        "    Args:\n",
        "        eval_pred EvalPrediction:\n",
        "            Evaluation output (always contains labels), to be used to compute metrics.\n",
        "            It has one Numpy array with predictions and one with labels.\n",
        "            \n",
        "    Returns:\n",
        "        Dict: Dictionary of metric values\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "\n",
        "# TEST\n",
        "predictions = np.array([[0.7, 0.3, 0.9]])\n",
        "labels = np.array([2])\n",
        "eval_pred = EvalPrediction(predictions=predictions, label_ids=labels)\n",
        "assert compute_metrics(eval_pred)[\"accuracy\"] == 1\n",
        "\n",
        "predictions = np.array([[0.7, 0.3, 0.9], [0.9, 0.1, 0.1]])\n",
        "labels = np.array([2, 1])\n",
        "eval_pred = EvalPrediction(predictions=predictions, label_ids=labels)\n",
        "assert compute_metrics(eval_pred)[\"accuracy\"] == 0.5"
      ],
      "metadata": {
        "id": "ucdfL-Zsz4Bl"
      },
      "id": "ucdfL-Zsz4Bl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0f68aa1"
      },
      "source": [
        "# Training from scratch\n",
        "\n",
        "Now, we will start training our first Transformer model. We will begin by training a model **from scratch**, e.g. from randomly initialized weights.\n",
        "\n",
        "In the last assignment, we coded the entire training loop from scratch as well. In practice, it is often easier to use pre-implemented `Trainer` classes instead. Suprise: Huggingface provides such a `Trainer` implementation. \n",
        "\n",
        "The Huggingface `Trainer` abstracts away a lot of the complexity of training models! But to really understand what's going on and debug errors, it is crucial to know what's happening under the hood. \n",
        "\n",
        "**Task 8 (10 points)**: \n",
        "- Initialize the `distilroberta-base` model from HuggingFace **with random weights** and train it from scratch for 5 epochs. Use the `Trainer` class from HuggingFace for the training loop implementation and `AutoModelForSequenceClassification` instead of `AutoModel` to load the model. HINT: You definitely want to train on GPU this time, otherwise it will be very slow. \n",
        "\n",
        "* Why is it important to use `AutoModelForSequenceClassification` instead of `AutoModel`? How is the Transformer model architecture modified to predict the class labels? Describe in 3-6 sentences (rough guideline). HINT: You will need to resarch a bit on your own for this.\n",
        "\n",
        "\n",
        "You do not have to reach a specific perofrmance goal for this task. It is rather about building an understanding of how to work with Transformer models. An accuracy of below 60% after 5 epochs means things are probably not working as intended."
      ],
      "id": "c0f68aa1"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoConfig  # hint\n",
        "from transformers import TrainingArguments, Trainer  # hint\n",
        "\n",
        "# TODO: Create `TrainingArguments`. You can mostly use default values, but setting `per_device_train_batch_size` and `per_device_eval_batch_size` to 32 and a learning rate of 2e-5 worked well for us.\n",
        "# Set arguments to do the following: Evaluate and save a checkpoint every epoch. At the end of training, load the weights of the best checkpoint (measured by loss on the validation set). Set the seed to 1944 (Hasso's birthyear).\n",
        "# Don't forget to set the number of training epochs!\n",
        "\n",
        "# TODO: Load the model with *random weights*. HINT: number of class labels! HINT2: AutoConfig.from_pretrained and AutoModelForSequenceClassification.from_config\n",
        "# You'll get some warnings here, but usually they can just be ignored / are expected if you read them exactly\n",
        "\n",
        "# TODO: Initialize the `Trainer` and start training! Don't forget passing the `compute_metrics` method!\n",
        "\n",
        "# TODO: Final evaluation after training\n"
      ],
      "metadata": {
        "id": "5H8YQM0xz2iM"
      },
      "id": "5H8YQM0xz2iM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer: AutoModelForSequenceClassification vs. AutoModel\n"
      ],
      "metadata": {
        "id": "HOrWD2g1ULWa"
      },
      "id": "HOrWD2g1ULWa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning: Finetuning a pretrained model\n",
        "\n",
        "Nowadays, barely anyone trains a Transformer model from scratch for NLP. Instead, we initialize our model with **pretrained weights**. Usually these weights have been trained with massive amounts of data and compute and publicly released by big players like Google, Meta, Microsoft, etc... or SAP ;).\n",
        "\n",
        "HuggingFace `transformers` makes loading pretrained weights very easy with the `AutoModelFor<TaskDescription>.from_pretrained(<model-name>)` method.\n",
        "\n",
        "**Task 9 (5 points)**:\n",
        "* Load `distilroberta-base` with **pretrained weights** and finetune the model on our task for 5 epochs. Set a different `output_dir` than for the previous training. Otherwise, use the same `TrainingArguments` as before.\n",
        "* Which weights of the model were initialized from pretrained weights and which weights were still randomly initialized?\n",
        "* Briefly describe the differences you observe to training from scratch. What might be the reasons for these differences? (roughly 3-6 sentences).\n",
        "\n",
        "\n",
        "Again, you do not have to hit specific performance goals here."
      ],
      "metadata": {
        "id": "pqA3GMCuh2Yg"
      },
      "id": "pqA3GMCuh2Yg"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer, BertForSequenceClassification  # hint\n",
        "\n",
        "# TODO: Create `TrainingArguments`\n",
        "\n",
        "# TODO: Load the model with pretrained weights \n",
        "\n",
        "# TODO: Initialize the `Trainer` and start training!\n",
        "\n",
        "# TODO: Final evaluation after training\n"
      ],
      "metadata": {
        "id": "tNuYGX1_z1S9"
      },
      "id": "tNuYGX1_z1S9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Answers\n"
      ],
      "metadata": {
        "id": "cHAoRrJDVLdC"
      },
      "id": "cHAoRrJDVLdC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing\n",
        "\n",
        "**Task 10 (1 point)**: Evaluate the trained model from Task 9 (finetuning a pretrained model) on the test set. Is there a performance gap between validation and test set? "
      ],
      "metadata": {
        "id": "jIAUW1JSrySK"
      },
      "id": "jIAUW1JSrySK"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: evaluate on testset\n"
      ],
      "metadata": {
        "id": "95lxeX3oz7lV"
      },
      "id": "95lxeX3oz7lV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer: Is there a performance gap between validation and test set?\n"
      ],
      "metadata": {
        "id": "X9ugiCcKf9R4"
      },
      "id": "X9ugiCcKf9R4",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}